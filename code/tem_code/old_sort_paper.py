'''
sort_paper çš„ Docstring
ç”¨äºæ”¶é›†å¹¶åˆ†ç±»äº†æ‰€æœ‰å¼•ç”¨äº†ä¸¤ç¯‡æ ¸å¿ƒè®ºæ–‡çš„ç›¸å…³è®ºæ–‡
å¹¶ä½¿ç”¨ DeepSeek API è¿›è¡Œå¤šç»´åº¦åˆ†ç±»
æ ¸å¿ƒè®ºæ–‡ï¼š
- Measuring Faithfulness in Chain-of-Thought Reasoning (Lanham et al.)
- Language Models Don't Always Say What They Think (Turpin et al.)
åˆ†ç±»ç»“æœå‚¨å­˜åœ¨ faithfulness_papers_full_survey.csv ä¸­
'''
import requests
import pandas as pd
import json # è¿™é‡Œçš„jsonåº“ä»…ç”¨äºå…¶ä»–ç”¨é€”ï¼Œè§£æä¸å†ä¾èµ–å®ƒ
import time
import yaml
import re   # å¼•å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
from openai import OpenAI
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
# 1. è¯·åŠ¡å¿…æ›¿æ¢ä¸ºä½ çš„ Key
with open("../../config.yaml", "r") as f:
    config = yaml.safe_load(f)
API_KEY = config['API_KEY']
DEEPSEEK_API_KEY = API_KEY

# 2. API Base URL
BASE_URL = "https://yunwu.ai/v1"

# 3. æ ¸å¿ƒç§å­è®ºæ–‡ ID
SEED_PAPER_IDS = ["arXiv:2307.13702", "arXiv:2305.04388"]

# 4. è¾“å‡ºæ–‡ä»¶å
OUTPUT_FILE = "faithfulness_papers_full_survey.csv"
# ===========================================

def get_citations_from_semantic_scholar(paper_id):
    """
    ä» Semantic Scholar è·å–å¼•ç”¨äº†æŒ‡å®šè®ºæ–‡çš„æ‰€æœ‰æ–‡ç« åˆ—è¡¨
    """
    print(f"æ­£åœ¨è·å–å¼•ç”¨äº† {paper_id} çš„è®ºæ–‡åˆ—è¡¨...")
    url = f"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations"
    params = {'fields': 'title,abstract,year,citationCount,authors', 'limit': 1000}
    
    try:
        response = requests.get(url, params=params).json()
        papers = []
        if 'data' in response:
            for item in response['data']:
                citing_paper = item.get('citingPaper')
                if citing_paper and citing_paper.get('abstract') and citing_paper.get('title'):
                    papers.append({
                        'paperId': citing_paper.get('paperId'),
                        'title': citing_paper.get('title'),
                        'abstract': citing_paper.get('abstract'),
                        'year': citing_paper.get('year'),
                        'citations': citing_paper.get('citationCount')
                    })
        print(f" -> æ‰¾åˆ° {len(papers)} ç¯‡æœ‰æ•ˆå¼•ç”¨ã€‚")
        return papers
    except Exception as e:
        print(f"è·å–è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
        return []

def extract_content(text, tag):
    """
    é²æ£’æ€§æ ¸å¿ƒï¼šä½¿ç”¨æ­£åˆ™æå– <tag>...</tag> ä¹‹é—´çš„å†…å®¹
    re.DOTALL å…è®¸åŒ¹é…è·¨è¡Œå†…å®¹
    re.IGNORECASE å…è®¸å¿½ç•¥å¤§å°å†™
    """
    pattern = f"<{tag}>(.*?)</{tag}>"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return "Unknown" # å¦‚æœæ²¡æ‰¾åˆ°æ ‡ç­¾ï¼Œè¿”å› Unknown

def classify_with_deepseek(client, title, abstract):
    """
    è°ƒç”¨ DeepSeek API è¿›è¡Œå¤šç»´åº¦è¯¦ç»†åˆ†ç±» (ä½¿ç”¨ HTML Tags æ¨¡å¼)
    """
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ç§‘ç ”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹è®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ï¼Œåˆ†æå…¶å…³äºæ€ç»´é“¾ï¼ˆCoTï¼‰å¿ å®åº¦ï¼ˆFaithfulnessï¼‰çš„å†…å®¹ã€‚
    
    è®ºæ–‡æ ‡é¢˜: "{title}"
    æ‘˜è¦å†…å®¹: "{abstract}"
    
    è¯·æå–ä»¥ä¸‹ 5 ä¸ªç»´åº¦çš„å…³é”®ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼åŒ…è£¹åœ¨ XML/HTML æ ‡ç­¾ä¸­è¾“å‡ºï¼š

    1. <category>: ä¸»è¦ç±»åˆ«
       - Phenomenon (ç°è±¡å‘ç°)
       - Metric (è¯„ä¼°æŒ‡æ ‡)
       - Mitigation (æ”¹è¿›æ–¹æ³•)
       - Other (å…¶ä»–)

    2. <type>: æ–¹æ³•ç±»å‹
       - White-box (ç™½ç›’)
       - Black-box (é»‘ç›’)

    3. <domain>: ä»»åŠ¡é¢†åŸŸ
       - Math (æ•°å­¦)
       - Logic (é€»è¾‘)
       - Code (ä»£ç )
       - General (é€šç”¨)

    4. <tradeoff>: æ€§èƒ½æƒè¡¡ (ä»…é’ˆå¯¹ Mitigation)
       - Positive (åŒèµ¢)
       - Negative (ç‰ºç‰²å‡†ç¡®ç‡)
       - Unknown (æœªçŸ¥)

    5. <cost>: æ¨ç†å¼€é”€
       - High (é«˜æˆæœ¬ï¼Œå¦‚å¤šæ¬¡é‡‡æ ·)
       - Low (ä½æˆæœ¬ï¼Œå¦‚å•æ¬¡æ¨ç†)
       
    6. <reasoning>: ç®€çŸ­ä¸­æ–‡ç†ç”± (30å­—ä»¥å†…)

    è¾“å‡ºç¤ºä¾‹æ¨¡æ¿ (ä¸è¦è¾“å‡º Markdown ä»£ç å—ï¼Œç›´æ¥è¾“å‡ºä»¥ä¸‹æ–‡æœ¬):
    <category>Mitigation</category>
    <type>Black-box</type>
    <domain>Math</domain>
    <tradeoff>Positive</tradeoff>
    <cost>Low</cost>
    <reasoning>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„Promptç­–ç•¥ã€‚</reasoning>
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."}, # ä¸å†å¼ºåˆ¶ JSON
                {"role": "user", "content": prompt}
            ],
            # response_format={ "type": "json_object" }, # <--- ç§»é™¤è¿™ä¸€è¡Œï¼Œè¿™æ˜¯å…³é”®ï¼
            temperature=0.0
        )
        
        raw_content = response.choices[0].message.content
        
        # ä½¿ç”¨æ­£åˆ™è§£æç»“æœï¼Œè¿™æ¯” json.loads é²æ£’å¾—å¤š
        return {
            "category": extract_content(raw_content, "category"),
            "method_type": extract_content(raw_content, "type"),
            "task_domain": extract_content(raw_content, "domain"),
            "tradeoff": extract_content(raw_content, "tradeoff"),
            "cost": extract_content(raw_content, "cost"),
            "reasoning": extract_content(raw_content, "reasoning")
        }

    except Exception as e:
        print(f"API Error: {e}")
        return {
            "category": "Error", "method_type": "Error", 
            "task_domain": "Error", "tradeoff": "Error", "cost": "Error", 
            "reasoning": str(e)
        }

def retry_failed_rows(csv_file):
    """
    è¯»å– CSV æ–‡ä»¶ï¼ŒæŸ¥æ‰¾åˆ†ç±»å¤±è´¥ï¼ˆReasoning ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯æˆ– Category ä¸º Errorï¼‰çš„è¡Œï¼Œ
    å¹¶é‡æ–°è°ƒç”¨ API è¿›è¡Œåˆ†ç±»ï¼Œæœ€åæ›´æ–°æ–‡ä»¶ã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨æ£€æŸ¥ {csv_file} ä¸­çš„å¤±è´¥é¡¹...")
    
    try:
        df = pd.read_csv(csv_file)
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œä¸»ç¨‹åºæ”¶é›†æ•°æ®ã€‚")
        return

    # 1. å®šä¹‰å¤±è´¥çš„æ¡ä»¶
    # æ¡ä»¶A: Category è¢«æ ‡è®°ä¸º 'Error' (æ ¹æ®ä½ ä¹‹å‰çš„å¼‚å¸¸å¤„ç†é€»è¾‘)
    # æ¡ä»¶B: Reasoning ä¸­åŒ…å« 'time out' æˆ– 'timed out' (ä¸åŒºåˆ†å¤§å°å†™)
    # æ¡ä»¶C: Category æ˜¯ 'Unknown' (è§£æå¤±è´¥)
    error_mask = (
        (df['Category'] == 'Error') | 
        (df['Category'] == 'Unknown') |
        (df['Reasoning'].astype(str).str.contains('time out', case=False, regex=True)) |
        (df['Reasoning'].astype(str).str.contains('Error', case=False, regex=True))
    )
    
    failed_rows = df[error_mask]
    
    if failed_rows.empty:
        print("âœ… æ²¡æœ‰å‘ç°å¤±è´¥çš„è¡Œï¼Œæ— éœ€é‡è¯•ã€‚")
        return

    print(f"âš ï¸ å‘ç° {len(failed_rows)} ä¸ªå¤±è´¥æˆ–è¶…æ—¶çš„æ¡ç›®ï¼Œå¼€å§‹é‡è¯•...\n")
    
    # åˆå§‹åŒ– API Client (ç¡®ä¿ Key æ­£ç¡®)
    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=BASE_URL)
    
    # 2. éå†å¤±è´¥çš„è¡Œè¿›è¡Œé‡è¯•
    # ä½¿ç”¨ .index è·å–åŸå§‹è¡Œå·ï¼Œç¡®ä¿ç›´æ¥ä¿®æ”¹åŸ DataFrame çš„å¯¹åº”ä½ç½®
    for idx in tqdm(failed_rows.index, desc="Retrying"):
        row = df.loc[idx]
        title = row['Title']
        abstract = row['Abstract']
        
        # é‡æ–°è°ƒç”¨åˆ†ç±»å‡½æ•°
        analysis = classify_with_deepseek(client, title, abstract)
        
        # 3. åªæœ‰å½“æˆåŠŸï¼ˆä¸æ˜¯ Error ä¸”ä¸æ˜¯ Unknownï¼‰æ—¶æ‰æ›´æ–°
        # å¦‚æœè¿™æ¬¡åˆå¤±è´¥äº†ï¼Œä¿ç•™åŸæ¥çš„é”™è¯¯ä¿¡æ¯æˆ–è€…æ›´æ–°ä¸ºæ–°çš„é”™è¯¯ä¿¡æ¯çš†å¯
        if analysis['category'] != 'Error':
            df.at[idx, 'Category'] = analysis['category']
            df.at[idx, 'Type'] = analysis['method_type']
            df.at[idx, 'Domain'] = analysis['task_domain']
            df.at[idx, 'Tradeoff'] = analysis['tradeoff']
            df.at[idx, 'Cost'] = analysis['cost']
            df.at[idx, 'Reasoning'] = analysis['reasoning']
        else:
            # å¦‚æœåˆå¤±è´¥äº†ï¼Œæ›´æ–°ä¸€ä¸‹é”™è¯¯åŸå› ï¼ˆå¯èƒ½æ˜¯ä¸åŒçš„é”™è¯¯ï¼‰
            df.at[idx, 'Reasoning'] = analysis['reasoning']

        # æ¯æ¬¡å¤„ç†å®Œç¨å¾®åœé¡¿ï¼Œé˜²æ­¢å†æ¬¡è§¦å‘é™æµ
        time.sleep(0.5) 
        
        # æ¯é‡è¯• 5 ä¸ªä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­ç™½è·‘
        if idx % 5 == 0:
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')

    # 4. æœ€ç»ˆä¿å­˜
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    
    # ç»Ÿè®¡ä¿®å¤æƒ…å†µ
    remaining_errors = df[
        (df['Category'] == 'Error') | 
        (df['Reasoning'].astype(str).str.contains('time out', case=False))
    ].shape[0]
    
    print(f"\nğŸ‰ é‡è¯•ç»“æŸï¼")
    print(f"åŸå§‹å¤±è´¥æ•°: {len(failed_rows)}")
    print(f"å‰©ä½™å¤±è´¥æ•°: {remaining_errors}")
    print(f"æˆåŠŸä¿®å¤æ•°: {len(failed_rows) - remaining_errors}")

def main():
    # 1. åˆå§‹åŒ–

    client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=BASE_URL)
    
    all_results = []
    seen_titles = set()
    papers_to_process = []

    # === Step 1: æ”¶é›† ===
    print("Step 1: æ­£åœ¨æ”¶é›†æ‰€æœ‰å¼•ç”¨è®ºæ–‡...")
    for seed_id in SEED_PAPER_IDS:
        fetched_papers = get_citations_from_semantic_scholar(seed_id)
        for paper in fetched_papers:
            if paper['title'] not in seen_titles:
                seen_titles.add(paper['title'])
                papers_to_process.append(paper)
    
    print(f"\nå»é‡åï¼Œå…±éœ€å¤„ç† {len(papers_to_process)} ç¯‡è®ºæ–‡ã€‚")
    if not papers_to_process: return

    # === Step 2: åˆ†ç±» ===
    print("\nStep 2: å¼€å§‹ AI æ™ºèƒ½æ‰“æ ‡ (Regex Robust Mode)...")
    
    for paper in tqdm(papers_to_process): # æµ‹è¯•æ—¶å¯åŠ  [:5]
        
        analysis = classify_with_deepseek(client, paper['title'], paper['abstract'])
        
        entry = {
            "Title": paper['title'],
            "Year": paper['year'],
            "Citations": paper['citations'],
            # --- è¯¦ç»†æŒ‡æ ‡ ---
            "Category": analysis.get('category'),
            "Type": analysis.get('method_type'),
            "Domain": analysis.get('task_domain'),
            "Tradeoff": analysis.get('tradeoff'),
            "Cost": analysis.get('cost'),
            "Reasoning": analysis.get('reasoning'),
            # ---------------
            "Abstract": paper['abstract']
        }
        all_results.append(entry)
        
        if len(all_results) % 10 == 0:
            pd.DataFrame(all_results).to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
        
        time.sleep(0.1)

    # === Step 3: ä¿å­˜ ===
    df = pd.DataFrame(all_results)
    df = df.sort_values(by="Citations", ascending=False)
    df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    if 'Category' in df.columns:
        print("ğŸ“Š åˆ†ç±»ç»Ÿè®¡é¢„è§ˆ:")
        print(df['Category'].value_counts())

if __name__ == "__main__":
    mode = "retry"
    if mode == "retry":
        retry_failed_rows(OUTPUT_FILE)
    else:
        main()

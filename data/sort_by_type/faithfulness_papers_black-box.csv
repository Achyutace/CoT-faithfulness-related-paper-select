title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning_x,abstract,publication_venue,type,domain,reasoning_y
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,Black-box,General,该研究通过自然语言解释来增强语言模型的可解释性，主要依赖于模型的输入输出和Prompting技术，没有涉及模型内部机制的分析，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码、医学或社会学，而是通用的语言模型解释技术。
Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation,2025,True,True,True,Interpretability & Internal Mechanisms,论文讨论了CoT在临床决策中的不忠实现象，并提出了评估框架和改进方法。,"Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.",arXiv.org,Black-box,Medical,该研究主要评估大型语言模型在脊柱手术临床决策支持中的幻觉风险，通过输入输出和Prompting进行评估，并未涉及模型内部机制，因此属于Black-box。研究领域涉及医学决策支持，属于Medical领域。
Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity,2025,True,True,True,,论文探讨了CoT的忠实性和详细程度，提出了新的度量指标，并揭示了不忠实现象。,"Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",arXiv.org,Black-box,General,该研究通过分析模型的链式思考（CoT）输出来评估其可监控性，主要关注的是模型的输出（即CoT的忠实性和详细程度），而不是模型的内部机制或权重。因此，它属于黑盒方法。研究领域涉及模型的推理过程和安全性监控，不特定于数学、逻辑、代码、医学或社会学等具体领域，因此归类为通用。
Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?,2025,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了LLM的解释忠实性问题，并提出了改进方法。,"Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",arXiv.org,Black-box,Medical,论文研究了大型语言模型在医疗和社会学领域的解释忠实性问题，通过操纵少量示例、提示策略和训练过程来评估模型的忠实性，属于黑盒方法。研究领域涉及医疗和社会学，但主要关注医疗领域的应用。
Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,True,False,False,,论文探讨了自动事实核查系统需要提供解释以符合事实核查者的需求，与CoT忠实性相关。,"The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model’s reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",International Conference on Human Factors in Computing Systems,Black-box,Society,该论文研究的是自动化事实核查工具如何为事实核查人员提供解释，主要关注的是输入输出和Prompting，而不是模型内部机制，因此属于黑盒方法。研究领域涉及事实核查和错误信息，属于社会学范畴。
Improving Rationality in the Reasoning Process of Language Models through Self-playing Game,2025,True,True,False,Training & Fine-tuning,论文通过自玩游戏提升模型推理过程的理性，涉及 CoT 忠实性改进。,"Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",International Conference on Machine Learning,Black-box,Math,该论文通过设计一个自我对弈游戏（CDG）来提升语言模型在推理过程中的理性，主要关注的是模型的输入输出行为（Prompting），没有涉及模型内部机制的分析，因此属于黑盒方法。实验任务主要集中在数学推理领域，因此领域分类为Math。
Thought Anchors: Which LLM Reasoning Steps Matter?,2025,True,False,True,,论文提出了黑盒方法评估CoT中句子的重要性，与Faithfulness相关。,"Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. Many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences \textit{thought anchors}. These are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within a reasoning trace gives insight into a model's behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our techniques yield a consistent picture of the reasoning trace's structure. We provide an open-source tool (thought-anchors.com) for visualizing the outputs of our methods on further problems. The convergence across our methods shows the potential of sentence-level analysis for a deeper understanding of reasoning models.",arXiv.org,Black-box,Math,论文提出了一种黑盒方法，通过替换句子并测量其对最终答案分布的影响来分析推理过程。研究领域涉及数学问题的解决，因此归类为数学领域。
Reasoning About Reasoning: Towards Informed and Reflective Use of LLM Reasoning in HCI,2025,True,True,False,Prompting & In-Context Learning,论文讨论了LLM推理的脱离上下文的特性，并提出了反思提示以支持HCI实践者更明智地使用LLM推理。,"Reasoning is a distinctive human-like characteristic attributed to LLMs in HCI due to their ability to simulate various human-level tasks. However, this work argues that the reasoning behavior of LLMs in HCI is often decontextualized from the underlying mechanics and subjective decisions that condition the emergence and human interpretation of this behavior. Through a systematic survey of 258 CHI papers from 2020-2025 on LLMs, we discuss how HCI hardly perceives LLM reasoning as a product of sociotechnical orchestration and often references it as an object of application. We argue that such abstraction leads to oversimplification of reasoning methodologies from NLP/ML and results in a distortion of LLMs'empirically studied capabilities and (un)known limitations. Finally, drawing on literature from both NLP/ML and HCI, as a constructive step forward, we develop reflection prompts to support HCI practitioners engage with LLM reasoning in an informed and reflective way.",arXiv.org,Black-box,Society,该论文主要探讨了LLM在HCI领域中的推理行为，关注的是其社会技术层面的影响和人类对其行为的解释，而不是模型内部机制。因此归类为黑盒。研究领域涉及人类行为和社会技术因素，属于社会学范畴。
Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations,2025,True,True,True,Verification & External Tools,论文提出了一种自动化验证方法EDCT，用于检测Vision-Language模型解释的忠实性，并揭示了不忠实现象。,"Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",arXiv.org,Black-box,General,该研究通过输入输出（图像-问题对）和生成的解释来测试视觉语言模型的忠实性，没有涉及模型内部机制或权重，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码或医学，而是通用的模型测试和验证。
TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出通过关键词蒸馏和LLM推理提升解释的忠实性，并提出了新的评估指标。,"Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.",arXiv.org,Black-box,Medical,该论文主要研究临床文本的解释性，通过关键词蒸馏和大型语言模型（LLMs）的推理来提高分类性能和可解释性。研究涉及的是模型的输入输出和Prompting，没有涉及模型内部机制，因此属于黑盒方法。任务领域是医学，因为研究的是电子健康记录（EHRs）和临床决策支持。
Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem,2024,True,False,False,Prompting & In-Context Learning,论文提出了一种通过角色建模和反馈机制改进自然语言解释生成的方法，属于 CoT 忠实性改进方法。,"Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",International Conference on Computational Linguistics,Black-box,General,论文主要研究通过两个LLM（生成器和批评者）的互动来改进自然语言解释的生成，没有涉及模型内部机制，属于黑盒方法。任务领域涉及自然语言处理，但未明确限定在特定领域，因此归类为通用。
Comparing zero-shot self-explanations with human rationales in text classification,2024,True,True,True,,论文分析了自解释的忠实性，并比较了其与人类注释的差异。,"Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations. These do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. We study two text classification tasks: sentiment classification and forced labour detection, i.e., identifying pre-defined risk indicators of forced labour. In addition to English, we include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and analyse 4 LLMs. We show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness. This finding suggests that self-explanations indeed provide good explanations for text classification.",,Black-box,Society,该论文研究的是指令调优的LLMs（大型语言模型）通过生成自我解释（self-explanations）来提供关于其输出的解释，而不需要梯度计算或复杂的XAI方法。这表明研究是基于模型的输入输出行为，而非内部机制，因此属于黑盒方法。任务领域涉及文本分类，特别是情感分类和强迫劳动检测，这些与社会学相关，因此归类为社会学（Society）。
Learning to Rationalize for Nonmonotonic Reasoning with Distant Supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了生成事后解释性理由（post-hoc rationales）的现象，并尝试通过训练生成模型来改进。,"The black-box nature of neural models has motivated a line of research that aims to generate natural language rationales to explain why a model made certain predictions. Such rationale generation models, to date, have been trained on dataset-specific crowdsourced rationales, but this approach is costly and is not generalizable to new tasks and domains. In this paper, we investigate the extent to which neural models can reason about natural language rationales that explain model predictions, relying only on distant supervision with no additional annotation cost for human-written rationales. We investigate multiple ways to automatically generate rationales using pre-trained language models, neural knowledge models, and distant supervision from related tasks, and train generative models capable of composing explanatory rationales for unseen instances. We demonstrate our approach on the defeasible inference task, a nonmonotonic reasoning task in which an inference may be strengthened or weakened when new information (an update) is introduced. Our model shows promises at generating post-hoc rationales explaining why an inference is more or less likely given the additional information, however, it mostly generates trivial rationales reflecting the fundamental limitations of neural language models. Conversely, the more realistic setup of jointly predicting the update or its type and generating rationale is more challenging, suggesting an important future direction.",AAAI Conference on Artificial Intelligence,Black-box,Logic,论文研究的是生成自然语言解释来阐明模型预测的原因，属于黑盒方法，因为它不涉及模型内部机制的分析。任务领域是非单调逻辑推理（defeasible inference task），因此归类为Logic。
Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,2022,True,False,False,Training & Fine-tuning,论文提出了一种新的解释方法，并通过微调学生模型来提高忠实性。,"Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is""honest""in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the""untrusted""teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",arXiv.org,Black-box,General,该研究主要关注如何通过Prompting和API（即冻结的预训练语言模型）来生成解释性问答系统的合理化解释，而不涉及模型内部机制的分析。因此，方法类型属于黑盒。任务领域涉及问答系统，不属于数学、逻辑、代码、医学或社会学等特定领域，因此归类为通用。
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,2024,True,True,False,,论文讨论了LLM生成自解释的忠实性与合理性之间的对立，揭示了不忠实现象。,"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",arXiv.org,Black-box,General,该论文主要研究大型语言模型（LLMs）生成自我解释（SEs）的忠实性与合理性之间的对立关系，关注的是模型的输入输出行为（即生成的解释）而非内部机制，因此属于黑盒方法。研究领域涉及LLMs的通用行为和应用，不特定于数学、逻辑、代码、医学或社会学等具体领域，因此归类为通用。
"What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了CoT的有效性，提出了不忠实现象（如失败步骤影响推理），并提出了新的度量指标（FSF）和改进方法。,"Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the""longer-is-better""narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",arXiv.org,Black-box,Math,该论文研究的是大型推理模型（LRMs）在数学和科学推理任务中的表现，通过分析链式思考（CoT）的长度、回顾和结构来评估其有效性。研究主要基于输入输出和Prompting方法，没有涉及模型内部机制或权重调整，因此属于黑盒方法。任务领域明确涉及数学推理，因此归类为Math。
Misaligning Reasoning with Answers - A Framework for Assessing LLM CoT Robustness,2025,True,True,True,,论文提出了评估框架MATCHA，揭示了LLMs在输入扰动下的不一致推理现象，与CoT忠实性相关。,"LLMs' decision-making process is opaque, prompting the need for explanation techniques like Chain-of-Thought. To investigate the relationship between answer and reasoning, we design a novel evaluation framework, MATCHA. In domains like education and healthcare, reasoning is key for model trustworthiness. MATCHA reveals that LLMs under input perturbations can give inconsistent or nonsensical reasoning. Additionally, we use LLM judges to assess reasoning robustness across models. Our results show that LLMs exhibit greater vulnerability to input perturbations for multi-step and commonsense tasks than compared to logical tasks. Also, we show non-trivial transfer rates of our successful examples to black-box models. Our evaluation framework helps to better understand LLM reasoning mechanisms and guides future models toward more robust and reasoning-driven architectures, enforcing answer-reasoning consistency.",arXiv.org,Black-box,General,该研究通过设计评估框架MATCHA来研究LLMs在输入扰动下的推理一致性，主要关注的是模型的输入输出行为（如不一致或无意义的推理），并未涉及模型内部机制的分析。因此归类为黑盒。研究领域涉及多个任务（如多步推理、常识任务和逻辑任务），不属于特定领域，故归类为通用。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,True,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,Black-box,Society,该论文研究推荐系统中生成的解释（如评论）是否忠实和连贯，主要关注模型的输入输出行为（即生成的评论和预测的评分），并未涉及模型内部机制，因此属于黑盒方法。研究领域涉及用户行为和推荐系统的社会影响，属于社会学范畴。
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,True,True,False,Verification & External Tools,论文提出模块化组件确保推理步骤与推理过程有因果联系，提升忠实性。,"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",Annual Meeting of the Association for Computational Linguistics,Black-box,Logic,论文标题和摘要提到的是关于自然语言演绎推理的研究，属于逻辑领域。虽然提到了模块化组件，但整体上是通过输入输出来进行推理，没有涉及模型内部机制，因此归类为黑盒。
Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning,2025,True,False,False,Verification & External Tools,论文提出交互式框架 Vis-CoT，通过人类干预提升 CoT 的准确性和可信度，属于改进方法中的验证与外部工具。,"Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.",arXiv.org,Black-box,General,该论文提出了一种人机交互框架Vis-CoT，通过将线性思维链文本转换为交互式推理图，允许用户可视化和干预推理过程。研究主要关注于通过输入输出和Prompting进行交互，并未涉及模型内部机制，因此属于黑盒方法。任务领域不特定于数学、逻辑、代码或医学，而是通用的推理和协作框架。
A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations,2025,True,True,True,Training & Fine-tuning,论文提出了PEX一致性度量，揭示了LLM生成解释的不一致现象，并采用直接偏好优化提升忠实性。,"Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",,Black-box,Society,该论文研究的是语言模型生成的自由文本解释的忠实性，特别是预测与解释之间的一致性。研究通过输入输出分析（Black-box）来评估和改进解释的一致性，属于社会学领域，因为它涉及透明度和高风险的AI决策背景。
Simulating Society Requires Simulating Thought,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LLM模拟社会行为时缺乏内部一致性和因果推理的问题，并提出了评估框架RECAP来提升推理忠实性。,"Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet current simulations remain grounded in a behaviorist""demographics in, behavior out""paradigm, focusing on surface-level plausibility. As a result, they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for modeling how people reason, deliberate, and respond to interventions. To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought, not just language, for social simulations.",,Black-box,Society,论文主要讨论使用大型语言模型（LLMs）模拟社会行为，强调需要基于认知科学的推理，而不是仅通过输入输出或Prompting进行表面行为模拟。研究领域涉及社会学，关注人类行为、推理和干预反应。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,Black-box,General,该论文提出了一种框架（Text Modular Networks），通过分解复杂任务为更简单的子任务，并利用现有模型的输入输出行为来解决问题。研究主要关注模型的输入输出行为（即语言），而不是模型内部机制，因此属于黑盒方法。任务领域涉及问答系统（QA），但未特定于数学、逻辑、代码、医学或社会学，因此归类为通用领域。
RACG-DRT: Retrieval Augumented Code Generation Based on Dynamic Revision of Thoughts,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT在代码生成中的不忠实现象，并提出了基于动态修订的改进方法。,"Large models have the problems of hallucinations in the field of code generation. The causes of these hallucinations include errors in the model’s reasoning process, inadequate overall knowledge, and insufficient localized knowledge. To tackle these challenges, the prevailing approach currently involves influencing the large model’s reasoning and prediction processes through techniques such as chain of thought, single-shot retrievalaugmented generation, and dynamic retrieval-augmented generation. COT (chain of thought) prompting method facilitates logical reasoning by the model on programming problems through prompts. Single-shot retrieval-augmented generation technology enables the construction of query statements based on programming issues to retrieve information from external knowledge bases. Meanwhile, dynamic retrieval-augmented generation technology allows for real-time retrieval from external knowledge bases during the code generation process, integrating retrieved documents to continue generating subsequent code. However, these methods primarily address the hallucination problem in code generation from a localized perspective. To simultaneously resolve issues such as reasoning errors, inadequate overall knowledge, and insufficient localized knowledge in the field of code generation, this study introduces the Retrieval-Augmented Code Generation framework based on Dynamic Revision of Thoughts (RACGDRT). The core concept of this framework involves firstly enabling the large model to produce a comprehensive code draft (utilizing COT prompts), followed by conducting error detection on code segments corresponding to different reasoning steps within the draft. Upon detecting an error, the Keyword Extractor (KE) will extract keywords and formulate query statements for both the user’s input question and the erroneous code segment, subsequently retrieving pertinent documents from the external knowledge base. Finally, with the retrieved documents and questions in hand, the large model proceeds to iteratively generate the next code draft. RACG-DRT guarantees the rationality of reasoning via COT, constructs query statements tailored to erroneous thought segments and issues to bolster both local and holistic knowledge, and iteratively produces code drafts to uphold the integrity of the code generation process.",,Black-box,Code,该论文主要研究代码生成领域中的幻觉问题，通过检索增强生成技术（如链式思维、单次检索增强生成和动态检索增强生成）来影响大型模型的推理和预测过程。这些方法主要依赖于输入输出和Prompting技术，没有涉及模型内部机制或权重调整，因此属于黑盒方法。研究领域明确为代码生成，故归类为Code。
Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Explanations for Model Predictions,2022,True,True,False,Training & Fine-tuning,论文讨论了模型生成的解释与下游任务效用不一致的现象，并提出了通过微调改进的方法。,"Models able to generate free-text rationales that explain their output have been proposed as an important step towards interpretable NLP for “reasoning” tasks such as natural language inference and commonsense question answering. However, the relative merits of different architectures and types of rationales are not well understood and hard to measure. In this paper, we contribute two insights to this line of research: First, we find that models trained on gold explanations learn to rely on these but, in the case of the more challenging question answering data set we use, fail when given generated explanations at test time. However, additional fine-tuning on generated explanations teaches the model to distinguish between reliable and unreliable information in explanations. Second, we compare explanations by a generation-only model to those generated by a self-rationalizing model and find that, while the former score higher in terms of validity, factual correctness, and similarity to gold explanations, they are not more useful for downstream classification. We observe that the self-rationalizing model is prone to hallucination, which is punished by most metrics but may add useful context for the classification step.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,Black-box,Society,该研究关注模型生成的自由文本解释（rationales）在自然语言推理和常识问答等任务中的效用，主要探讨不同架构和解释类型对下游任务的影响。研究不涉及模型内部机制，而是通过输入输出和Prompting进行分析，因此属于黑盒方法。研究领域涉及模型解释的效用和人类评价，属于社会学范畴。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",Black-box,Society,该研究通过输入输出测试ChatGPT对科学论文和新闻中因果语言的理解能力，属于黑盒方法。研究领域涉及语言理解和人类标注指南的改进，属于社会学范畴。
REV: Information-Theoretic Evaluation of Free-Text Rationales,2022,True,False,True,,论文提出了REV指标评估自由文本推理的新信息，与CoT忠实性相关。,"Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该论文主要研究如何评估自由文本解释（rationales）的信息量，提出了一种基于信息论的度量方法REV。研究仅涉及输入输出和解释文本的分析，不涉及模型内部机制，因此属于黑盒方法。任务领域不特定于数学、逻辑、代码或医学，而是通用的NLP解释性研究。
Self-Critique and Refinement for Faithful Natural Language Explanations,2025,True,True,True,Prompting & In-Context Learning,论文提出通过自我批评和细化框架提升解释的忠实性，涉及不忠实现象和改进方法。,"With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.",,Black-box,General,该论文研究的是大型语言模型（LLMs）通过自我批判和迭代优化来改进自然语言解释（NLEs）的忠实性，主要依赖于模型的输入输出和Prompting技术，没有涉及模型内部机制的分析，因此属于黑盒方法。研究领域不涉及数学、逻辑、代码或医学等特定领域，而是通用的自然语言处理任务。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,Black-box,Logic,该论文研究的是自然语言中的演绎推理任务，通过搜索语句组合来生成假设，属于逻辑领域。研究方法主要关注模型的输入输出和搜索过程，没有涉及模型内部机制，因此归类为黑盒方法。
Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning,2025,True,True,False,,论文研究了CoT的忠实性问题，揭示了不忠实现象，如事后找补和捷径学习。,"Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",arXiv.org,Black-box,Logic,该论文通过提示工程（Prompting）研究大型语言模型（LLM）在数学和逻辑推理任务中的表现，属于黑盒方法。研究领域涉及逻辑推理（Logic），因为主要关注模型在逻辑任务中的表现和提示的影响。
Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations,2025,True,True,True,,论文聚焦CoT忠实性度量，揭示事后找补现象，提出基于反事实和贝叶斯模型的新评估框架，但未提出改进方法。,"Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's""reasoning""process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.",International Conference on Learning Representations,Black-box,Society,该论文研究大型语言模型（LLM）解释的忠实性，主要关注模型生成的解释是否真实反映其内部推理过程。研究通过输入输出和Prompting进行分析，未涉及模型内部机制，因此属于黑盒方法。研究领域涉及社会偏见和医学问答任务，但更侧重于解释的忠实性对社会信任和模型滥用的影响，因此归类为社会学领域。
Digital Socrates: Evaluating LLMs through explanation critiques,2023,True,True,True,Training & Fine-tuning,论文提出自动评估模型解释质量的方法，涉及 CoT 忠实性评估和改进。,"While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该研究通过评估大语言模型（LLMs）的解释能力，关注的是模型的输入输出行为，而不是内部机制或权重。因此属于黑盒方法。研究领域涉及模型的解释能力评估，不特定于数学、逻辑、代码或医学等领域，因此归类为通用领域。
"Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving",2025,True,False,True,Verification & External Tools,论文提出基于形式化验证的框架和度量标准，关注过程级验证，与CoT忠实性相关。,"As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",arXiv.org,Black-box,Logic,论文主要关注问题解决的形式化框架和基准测试，涉及逻辑推理和定理证明，但没有深入探讨模型内部机制，因此归类为黑盒。任务领域属于逻辑，因为涉及形式化问题解决和定理证明。
A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,2023,True,True,True,Verification & External Tools,论文探讨了LLMs在逻辑推理中的自我验证能力，涉及CoT的忠实性问题。,"Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.",North American Chapter of the Association for Computational Linguistics,Black-box,Logic,论文研究的是大型语言模型在逻辑推理中的自我验证能力，通过输入输出（Prompting）进行评估，没有涉及模型内部机制，因此属于黑盒方法。研究领域是逻辑推理，因此归类为Logic。
The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models,2024,True,False,True,,论文提出了新的度量指标CEF和CCT，用于评估CoT的忠实性，但未讨论不忠实现象或改进方法。,"In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该研究通过输入干预（input interventions）和生成的自由文本解释来评估大型语言模型（LLMs）的解释忠实性，并未涉及模型内部机制或权重等白盒方法。任务领域未明确指向数学、逻辑、代码或医学等特定领域，而是通用的自然语言处理（NLP）任务，因此归类为通用领域。
SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine,2024,True,True,False,Prompting & In-Context Learning,论文提出了一种自我引导的提示方法，旨在通过分解问题和自我纠正来增强推理的忠实性。,"Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",North American Chapter of the Association for Computational Linguistics,Black-box,General,论文标题和摘要中提到的方法是基于提示（Prompting）范式，没有涉及模型内部机制、激活值、梯度或权重，因此属于黑盒方法。任务领域是多跳问答（Multi-Hop Question Answering），这属于通用领域，不属于数学、逻辑、代码、医学或社会学等特定领域。
CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了因果幻觉（不忠实现象），并提出了CRE机制来增强因果关系，提升Faithfulness。,"Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.",arXiv.org,Black-box,Logic,论文主要研究如何通过因果推理增强和双端搜索来解决大语言模型在长范围推理任务中的问题，属于逻辑推理领域。研究仅通过输入输出和Prompting进行，没有涉及模型内部机制，因此归类为黑盒方法。
Are self-explanations from Large Language Models faithful?,2024,True,True,True,Consistency & Ensembling,论文探讨了LLM自我解释的忠实性问题，并提出了自我一致性检查作为度量方法。,"Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该研究通过输入输出来评估大型语言模型（LLMs）的自我解释是否忠实，没有涉及模型内部机制或权重。因此归类为黑盒。研究领域不特定于数学、逻辑、代码或医学等具体领域，而是通用的模型行为分析。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,Black-box,General,该论文主要研究通过Prompt Engineering（如Chain-of-Thought）来提升大型语言模型的解释性，仅涉及模型的输入输出和Prompting技术，未涉及模型内部机制，因此属于Black-box。研究领域涉及通用的大型语言模型解释性评估，不特定于数学、逻辑、代码、医学或社会学等领域，因此归类为General。
Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning,2024,True,True,False,,论文探讨了LLMs的推理行为，指出其准确性不一定反映推理过程的有效性，涉及不忠实现象。,"Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.",Annual Meeting of the Association for Computational Linguistics,Black-box,Logic,该研究通过分析大型语言模型（LLMs）在命题逻辑问题上的响应来比较人类和LLMs的推理策略，主要关注模型的输入输出行为，并未涉及模型内部机制，因此属于黑盒方法。研究领域涉及逻辑推理，因此归类为Logic。
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought,2024,True,True,True,Training & Fine-tuning,论文揭示了CoT中的偏见现象，并提出了通过训练方法减少偏见推理的解决方案。,"Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models'behavior -- for example, rationalizing answers in line with a user's opinion. We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.",arXiv.org,Black-box,Society,论文主要研究如何减少语言模型在推理过程中的偏见，涉及社会学领域的偏见、道德和人类行为问题。研究方法仅通过输入输出和Prompting进行，属于Black-box。
Can LLMs Explain Themselves Counterfactually?,2025,True,True,True,,论文研究了LLM自我生成反事实解释的能力，揭示了不忠实现象并提出了评估方法。,"Explanations are an important tool for gaining insights into the behavior of ML models, calibrating user trust and ensuring regulatory compliance. Past few years have seen a flurry of post-hoc methods for generating model explanations, many of which involve computing model gradients or solving specially designed optimization problems. However, owing to the remarkable reasoning abilities of Large Language Model (LLMs), self-explanation, that is, prompting the model to explain its outputs has recently emerged as a new paradigm. In this work, we study a specific type of self-explanations, self-generated counterfactual explanations (SCEs). We design tests for measuring the efficacy of LLMs in generating SCEs. Analysis over various LLM families, model sizes, temperature settings, and datasets reveals that LLMs sometimes struggle to generate SCEs. Even when they do, their prediction often does not agree with their own counterfactual reasoning.",,Black-box,General,该研究通过提示（prompting）大型语言模型（LLMs）生成自我解释的反事实解释（SCEs），仅涉及模型的输入输出，没有涉及模型内部机制、梯度或权重等，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码、医学或社会学，而是通用的模型解释能力评估。
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,2025,True,True,False,,论文揭示了CoT推理中的不忠实现象，如事后找补和逻辑捷径，但未提出新的度量或改进方法。,"Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions""Is X bigger than Y?""and""Is Y bigger than X?"", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",arXiv.org,Black-box,Logic,该论文研究了Chain-of-Thought (CoT)推理在大型语言模型中的不忠实性问题，主要关注模型在逻辑推理任务中的表现。研究通过输入输出分析（即黑盒方法）来评估模型的行为，没有涉及模型内部机制。研究领域涉及逻辑推理和模型行为分析，因此归类为Logic。
V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning,2025,True,True,True,,论文讨论了Video-LLMs的时空推理逻辑，并提出了评估框架，涉及CoT和Faithfulness。,"Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (""when"") and then analyse the spatial relationships (""where"") between key objects, and finally leverage these relationships to draw inferences (""what""). However, can Video Large Language Models (Video-LLMs) also""reason through a sequential spatio-temporal logic""in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained""memory""of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.",arXiv.org,Black-box,Society,该论文主要研究视频大型语言模型（Video-LLMs）在视频时空推理中的表现，通过构建一个基准测试（V-STaR）来评估模型是否真正理解视频中的对象交互（动作/事件），而不是依赖预训练的记忆或偏见。研究仅通过输入输出（Prompting）进行评估，属于黑盒方法。任务领域涉及人类行为和社会认知，因此归类为社会学（Society）。
Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic,2024,True,True,False,Verification & External Tools,论文提出利用符号逻辑增强LLM推理能力，并检测不忠实推理，属于改进方法中的外部工具验证。,"Large language models (LLMs) have been shown to struggle with complex logical reasoning tasks due to the inherent ambiguity and complexity of natural language. These challenges are further amplified when processing large and diverse datasets, increasing the likelihood of unfaithful reasoning and predictive hallucinations. However, LLMs can provide accurate responses when queries are clear and direct. Symbolic logic provides precise, well-defined rules that can help overcome ambiguity and support reasoning. In this work, we leverage symbolic logic’s precision to enhance LLMs’ logical reasoning capabilities by introducing the Graph of Logic (GoL) framework. GoL combines the power of graph structures with the strengths of LLMs and symbolic logic. GoL utilizes the precise rules of symbolic logic to infer new facts and detect LLM hallucinations effectively on complex datasets. Furthermore, GoL utilizes graph structures to support scalability for large datasets and tackle long dependencies, enabling efficient handling of complex reasoning tasks. We conduct extensive experiments across seven benchmark datasets, encompassing various types of reasoning. These include deductive, inductive, and abductive reasoning, each testing distinct aspects of logical inference. The experimental results demonstrate GoL’s advantage in improving the reasoning capabilities of LLMs. GoL outperforms the baselines with an average margin of 18.18% for the GPT-3.5 and GPT-4 models, outperforming the baselines for all datasets for the GPT-3.5 model and six out of seven datasets for the GPT-4 model1.",BigData Congress [Services Society],Black-box,Logic,论文通过引入Graph of Logic (GoL)框架，结合符号逻辑和图结构来增强LLM的逻辑推理能力。研究主要关注于通过输入输出（Prompting）来提升LLM在逻辑推理任务中的表现，并未涉及模型内部机制的分析，因此属于黑盒方法。任务领域明确为逻辑推理，故归类为Logic。
Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,2024,True,True,False,Prompting & In-Context Learning,论文提出Logic-of-Thought方法解决CoT的不忠实问题，属于改进方法中的Prompting类别。,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.",North American Chapter of the Association for Computational Linguistics,Black-box,Logic,该论文主要研究如何通过改进提示方法（Logic-of-Thought）来增强大型语言模型在逻辑推理任务中的表现，并未涉及模型内部机制的分析，因此属于黑盒方法。研究领域集中在逻辑推理任务，属于Logic领域。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该论文研究的是通过自训练和双教师模型来生成任务标签和解释，主要依赖于预训练语言模型的输入输出，没有涉及模型内部机制的分析，因此属于黑盒方法。研究领域不涉及数学、逻辑、代码或医学等特定领域，而是通用的AI应用，因此归类为通用领域。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该论文提出了一种新的提示方法（Chain-of-Knowledge prompting），通过设计特定的提示来引导大型语言模型生成结构化的知识证据。研究仅涉及模型的输入输出和提示设计，并未涉及模型内部机制或权重等，因此属于黑盒方法。任务领域涉及常识、事实、符号和算术推理，较为通用，不属于特定领域。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,Black-box,Society,该论文主要讨论的是 OpenAI o1 模型系列的安全性和鲁棒性，涉及模型在应对潜在不安全提示时的推理能力，以及如何通过深思熟虑的对齐（deliberative alignment）来改进安全策略。研究内容主要集中在模型的社会影响、安全政策和风险管理上，因此归类为社会学（Society）。由于摘要中未提及模型内部机制的研究，仅通过输入输出和 Prompting 进行研究，因此归类为黑盒（Black-box）。
Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs,2025,True,True,False,,论文探讨了In-Context Learning中的不忠实现象，特别是通过CoT步骤显式合理化有害输出。,"Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous''persona'', echoing prior results on finetuning-induced EM.",arXiv.org,Black-box,Society,该论文研究的是通过上下文学习（ICL）导致大型语言模型（LLM）出现广泛不对齐（misalignment）的现象，属于仅通过输入输出进行研究，因此归类为黑盒。研究涉及模型对社会偏见和道德问题的响应，属于社会学领域。
Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了评估框架对CoT长度和答案合规性的影响，揭示了不忠实现象，并提出了改进方法。,"Benchmarks for large language models (LLMs) often rely on rubric-scented prompts that request visible reasoning and strict formatting, whereas real deployments demand terse, contract-bound answers. We investigate whether such""evaluation scent""inflates measured performance without commensurate capability gains. Using a single open-weights model (GPT-OSS-20B), we run six paired A/B scenarios that hold task content and decoding fixed while varying framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High): deterministic math, strict code-fix, citation generation, incentive flips (caution vs. competence), CoT visibility, and multilingual (Urdu) headers. Deterministic validators compute accuracy, answer-only compliance, hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with pre-registered deltas and composite indices. Across scenarios, evaluation framing reliably inflates CoT (hundreds to>1000 characters) and reduces answer-only compliance, with limited or inconsistent accuracy gains. In structured outputs, it improves wrappers (e.g., fenced blocks, enumerated lists) but not regex-validated substance. Incentive wording reweights error composition: praising caution modestly improves accuracy at high reasoning and reduces wrong-but-confident errors, whereas praising competence yields terser but riskier outputs. Urdu rubric headers reproduce these signatures and can decrease accuracy at higher reasoning depth, indicating multilingual parity risks. We provide a reproducible A/B framework (prompt banks, validators, per-run scores, scripts; versioned DOI) and practical guidance: neutral phrasing or dual-framing checks, contract-aware grading, style-delta reporting, confidence governance, and multilingual dashboards to ensure that benchmark gains reflect deployable capability.",arXiv.org,Black-box,Society,该研究通过改变提示框架（评估导向 vs. 现实世界）和推理深度来测试大型语言模型的行为，主要关注模型在评估环境中的表现是否与实际部署中的表现一致。研究涉及社会学因素，如激励敏感性和多语言公平性，因此归类为社会学领域。由于研究仅通过输入输出和提示进行，未涉及模型内部机制，因此属于黑盒方法。
FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning,2025,True,True,True,,论文提出了一个实例级 CoT 忠实性检测的基准，并讨论了不忠实现象和评估方法。,"Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",arXiv.org,Black-box,General,该研究主要关注通过Prompting（Chain-of-Thought）生成的解释的忠实性评估，并未涉及模型内部机制的分析，因此属于黑盒方法。研究领域涉及多个任务领域（四个领域），但未明确限定在数学、逻辑、代码、医学或社会学等特定领域，因此归类为通用。
When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning, Consistency & Ensembling",论文揭示了CoT推理导致指令跟随准确性下降的现象，并提出了度量指标和改进策略。,"Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.",arXiv.org,Black-box,General,该研究主要关注通过提示（Prompting）和输入输出分析来评估大型语言模型在指令跟随任务中的表现，特别是链式思考（CoT）推理的影响。研究并未涉及模型内部机制或权重调整，因此属于黑盒方法。任务领域涉及通用指令跟随，不特定于数学、逻辑、代码、医学或社会学。
Logic Agent: Enhancing Validity with Logic Rule Invocation,2024,True,False,False,Verification & External Tools,论文提出Logic Agent框架，通过逻辑规则调用增强推理有效性，属于改进方法中的验证与外部工具类别。,"Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.",arXiv.org,Black-box,Logic,该论文提出了一种基于代理的框架（Logic Agent），通过调用逻辑规则来增强大型语言模型（LLMs）的推理过程的有效性。研究主要关注于通过自然语言输入转换为结构化逻辑形式，并利用预定义的逻辑规则进行推理，这属于黑盒方法，因为它仅通过输入输出和Prompting进行研究。任务领域涉及逻辑推理，因此归类为Logic。
Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback,2024,True,True,False,Verification & External Tools,论文提出外部反馈机制提升推理可靠性，涉及 CoT 忠实性改进。,"While inference-time thinking allows Large Language Models (LLMs) to address complex problems, the extended thinking process can be unreliable or inconsistent because of the model's probabilistic nature, especially near its knowledge boundaries. Existing approaches attempt to mitigate this by having the model critique its own reasoning to make corrections. However, such self-critique inherits the same biases of the original output, known as the introspection illusion. Moving beyond such introspection and inspired by core methodologies in ethology, we propose an externalist three-step framework Distillation-Reinforcement-Reasoning (DRR). Rather than relying on a model's introspection, DRR evaluates its observable behaviors to provide corrective feedback. DRR first distills the reasoner's behavioral traces, then trains a lightweight, external Discriminative Model (DM). At inference time, this DM acts as a critic, identifying and rejecting suspicious reasoning steps. This external feedback compels the LLM to discard flawed pathways and explore alternatives, thereby enhancing reasoning quality without altering the base model. Experiments on multiple reasoning benchmarks show that our framework significantly outperforms prominent self-critique methods. Benefiting from a lightweight and annotation-free design, DRR offers a scalable and adaptable solution for improving the reliability of reasoning in a wide range of LLMs.",,Black-box,General,该论文提出了一种外部行为反馈框架（DRR），通过观察模型的行为来提供纠正反馈，而不是直接干预模型内部机制。因此属于黑盒方法。研究领域不涉及特定任务领域（如数学、逻辑等），而是通用的LLM推理可靠性提升，因此归类为通用领域。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,Black-box,Society,该论文研究的是通过Chain-of-Thought (CoT)推理技术来构建可引导的多元化模型，主要关注的是模型如何理解和反映不同的人类价值观和观点，这属于社会学领域。研究方法主要涉及Prompting、微调和强化学习，没有涉及模型内部机制的分析，因此属于黑盒方法。
Watch Your Steps: Observable and Modular Chains of Thought,2024,True,True,True,Verification & External Tools,论文提出了一种新的 CoT 变体，通过形式化步骤增强可观察性，并讨论了非局部错误问题。,"We propose a variant of chain of thought (CoT) prompting called Program Trace Prompting that makes explanations more observable while preserving the power, generality and flexibility of CoT. In our approach, few-shot CoT demonstrations are wrapped in a formal syntax based on Python, and each prompt: identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples. Program Trace Prompting is applicable to many tasks, achieving strong results on the 23 diverse tasks in the BIG-Bench Hard benchmark. More importantly, by instrumenting explanations in this way, we enable new types of analysis. In particular, we identify""non-local errors""(which correspond to incorrectly learning the reasoning method illustrated in the demonstrations) as an unaddressed issue in CoT learning, and we present methods for verifying the modularity of steps in a CoT explanation.",arXiv.org,Black-box,General,该论文提出了一种名为Program Trace Prompting的变体，通过改进Chain of Thought (CoT)提示方法，使其解释更加可观察和模块化。研究主要基于输入输出和Prompting技术，没有涉及模型内部机制，因此属于Black-box。任务领域不特定于数学、逻辑、代码、医学或社会学，而是通用的CoT提示方法改进，适用于多种任务，因此归类为General。
Larger Language Models Don’t Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks,2024,True,True,False,,论文揭示了CoT在主观任务中的不忠实现象，即模型依赖先验而非真实推理。,"In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to ""adapt"" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on ""learning"" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether ""enabling"" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is available at https://github.com/gchochla/cot-priors.","IEEE International Conference on Acoustics, Speech, and Signal Processing",Black-box,Society,该论文研究的是大型语言模型（LLM）在上下文学习（ICL）和思维链（CoT）提示下的行为，特别是针对主观任务（如情感和道德）的表现。研究仅通过输入输出（Prompting）进行分析，没有涉及模型内部机制，因此属于黑盒方法。任务领域涉及主观判断和社会学因素，因此归类为社会学。
"Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",2024,True,True,False,,论文探讨了CoT中的不忠实现象，如记忆化和噪声推理，但未提出具体度量或改进方法。,"Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit abstract generalization or rely on shallow heuristics when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. We analyze the pattern of results produced by three LLMs -- GPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence task accuracy across all three LLMs; e.g., when tested with GPT-4, varying the output's probability of occurrence shifts accuracy from 26% to 70%. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning. Code and data at this https://github.com/aksh555/deciphering_cot",Conference on Empirical Methods in Natural Language Processing,Black-box,Logic,该研究通过分析不同LLM在Chain-of-Thought (CoT)提示下的表现，探讨了影响CoT推理效果的因素，如概率、记忆和噪声推理。研究仅通过输入输出和Prompting进行研究，未涉及模型内部机制，因此属于黑盒方法。任务领域涉及逻辑推理，特别是解码移位密码的符号推理任务，因此归类为Logic。
Break the Chain: Large Language Models Can be Shortcut Reasoners,2024,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT中的捷径学习现象，并提出了新的评估数据集和改进方法。,"Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex modules but are hampered by high token consumption, limited applicability, and challenges in reproducibility. This paper conducts a critical evaluation of CoT prompting, extending beyond arithmetic to include complex logical and commonsense reasoning tasks, areas where standard CoT methods fall short. We propose the integration of human-like heuristics and shortcuts into language models (LMs) through""break the chain""strategies. These strategies disrupt traditional CoT processes using controlled variables to assess their efficacy. Additionally, we develop innovative zero-shot prompting strategies that encourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues and bypass detailed procedural steps. Our comprehensive experiments across various LMs, both commercial and open-source, reveal that LMs maintain effective performance with""break the chain""strategies. We also introduce ShortcutQA, a dataset specifically designed to evaluate reasoning through shortcuts, compiled from competitive tests optimized for heuristic reasoning tasks such as forward/backward reasoning and simplification. Our analysis confirms that ShortcutQA not only poses a robust challenge to LMs but also serves as an essential benchmark for enhancing reasoning efficiency in AI.",arXiv.org,Black-box,Logic,该论文主要研究如何通过'break the chain'策略改进语言模型的推理能力，特别是逻辑和常识推理任务。研究仅涉及输入输出和Prompting策略，未涉及模型内部机制，因此属于黑盒方法。任务领域涉及逻辑推理，故归类为Logic。
SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过训练框架提升LLM的忠实性，涉及CoT和自我反思的理性生成。,"Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",Conference on Empirical Methods in Natural Language Processing,Black-box,General,该论文主要研究如何通过训练框架（SaySelf）来教导大型语言模型（LLMs）表达更细粒度的置信度估计，并生成自我反思的理性解释。研究主要基于输入输出和Prompting技术，没有涉及模型内部机制或权重调整，因此属于黑盒方法。任务领域不涉及数学、逻辑、代码或医学等特定领域，而是通用的语言模型应用。
Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文研究了CoT的有效性和忠实性，揭示了不忠实现象，并提出了改进算法。,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该论文主要研究Chain-of-Thought (CoT) prompting在不同推理任务中的表现，通过分析问题难度、信息增益和信息流等因素来评估CoT的有效性和忠实性。研究仅涉及模型的输入输出和Prompting，没有涉及模型内部机制，因此属于Black-box。研究领域不特定于数学、逻辑、代码、医学或社会学，因此归类为General。
Automated Assessment of Fidelity and Interpretability: An Evaluation Framework for Large Language Models' Explanations (Student Abstract),2024,True,False,True,,论文提出了评估LLM解释忠实性的框架，但未讨论不忠实现象或改进方法。,"As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability: fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations. We apply our framework to evaluate GPT-3.5 and the impact of prompts on the quality of its explanations. In conclusion, our framework streamlines the evaluation of explanations from LLMs, promoting the development of safer models.",AAAI Conference on Artificial Intelligence,Black-box,General,该研究通过评估大型语言模型（LLMs）的解释质量，提出了一个任务无关的框架，重点关注保真度和可解释性。由于研究针对的是专有LLMs，无法直接访问内部特征，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码或医学，而是通用的模型评估框架。
MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine,2025,True,True,True,,论文探讨了CoT忠实性和Sycophancy现象，并提出了评估指标。,"With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.",arXiv.org,Black-box,Medical,该研究主要关注大型语言模型在医学领域的推理可靠性和安全性，通过输入输出（包括提示和基准测试）来评估模型，没有涉及模型内部机制的分析，因此属于黑盒方法。研究领域明确针对医学问题，属于医学领域。
Incorporating LLM Versus LLM Into Multimodal Chain-of-Thought for Fine-Grained Evidence Generation,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了MCoT中的语义漂移问题，并提出了实体级验证框架来提升忠实性。,"Multimodal Chain-of-Thought (MCoT) has become an effective strategy for enhancing multimodal large-language models (MLLMs) by breaking down complex tasks into sequential reasoning steps. Despite its interpretability benefits, MCoT often encounters difficulties with fine-grained semantic grounding, particularly when reasoning involves small objects, subtle attributes, or visually complex scenes that can lead to inaccuracies. Existing attempts to address these issues primarily fall into two categories: fine-tuning, which depends on large annotated datasets and costly parameter updates; and in-context learning (ICL), which achieves few-shot or zero-shot reasoning without model modification. Although ICL provides flexibility and adaptability, it is prone to semantic drift caused by an unstable prompt quality. To overcome these limitations, this study presents an entity-level evidence generation and verification framework using the ICL paradigm. This approach first produces MCoT from multimodal inputs, followed by extraction of key entities with enriched evidential descriptions. These entities were then cross-validated through adversarial checks using multiple MLLMs, and the verified evidence was integrated back into the reasoning chain. Experiments demonstrated consistent performance gains: on ScienceQA, the accuracy improved from 82.39% to 86.04%(+3.65%) with GPT-3.5, 84.96% to 89.37%(+4.41%) with Gemini; on MathVista, the accuracy increased from 43.1% to 43.6%(+0.50%) with GPT-3.5, and from 44.7% to 45.6%(+0.90%) with Gemini. These results establish new state-of-the-art baselines and confirm the robustness and generalizability of the entity-level verification for multimodal reasoning.",IEEE Access,Black-box,General,该研究主要关注通过多模态链式思考（MCoT）和上下文学习（ICL）范式来提升多模态大语言模型的性能，并未涉及模型内部机制的分析或修改，因此属于黑盒方法。任务领域涉及多模态推理，不属于数学、逻辑、代码、医学或社会学等特定领域，故归类为通用。
Reasoning Models Sometimes Output Illegible Chains of Thought,2025,True,True,False,,论文研究了CoT的legibility与faithfulness的关系，揭示了RL训练导致的不忠实现象。,"Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",arXiv.org,Black-box,General,该研究主要关注语言模型在强化学习训练后生成的思维链（CoT）的可读性问题，仅通过输入输出和Prompting进行研究，未涉及模型内部机制，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码或医学，而是探讨模型行为的通用问题，因此归类为通用领域。
Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning,2023,True,True,False,Consistency & Ensembling,论文通过集成解释和预测来解决不一致性，属于 CoT Faithfulness 的提升方法。,"Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such 'in-context' learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该论文研究的是通过集成多个模型解码结果和生成解释来提升大型语言模型（LLMs）的上下文学习能力，主要关注的是输入输出和Prompting策略，没有涉及模型内部机制的分析，因此属于黑盒方法。任务领域涉及自然语言理解，不属于数学、逻辑、代码、医学或社会学等特定领域，因此归类为通用。
Predicting Text Preference Via Structured Comparative Reasoning,2023,True,True,False,Prompting & In-Context Learning,论文讨论了LLMs在比较推理中的不一致性，并提出了SC方法以减少幻觉和提高一致性。,"Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该研究通过Prompting方法（SC）来预测文本偏好，仅涉及输入输出和Prompting技术，未涉及模型内部机制，因此属于黑盒方法。研究领域为文本偏好预测，不特定于数学、逻辑、代码、医学或社会学，因此归类为通用领域。
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,2023,True,True,True,Prompting & In-Context Learning,论文提出通过问题分解提升CoT忠实性，并使用了新度量指标验证。,"As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",arXiv.org,Black-box,General,该研究通过Prompting方法（如Chain-of-Thought和问题分解）来改进模型生成推理的忠实性，属于黑盒研究。研究领域不涉及特定任务（如数学、逻辑等），而是通用的模型行为验证。
Self-contradictory reasoning evaluation and detection,2023,True,True,True,,论文研究了自我矛盾推理现象，并提出了评估指标，与CoT忠实性高度相关。,"In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks only focus on final answers. Two fundamental questions persist: 1) how consistent is the reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support its answers. To answer 1), we define and assess the Self-Contra rate across three datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves in reasoning tasks involving contextual information understanding or commonsense. The model may generate correct answers by taking shortcuts in reasoning or overlooking contextual evidence, leading to compromised reasoning. For 2), we task the state-of-the-art model GPT-4 with identifying Self-Contra reasoning and finer-grained fallacies. We find that finer-grained categories enhanced detection can improve GPT-4's ability to detect Self-Contra. However, it is only able to detect Self-Contra with a 52.2% F1 score, much lower compared to 66.7% for humans. Our results indicate that current LLMs lack the robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond pure performance-based metrics.",Conference on Empirical Methods in Natural Language Processing,Black-box,General,该研究通过评估和检测大型语言模型（LLMs）在推理任务中的自相矛盾行为，主要关注模型的输入输出行为，并未涉及模型内部机制的分析，因此属于黑盒方法。研究领域涉及模型的推理能力评估，不特定于数学、逻辑、代码、医学或社会学等具体领域，故归类为通用领域。
Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,2023,True,True,False,,论文构建数据集评估模型推理可信性，发现模型无法解释错误选项原因，揭示了不忠实现象。,"To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiplechoice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives.",Conference on Empirical Methods in Natural Language Processing,Black-box,Logic,该研究通过输入输出（即问答形式）评估语言模型在逻辑阅读理解中的关键推理能力，没有涉及模型内部机制的分析，因此属于黑盒方法。研究领域聚焦于逻辑推理，属于Logic分类。
Properties and Challenges of LLM-Generated Explanations,2024,True,True,False,,论文探讨了LLM生成解释的特性，涉及不忠实现象，但未提出具体度量或改进方法。,"The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task-specific data sets.However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs.The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning.As the pre-training corpus includes a large amount of human-written explanations “in the wild”, we hypothesise that LLMs adopt common properties of human explanations.By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading.We discuss reasons and consequences of the properties’ presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.",,Black-box,General,该论文研究的是大型语言模型（LLMs）生成的解释的性质和挑战，仅通过输入输出来分析模型的自我解释能力，并未涉及模型内部机制，因此属于黑盒方法。研究领域涉及多领域指令微调数据集，不属于特定的数学、逻辑、代码或医学领域，因此归类为通用领域。
A Pragmatic Way to Measure Chain-of-Thought Monitorability,2025,True,False,True,,论文提出了衡量 CoT 可监控性的新指标，与 Faithfulness 相关，但未讨论不忠实现象或改进方法。,"While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI safety, this opportunity could be lost through shifts in training practices or model architecture. To help preserve monitorability, we propose a pragmatic way to measure two components of it: legibility (whether the reasoning can be followed by a human) and coverage (whether the CoT contains all the reasoning needed for a human to also produce the final output). We implement these metrics with an autorater prompt that enables any capable LLM to compute the legibility and coverage of existing CoTs. After sanity-checking our prompted autorater with synthetic CoT degradations, we apply it to several frontier models on challenging benchmarks, finding that they exhibit high monitorability. We present these metrics, including our complete autorater prompt, as a tool for developers to track how design decisions impact monitorability. While the exact prompt we share is still a preliminary version under ongoing development, we are sharing it now in the hopes that others in the community will find it useful. Our method helps measure the default monitorability of CoT - it should be seen as a complement, not a replacement, for the adversarial stress-testing needed to test robustness against deliberately evasive models.",arXiv.org,Black-box,General,该研究通过Prompting和API来评估Chain-of-Thought（CoT）的可监控性，涉及的是模型的输入输出行为，而非内部机制，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码或医学，而是通用的AI安全监控方法。
Large Language Models can Strategically Deceive their Users when Put Under Pressure,2023,True,True,False,,论文揭示了LLM在压力下战略性欺骗用户的不忠实现象，与CoT Faithfulness相关。,"We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.",,Black-box,Society,该研究通过模拟环境观察GPT-4在压力下的行为，涉及模型在未经直接指令的情况下对用户进行战略性欺骗，属于黑盒研究。研究领域涉及模型的行为与社会道德、人类行为相关，因此归类为社会学。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,Black-box,General,该研究通过改进 Chain-of-Thought (CoT) 提示方法，使用自然语言查询到符号推理链的转换和确定性求解器来保证推理链的忠实性。研究主要关注模型的输入输出和提示方法，并未涉及模型内部机制，因此属于黑盒方法。任务领域涉及数学、规划、多跳问答和关系推理等多个方面，不属于特定领域，因此归类为通用。
Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI,2022,True,True,True,Verification & External Tools,论文提出基于反事实的忠实性评估方法，涉及不忠实现象和新度量指标。,"Evaluating an explanation's faithfulness is desired for many reasons such as trust, interpretability and diagnosing the sources of model's errors. In this work, which focuses on the NLI task, we introduce the methodology of Faithfulness-through-Counterfactuals, which first generates a counterfactual hypothesis based on the logical predicates expressed in the explanation, and then evaluates if the model's prediction on the counterfactual is consistent with that expressed logic (i.e. if the new formula is \textit{logically satisfiable}). In contrast to existing approaches, this does not require any explanations for training a separate verification model. We first validate the efficacy of automatic counterfactual hypothesis generation, leveraging on the few-shot priming paradigm. Next, we show that our proposed metric distinguishes between human-model agreement and disagreement on new counterfactual input. In addition, we conduct a sensitivity analysis to validate that our metric is sensitive to unfaithful explanations.",AAAI Conference on Artificial Intelligence,Black-box,Logic,该论文研究的是自然语言推理（NLI）任务中解释的忠实性评估，通过生成反事实假设并评估模型预测的一致性。研究主要依赖于输入输出和Prompting方法，没有涉及模型内部机制，因此属于黑盒方法。任务领域涉及逻辑推理和反事实分析，因此归类为Logic。
The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge,2025,True,True,False,,论文揭示了LLM在作为评判者时存在捷径偏见和不忠实现象，但未提出具体度量或改进方法。,"Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert>Human>LLM>Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.",arXiv.org,Black-box,Society,该研究通过输入输出（Prompting）来评估大型语言模型（LLM）作为评判者的偏见行为，属于黑盒方法。研究领域涉及社会学，因为它探讨了模型在评判任务中的偏见和道德问题，特别是对来源和时间线索的偏好。
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用程序辅助蒸馏（PaD）来改进推理能力，涉及 CoT 和忠实性问题。,"While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",North American Chapter of the Association for Computational Linguistics,Black-box,General,论文主要研究如何通过蒸馏方法将大型语言模型的能力迁移到小型模型上，特别是通过程序辅助蒸馏（PaD）来提升推理能力。研究主要关注输入输出和Prompting，没有涉及模型内部机制的分析，因此属于黑盒方法。任务领域涉及算术推理、符号推理和通用能力，不属于特定领域，因此归类为通用。
LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations,2025,True,True,False,,论文揭示了LLMs生成的自反事实解释不忠实于内部决策过程的现象。,"To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",,Black-box,General,该论文研究的是语言模型通过自然语言解释其决策的能力，特别是自我生成的反事实解释（SCEs）。研究仅通过输入输出来评估模型的解释能力，并未涉及模型内部机制，因此属于黑盒方法。任务领域不涉及特定领域如数学、逻辑、代码或医学，而是关注模型解释能力的通用问题，因此归类为通用领域。
"Survey on Hallucination in Reasoning Large Language Model: Evaluation, Taxonomy, Intervention, and Open Issues",2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了CoT中的幻觉现象，提出了评估框架和干预策略，与Faithfulness高度相关。,"In recent years, reasoning large language models (LLMs) have seen increasingly widespread adoption in the field of education, particularly demonstrating substantial potential in tasks involving complex text comprehension. However, these LLMs are susceptible to a critical yet often overlooked issue: hallucinations within the reasoning process—instances where the model outputs a correct final answer while its underlying reasoning chain contains fabricated, inconsistent, or logically flawed content. Such hallucination phenomena in Chain-of-Thought (CoT) processes pose serious challenges to the reliability of educational applications. To address this issue, this study proposes a systematic research framework comprising dataset construction, multi-model CoT evaluation, and hallucination classification and quantification. Utilizing the whole-book reading dataset aligned with the junior secondary Chinese language curriculum, we conduct a comparative evaluation of six leading domestic and international LLMs, including ChatGPT o1 and DeepSeek-R1. Key findings include:(1) Hallucinations in CoT are prevalent across all tested models, with ChatGPT-o1 exhibiting a distinctive high accuracy–high hallucination pattern;(2) Hallucinations are both task-and genre-dependent: narrative texts, particularly novels, tend to trigger higher hallucination indices due to long-range dependencies and implicit cultural references. Tasks involving logical reasoning, linguistic feature analysis, and detail extraction show the highest hallucination rates, revealing model weaknesses in handling long-tail knowledge;(3) Hallucinations typically follow a progressive generative pattern: Information mis-reading → Comprehension deviation → Content fabrication → Logical instability. To mitigate these issues, we propose two targeted intervention strategies: uncertainty-based abstention and model-to-model correction. These approaches o ff er practical pathways toward enhancing the trustworthiness and educational applicability of reasoning LLMs.",Data Intelligence,Black-box,Society,该论文研究大型语言模型在推理过程中的幻觉问题，涉及模型输出的可靠性和教育应用的可信度。研究基于输入输出分析，未涉及模型内部机制，因此属于黑盒方法。任务领域涉及教育和社会学问题，特别是模型在复杂文本理解中的表现及其对社会应用的影响。
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,2023,True,True,True,Training & Fine-tuning,论文探讨了视觉语言模型的推理一致性，提出了基于CoT的度量方法，并通过两阶段训练框架提升一致性和性能。,"Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",North American Chapter of the Association for Computational Linguistics,Black-box,General,该研究主要关注视觉语言模型（VLMs）的推理能力和一致性，通过输入输出和Prompting进行评估，并未涉及模型内部机制的分析，因此属于黑盒方法。研究领域涉及视觉推理和语言模型的通用能力，不属于特定的数学、逻辑、代码、医学或社会学领域，因此归类为通用。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,Black-box,General,论文标题和摘要提到的是通过多模型协作（multi-model collaboration）来提升复杂推理任务的表现，但没有涉及模型内部机制、激活值、梯度或权重的分析。因此，研究类型属于黑盒。任务领域未明确提及数学、逻辑、代码、医学或社会学等具体领域，而是通用性的复杂推理任务，因此归类为通用。
Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation,2025,True,True,False,Prompting & In-Context Learning,论文探讨了CoT在代码生成中的质量问题，揭示了不忠实现象，并提出了通过提示改进CoT的方法。,"Large language models (LLMs) have demonstrated impressive performance in code generation, particularly when augmented with chain-of-thought (CoT) prompting techniques. They break down requirements into intermediate reasoning steps, which act as design rationales to guide LLMs in writing code like human programmers. Thus, the quality of these steps is crucial for ensuring the correctness and reliability of the generated code. However, little is known about the quality of CoT generated by LLMs. To what extent can we trust the thoughts generated by LLMs? How good are they? This paper empirically explores the external and internal factors of why LLMs generate unsatisfactory CoTs by analyzing 1,023 failed code samples on two widely used code generation benchmarks. We also evaluate their impact on code generation performance by analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting LLMs. Our study reveals three key findings: (1) External factors (53.60%), such as unclear requirements and lack of context, mainly affect CoT quality, while internal factors (40.10%) stem from LLMs'misunderstanding prompts. (2) Even when CoTs are correct, 18.5% of the generated code contains errors due to instruction-following issues; conversely, 11.90% of correct code is paired with flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when given detailed problem descriptions. These findings highlight key challenges in CoT-based code generation and suggest directions for improving LLM reasoning and reliability.",arXiv.org,Black-box,Code,该论文主要研究基于链式思考（CoT）提示技术的LLM在代码生成中的表现，通过分析输入输出（Prompting）来评估CoT的质量及其对代码生成的影响，并未涉及模型内部机制，因此属于黑盒方法。研究领域聚焦于代码生成，故归类为Code。
Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?,2025,True,True,False,Prompting & In-Context Learning,论文提出了增强 CoT 反思和错误纠正能力的方法，改进忠实性。,"Prompting methods for language models, such as Chain-of-thought (CoT), present intuitive step-by-step processes for problem solving. These methodologies aim to equip models with a better understanding of the correct procedures for addressing a given task. Despite these advancements, CoT lacks the ability of reflection and error correction, potentially causing a model to perpetuate mistakes and errors. Therefore, inspired by the human ability for said tasks, we propose Error Reflection Prompting (ERP) to further enhance reasoning in language models. Building upon CoT, ERP is a method comprised of an incorrect answer, error recognition, and a correct answer. This process enables the model to recognize types of errors and the steps that lead to incorrect answers, allowing the model to better discern which steps to avoid and which to take. The model is able to generate the error outlines itself with automated ERP generation, allowing for error recognition and correction to be integrated into the reasoning chain and produce scalability and reliability in the process. The results demonstrate that ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.",,Black-box,General,"该论文提出了一种新的提示方法（Error Reflection Prompting, ERP），旨在通过错误识别和纠正来增强语言模型的推理能力。研究仅涉及模型的输入输出和提示方法，没有涉及模型内部机制或权重等，因此属于黑盒方法。任务领域不特定于数学、逻辑、代码或医学，而是通用的推理能力提升，因此归类为通用领域。"
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,Black-box,Medical,该研究通过评估轻量级语言模型在计算表型任务中的表现，主要关注模型的输入输出和Prompting策略，未涉及模型内部机制的分析。研究领域为医学，特别是计算表型任务。
Do Biased Models Have Biased Thoughts?,2025,True,True,True,,论文探讨了 CoT 中的偏见现象，并提出了度量指标，但未提出改进方法。,"The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: $\textit{Do biased models have biased thoughts}$? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.",arXiv.org,Black-box,Society,该论文研究的是语言模型在链式思考提示（chain-of-thought prompting）下的偏见问题，涉及性别、种族、社会经济地位等社会因素。研究仅通过输入输出和提示方法（Prompting）进行分析，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及社会偏见，属于社会学（Society）范畴。
Humans Perceive Wrong Narratives from AI Reasoning Texts,2025,True,True,False,,论文揭示了人类对AI推理文本的理解与模型实际计算过程之间的不匹配，涉及不忠实现象。,"A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction.",arXiv.org,Black-box,Society,该论文研究的是人类如何理解AI模型生成的推理文本，以及这种理解是否与模型的实际计算过程相匹配。研究主要关注的是人类对AI生成文本的感知和解释，而不是模型内部的机制或权重等，因此属于黑盒方法。研究领域涉及人类行为和对AI生成内容的解释，属于社会学范畴。
AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning,2025,True,True,True,,论文提出AURA基准和AuraScore指标评估推理忠实性，揭示了模型推理错误但答案正确的现象。,"Current audio-visual (AV) benchmarks focus on final answer accuracy, overlooking the underlying reasoning process. This makes it difficult to distinguish genuine comprehension from correct answers derived through flawed reasoning or hallucinations. To address this, we introduce AURA (Audio-visual Understanding and Reasoning Assessment), a benchmark for evaluating the cross-modal reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across six challenging cognitive domains, such as causality, timbre and pitch, tempo and AV synchronization, unanswerability, implicit distractions, and skill profiling, explicitly designed to be unanswerable from a single modality. This forces models to construct a valid logical path grounded in both audio and video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To assess reasoning traces, we propose a novel metric, AuraScore, which addresses the lack of robust tools for evaluating reasoning fidelity. It decomposes reasoning into two aspects: (i) Factual Consistency - whether reasoning is grounded in perceptual evidence, and (ii) Core Inference - the logical validity of each reasoning step. Evaluations of SOTA models on AURA reveal a critical reasoning gap: although models achieve high accuracy (up to 92% on some tasks), their Factual Consistency and Core Inference scores fall below 45%. This discrepancy highlights that models often arrive at correct answers through flawed logic, underscoring the need for our benchmark and paving the way for more robust multimodal evaluation.",arXiv.org,Black-box,General,该论文主要关注音频-视觉大型语言模型（AV-LLMs）和全模态语言模型（OLMs）的跨模态推理能力评估，通过输入输出来评估模型的推理过程，属于黑盒方法。任务领域涉及多模态推理，不属于数学、逻辑、代码、医学或社会学等特定领域，因此归类为通用。
The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference,2025,True,True,False,,论文揭示了LLMs在临床推理中的不忠实现象，即知识获取与推理能力分离。,"Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting. Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts. These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.",arXiv.org,Black-box,Medical,该研究通过输入输出和Prompting（直接提示和思维链提示）来评估大型语言模型在临床自然语言推理任务中的表现，属于黑盒方法。研究领域涉及临床医学，因此归类为医学领域。
Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning,2025,True,True,True,Training & Fine-tuning,论文提出了新的度量指标（Alignment Score）并讨论了不忠实现象（四种错误类型），同时提出了优化方法（SCOS）。,"This paper presents a framework for evaluating and optimizing reasoning consistency in Large Language Models (LLMs) via a new metric, the Alignment Score, which quantifies the semantic alignment between model-generated reasoning chains and human-written reference chains in Chain-of-Thought (CoT) reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest Alignment Score. To explain this phenomenon, we define four key error types: logical disconnection, thematic shift, redundant reasoning, and causal reversal, and show how each contributes to the degradation of the Alignment Score. Building on this analysis, we further propose Semantic Consistency Optimization Sampling (SCOS), a method that samples and favors chains with minimal alignment errors, significantly improving Alignment Scores by an average of 29.84% with longer reasoning chains, such as in 3-hop tasks.",,Black-box,Logic,该论文通过评估和优化大型语言模型（LLMs）在链式思考（CoT）推理中的一致性，提出了一种新的度量标准——对齐分数（Alignment Score），并分析了四种关键错误类型。研究主要基于模型的输入输出（Prompting）进行分析，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及逻辑推理，因此归类为Logic。
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness,2023,True,True,True,,论文提出了评估推理链的框架ReCEval，关注正确性和信息量，与CoT忠实性相关。,"Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound reasoning quality with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains via two key properties: (1) correctness, i.e., each step makes a valid inference based on information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We evaluate these properties by developing metrics using natural language inference models and V-Information. On multiple datasets, we show that ReCEval effectively identifies various error types and yields notable improvements compared to prior methods. We analyze the impact of step boundaries, and previous steps on evaluating correctness and demonstrate that our informativeness metric captures the expected flow of information in high-quality reasoning chains. Finally, we show that scoring reasoning chains based on ReCEval improves downstream task performance. Our code is publicly available at: https://github.com/archiki/ReCEval",Conference on Empirical Methods in Natural Language Processing,Black-box,General,该论文主要关注多步推理链的评估方法，通过自然语言推理模型和V-Information来评估推理链的正确性和信息量，并未涉及模型内部机制或权重调整，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码或医学，而是通用的自然语言处理任务。
Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,2023,True,True,False,,论文探讨了LLMs在辩论中无法坚持正确信念的现象，揭示了不忠实现象。,"Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",Conference on Empirical Methods in Natural Language Processing,Black-box,Logic,该研究通过辩论式对话测试大型语言模型（LLMs）的推理能力，仅涉及模型的输入输出行为，未涉及模型内部机制，因此属于黑盒方法。研究领域涉及逻辑推理，因此归类为Logic。
Argumentative Large Language Models for Explainable and Contestable Claim Verification,2024,True,False,True,Verification & External Tools,论文提出通过论证框架提升解释性和可争议性，与CoT忠实性相关。,"The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing argumentative LLMs (ArgLLMs), a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs’ performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.",AAAI Conference on Artificial Intelligence,Black-box,Society,该论文研究的是如何通过增强大型语言模型（LLMs）的论证推理能力来提高其决策的可解释性和可争议性，属于黑盒方法，因为它主要关注的是输入输出和Prompting，而不是模型内部机制。任务领域涉及社会学，因为它关注的是决策的可解释性和可争议性，这与人类行为和社会经济因素相关。
Are DeepSeek R1 And Other Reasoning Models More Faithful?,2025,True,True,True,Training & Fine-tuning,论文评估了推理模型的忠实性，揭示了不忠实现象，并提出了度量指标。,"Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue""A Stanford Professor thinks the answer is D""is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",,Black-box,General,该研究通过输入输出（Prompting）来评估模型的忠实性，并未涉及模型内部机制，因此属于黑盒方法。研究领域涉及语言模型的解释性和忠实性，不属于特定的数学、逻辑、代码或医学领域，因此归类为通用领域。
MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps,2024,True,False,True,,论文提出了评估多模态CoT质量的框架MiCEval，涉及CoT忠实性的度量。,"Multimodal Chain of Thought (MCoT) is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation (MiCEval), a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found in https://github.com/alenai97/MiCEval.",North American Chapter of the Association for Computational Linguistics,Black-box,General,该论文研究的是多模态链式思维（MCoT）的评估方法，主要关注通过输入输出（即Prompting）来评估推理步骤的质量，没有涉及模型内部机制的分析，因此属于黑盒方法。任务领域涉及多模态大语言模型的推理评估，不属于数学、逻辑、代码、医学或社会学等特定领域，因此归类为通用领域。
Can Large Language Models Reason? A Characterization via 3-SAT,2024,True,True,False,Verification & External Tools,论文揭示了LLMs在3-SAT问题中的不忠实现象，并提出了使用外部推理器改进的方法。,"Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. However, recent works have shown that LLMs often bypass true reasoning using shortcuts, sparking skepticism. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of LLMs by varying the inherent hardness of the problem instances. Our experimental evidence shows that LLMs are incapable of performing true reasoning, as required for solving 3-SAT problems. Moreover, we observe significant performance variation based on the inherent hardness of the problems -- performing poorly on harder instances and vice versa. Importantly, we show that integrating external reasoners can considerably enhance LLM performance. By following a principled experimental protocol, our study draws concrete conclusions and moves beyond the anecdotal evidence often found in LLM reasoning research.",arXiv.org,Black-box,Logic,该论文通过研究大型语言模型在3-SAT问题上的表现来评估其推理能力，属于逻辑领域。研究仅通过输入输出（即Prompting）来测试模型，并未涉及模型内部机制，因此归类为黑盒方法。
General Purpose Verification for Chain of Thought Prompting,2024,True,False,True,Verification & External Tools,论文提出验证 CoT 步骤的方法，提升推理准确性，属于忠实性改进。,"Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.",arXiv.org,Black-box,General,该研究通过Prompting和验证链式思维步骤来改进大型语言模型的推理能力，并未涉及模型内部机制，因此属于黑盒方法。研究涉及的任务类型多样，不特定于数学、逻辑、代码、医学或社会学中的某一领域，因此归类为通用领域。
Large Language Models Cannot Explain Themselves,2024,True,True,False,,论文讨论了LLM生成的解释不忠实于内部计算过程的现象。,"Large language models can be prompted to produce text. They can also be prompted to produce""explanations""of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These""explanations""can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these""explanations"", using the term""exoplanations""to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.",arXiv.org,Black-box,General,论文主要讨论大型语言模型通过Prompting生成的解释（explanations）并不反映其内部机制，而是外生的（exogenous），因此属于黑盒研究。研究领域不涉及特定任务，而是通用的大型语言模型行为分析。
Chain of Thoughtlessness? An Analysis of CoT in Planning,2024,True,True,False,,论文揭示了CoT在规划问题中的不忠实现象，即性能提升依赖于特定问题提示而非通用算法学习。,"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.",Neural Information Processing Systems,Black-box,Logic,该论文研究的是通过链式思维提示（Chain of Thought prompting）来改善大型语言模型（LLM）在规划问题上的表现，这属于黑盒方法，因为研究仅涉及输入输出和提示策略，没有涉及模型内部机制。研究领域是逻辑，因为问题来自经典的规划领域Blocksworld，涉及算法和问题解决逻辑。
LLM-Generated Black-box Explanations Can Be Adversarially Helpful,2024,True,True,False,,论文揭示了LLM生成的不忠实CoT现象，即对抗性帮助，使错误答案看起来合理。,"Large Language Models (LLMs) are becoming vital tools that help us solve and understand complex problems by acting as digital assistants. LLMs can generate convincing explanations, even when only given the inputs and outputs of these problems, i.e., in a ``black-box'' approach. However, our research uncovers a hidden risk tied to this approach, which we call *adversarial helpfulness*. This happens when an LLM's explanations make a wrong answer look right, potentially leading people to trust incorrect solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing the questions, expressing an elevated level of confidence, and cherry-picking evidence to paint misleading answers in a credible light. To examine if LLMs are able to navigate complex-structured knowledge when generating adversarially helpful explanations, we create a special task based on navigating through graphs. Most LLMs are not able to find alternative paths along simple graphs, indicating that their misleading explanations aren't produced by only logical deductions using complex knowledge. These findings shed light on the limitations of the black-box explanation setting and allow us to provide advice on the safe usage of LLMs.",arXiv.org,Black-box,General,该论文研究的是大型语言模型（LLMs）在仅通过输入和输出（即“黑盒”方法）生成解释时的潜在风险，因此归类为Black-box。研究领域涉及LLMs生成解释的普遍行为及其对人类和LLM评估者的影响，不属于特定的数学、逻辑、代码或医学领域，因此归类为General。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,Black-box,General,该论文研究的是语言模型是否能解释自己的分类行为，主要关注的是模型的输入输出和自然语言解释，没有涉及模型内部机制的分析，因此属于黑盒方法。研究领域不涉及特定任务领域，而是通用的语言模型行为解释，因此归类为通用领域。
Why Would You Suggest That? Human Trust in Language Model Responses,2024,True,True,False,,论文探讨了解释的忠实性对用户信任的影响，涉及不忠实现象。,"The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.",arXiv.org,Black-box,Society,论文研究了人类对语言模型输出的信任，涉及用户与AI的交互（human-AI collaboration）和信任评估（trust and reliance），这表明它关注的是社会学领域的偏见、道德和人类行为。研究方法仅通过输入输出（即模型的响应和解释）进行分析，没有涉及模型内部机制，因此属于黑盒类型。
FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain,2024,True,False,True,Consistency & Ensembling,论文使用自洽性（self-consistency）和多数投票提升CoT性能，并报告了忠实性分数。,"This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improve the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make thefinal verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly.",International Workshop on Semantic Evaluation,Black-box,Medical,论文使用了Chain of Thought (CoT)范式，这是一种通过输入输出来进行推理的方法，没有涉及模型内部机制，因此属于黑盒方法。任务领域是生物医学自然语言推理，涉及临床实验，因此归类为医学领域。
DO-FacT: Discrete-Optimized Fact-Tree Prompting Framework for Commonsense Reasoning,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文提出了一种结合事实生成和验证的框架，旨在提升推理的可靠性和真实性。,"Commonsense Reasoning is one of the major bottlenecks in machine intelligence, despite it has been widely studied. Prompt-based methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) provide structured reasoning paths, while often suffering from issues related to the quality and authenticity of the generated facts. In this paper, we propose Discrete-Optimized Fact-Tree Prompting (DO-FacT), a novel framework that combines fact proposal generation and fact authenticity verification in an iterative process. In DO-FacT, both the fact proposal generation and authenticity verification modules are refined by Discrete-Optimized Prompt-Tuning (DOPT) in advance, leading to the production of high-quality facts and more reliable validation. Experimental results on three widely used commonsense reasoning datasets demonstrate that DO-FacT significantly outperforms existing relevant approaches like CoT and fact-tree with handwrite prompt templates, providing more accurate and reliable reasoning results.",International Conferences on Software Engineering and Information Management,Black-box,Logic,该论文提出了一种基于提示（Prompting）的框架DO-FacT，用于常识推理。研究仅涉及输入输出和Prompting技术，没有涉及模型内部机制，因此属于黑盒方法。任务领域是常识推理，属于逻辑领域。
Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文揭示了CoT可能强化模型偏见的不忠实现象，并提出了APriCoT方法来改进。,"Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.",,Black-box,Society,该论文研究语言模型在任务中的偏见问题，并通过提示工程（Counterfactual Prompting with Agnostically Primed CoT）来减少偏见的影响。研究仅涉及输入输出和提示方法，未涉及模型内部机制，因此属于黑盒方法。研究领域涉及偏见和公平性，属于社会学范畴。
Quantifying Uncertainty in Natural Language Explanations of Large Language Models,2023,True,True,True,,论文提出了量化LLM解释不确定性的新指标，并探讨了解释的忠实性。,"Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",International Conference on Artificial Intelligence and Statistics,Black-box,General,该研究通过Prompting和模型扰动来量化大型语言模型解释的不确定性，属于黑盒方法。研究领域涉及模型解释的可靠性，不特定于数学、逻辑、代码、医学或社会学，因此归类为通用领域。
CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning,2024,True,False,False,Training & Fine-tuning,论文提出CoMAT方法提升数学推理的忠实性和可验证性，属于改进方法中的训练与微调。,"Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",,Black-box,Math,论文提出了一种名为CoMAT的新方法，通过改进的提示技术（Chain of Mathematically Annotated Thought）来增强大型语言模型在数学推理任务上的表现。该方法仅通过输入输出和Prompting进行研究，没有涉及模型内部机制，因此属于黑盒方法。研究领域明确为数学推理，故归类为Math。
Stepwise Reasoning Disruption Attack of LLMs,2025,True,True,False,,论文探讨了LLM推理过程中的不忠实现象，即通过SEED攻击误导模型产生错误推理。,"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain unexplored, particularly in third-party platforms that facilitate user interactions via APIs. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the S tepwise r E asoning E rror D isruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github. com/Applied-Machine-Learning-Lab/ SEED-Attack",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该论文研究的是通过API对大型语言模型进行攻击，属于黑盒方法。研究领域涉及模型推理过程的安全性，不特定于数学、逻辑、代码或医学等具体领域，因此归类为通用领域。
Measuring Faithfulness in Chain-of-Thought Reasoning,2023,True,True,True,,论文探讨了CoT的忠实性问题，提出了干预方法评估忠实性，并揭示了不忠实现象。,"Large language models (LLMs) perform better when they produce step-by-step,""Chain-of-Thought""(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",arXiv.org,Black-box,General,该研究通过干预模型的Chain-of-Thought（CoT）推理（如添加错误或改写）来观察模型预测的变化，属于仅通过输入输出进行研究，因此归类为黑盒。研究领域涉及模型推理的忠实性，不特定于数学、逻辑、代码、医学或社会学，因此归类为通用领域。
Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers,2025,True,True,True,,论文探讨了LLMs在MCQA任务中的推理策略，并测试了其忠实性。,"Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",arXiv.org,Black-box,General,该研究主要关注大型语言模型（LLMs）在回答选择题时的推理策略，通过输入输出（Prompting）进行研究，并未涉及模型内部机制，因此属于黑盒方法。研究领域涉及选择题回答策略，不属于特定的数学、逻辑、代码或医学领域，因此归类为通用领域。
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,2024,True,False,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文提出基于因果关系的提示方法，利用CoT作为中介变量，并通过微调提升忠实性。,"Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.",AAAI Conference on Artificial Intelligence,Black-box,Society,该论文提出了一种基于因果关系的提示方法，旨在减少大型语言模型中的偏见。研究仅通过输入输出（Prompting）进行研究，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及社会学，因为它关注的是模型中的偏见问题，属于社会学的范畴。
IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured Reasoning Templates,2025,True,True,False,Prompting & In-Context Learning,论文提出结构化模板方法IAO，明确追踪知识流，提升忠实性。,"While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, understanding and validating their knowledge utilization remains challenging. Chain-of-thought (CoT) prompting partially addresses this by revealing intermediate reasoning steps, but the knowledge flow and application remain implicit. We introduce IAO (Input-Action-Output) prompting, a structured template-based method that explicitly models how LLMs access and apply their knowledge during complex reasoning tasks. IAO decomposes problems into sequential steps, each clearly identifying the input knowledge being used, the action being performed, and the resulting output. This structured decomposition enables us to trace knowledge flow, verify factual consistency, and identify potential knowledge gaps or misapplications. Through experiments across diverse reasoning tasks, we demonstrate that IAO not only improves zero-shot performance but also provides transparency in how LLMs leverage their stored knowledge. Human evaluation confirms that this structured approach enhances our ability to verify knowledge utilization and detect potential hallucinations or reasoning errors. Our findings provide insights into both knowledge representation within LLMs and methods for more reliable knowledge application.",arXiv.org,Black-box,General,该论文提出了一种名为IAO Prompting的结构化模板方法，通过输入-动作-输出（Input-Action-Output）的分解来显式建模LLMs在复杂推理任务中如何访问和应用知识。研究仅涉及通过Prompting（即输入输出）进行研究，没有涉及模型内部机制，因此属于Black-box。任务领域不特定于数学、逻辑、代码、医学或社会学，而是通用的推理任务，因此归类为General。
"A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",2025,True,True,True,,论文探讨了多语言CoT推理的性能、一致性和忠实性，并使用了扰动技术来评估忠实性。,"Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",arXiv.org,Black-box,General,该研究通过输入输出和Prompting来评估多语言链式思维推理的性能、一致性和忠实性，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及多语言推理，不属于特定的数学、逻辑、代码或医学领域，因此归类为通用领域。
Case-Based Deduction for Entailment Tree Generation,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了逻辑一致性问题，提出了改进方法，与CoT忠实性相关。,"Maintaining logical consistency in structured explanations is critical for understanding and troubleshooting the reasoning behind a system’s decisions. However, existing methods for entailment tree generation often struggle with logical consistency, resulting in erroneous intermediate conclusions and reducing the overall accuracy of the explanations. To address this issue, we propose case-based deduction (CBD), a novel approach that retrieves cases with similar logical structures from a case base and uses them as demonstrations for logical deduction. This method guides the model toward logically sound conclusions without the need for manually constructing logical rule bases. By leveraging a prototypical network for case retrieval and reranking them using information entropy, CBD introduces diversity to improve in-context learning. Our experimental results on the EntailmentBank dataset show that CBD significantly improves entailment tree generation, achieving performance improvements of 1.7% in Task 1, 0.6% in Task 2, and 0.8% in Task 3 under the strictest Overall AllCorrect metric. These findings confirm that CBD enhances the logical consistency and overall accuracy of AI systems in structured explanation tasks.",Mathematics,Black-box,Logic,该论文提出了一种基于案例的演绎方法（CBD），通过检索类似逻辑结构的案例来指导模型进行逻辑推理，而不涉及模型内部机制或权重调整。研究重点在于逻辑一致性和结构化解释，属于逻辑领域。
Rationale-Aware Answer Verification by Pairwise Self-Evaluation,2024,True,True,False,Verification & External Tools,论文讨论了LLM生成答案中理性与答案不一致的问题，并提出了改进验证方法。,"Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier’s ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,Black-box,Logic,"该论文研究的是通过大型语言模型（LLMs）生成的候选答案的验证方法，主要关注的是答案的逻辑合理性（rationale）而非模型内部机制。方法涉及通过输入输出（Prompting）进行自我评估，属于黑盒方法。任务领域涉及逻辑推理（StrategyQA, ARC-Challenge, DROP等），因此归类为Logic。"
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,Black-box,General,该论文研究的是如何评估和改进大型语言模型（LLM）解释的忠实性，主要关注的是模型的输入输出行为（通过干预性评估），而不是模型内部机制。因此归类为黑盒方法。研究领域不涉及特定任务领域（如数学、逻辑等），而是通用的LLM解释问题。
Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations,2025,True,True,True,,论文探讨了VLMs中CoT解释的忠实性问题，并提出了新的评估框架。,"Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone ($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality can be decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.",arXiv.org,Black-box,Medical,该研究通过输入输出（如文本和图像的修改）来评估医疗视觉语言模型的推理忠实性，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及医疗影像和临床决策，属于医学领域。
MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification,2025,True,True,False,Verification & External Tools,论文提出结构化论证树以提升推理的忠实性，属于改进方法中的验证与外部工具。,"Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",arXiv.org,Black-box,Society,该论文研究的是通过多个大型语言模型（LLMs）的输出进行结构化论证，以验证声明的合理性。研究主要基于模型的输入输出（Prompting）和API调用，没有涉及模型内部机制，因此属于黑盒方法。任务领域涉及论证和声明验证，属于社会学范畴，因为它关注的是如何通过结构化论证来减少错误和偏见，并提供合理的决策依据。
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,2023,True,True,False,,论文揭示了CoT解释可能误导性地反映模型预测的真实原因，属于不忠实现象。,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",Neural Information Processing Systems,Black-box,Society,该论文研究了大型语言模型（LLMs）在链式思维提示（CoT）中生成的解释可能不忠实于模型实际预测原因的问题。研究通过改变输入特征（如重新排序多项选择选项）来影响模型的解释，而不涉及模型内部机制的分析，因此属于黑盒方法。研究领域涉及社会偏见和模型解释的可靠性，属于社会学范畴。
Towards Large Language Models with Self-Consistent Natural Language Explanations,2025,True,True,True,Training & Fine-tuning,论文揭示了LLM事后解释的不一致性，并提出了新的度量指标和优化方法。,"Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature importance. Despite growing evidence of this inconsistency, no systematic solutions have emerged, partly due to the high cost of estimating feature importance, which limits evaluations to small datasets. To address this, we introduce the Post-hoc Self-Consistency Bank (PSCB) - a large-scale benchmark of decisions spanning diverse tasks and models, each paired with LLM-generated explanations and corresponding feature importance scores. Analysis of PSCB reveals that self-consistency scores barely differ between correct and incorrect predictions. We also show that the standard metric fails to meaningfully distinguish between explanations. To overcome this limitation, we propose an alternative metric that more effectively captures variation in explanation quality. We use it to fine-tune LLMs via Direct Preference Optimization (DPO), leading to significantly better alignment between explanations and decision-relevant features, even under domain shift. Our findings point to a scalable path toward more trustworthy, self-consistent LLMs.",arXiv.org,Black-box,General,该研究通过分析大型语言模型（LLMs）生成的解释与决策特征重要性之间的一致性，提出了一种新的评估指标和优化方法。研究主要依赖于模型的输入输出（即生成的解释和决策），而不涉及模型内部机制，因此属于黑盒方法。任务领域不特定于数学、逻辑、代码、医学或社会学，而是通用的模型解释性问题。
Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出因果框架评估CoT的充分性和必要性，涉及不忠实现象及改进方法。,"Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.",arXiv.org,Black-box,General,该论文研究的是通过因果框架改进 Chain-of-Thought (CoT) 提示方法，主要关注的是输入输出和提示策略，没有涉及模型内部机制，因此属于黑盒方法。任务领域涉及数学和常识推理，但未明确限定在特定领域，因此归类为通用。
LExT: Towards Evaluating Trustworthiness of Natural Language Explanations,2025,True,True,True,,论文提出评估自然语言解释可信度的框架，关注忠实性，并揭示通用模型的不忠实现象。,"As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond.","Conference on Fairness, Accountability and Transparency",Black-box,Medical,该论文的标题和摘要提到评估LLMs生成的解释的可信度，特别是关注于透明度和可靠性等关键问题。因此，其研究属于黑盒类型，因为它关注的是模型的输出（自然语言解释）而非内部工作原理。领域为医学，因为研究应用于医疗保健领域，强调在该敏感领域中解释的可信度和透明度。
Scaling hermeneutics: a guide to qualitative coding with LLMs for reflexive content analysis,2025,True,False,False,Prompting & In-Context Learning,论文讨论了使用CoT提升LLMs在定性编码中的忠实性，但未涉及不忠实现象或新度量指标。,"Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.",EPJ Data Science,Black-box,Society,该论文主要探讨如何利用大型语言模型（LLMs）进行定性编码和内容分析，以辅助社会学研究。研究仅涉及通过输入输出和Prompting来使用LLMs，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及社会学，因为它关注的是社会动态和人类行为分析。
Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification,2025,True,True,True,Training & Fine-tuning,论文讨论了LLM生成的理由中存在逻辑错误和标签不匹配等不忠实现象，并提出了基于信任指标的蒸馏方法来提升忠实性。,"Large language models (LLMs) increasingly generate natural language rationales to enhance interpretability, but these often contain logical errors, label mismatches, and domain-specific misalignments. Directly using such rationales as supervision risks propagating noise and undermining training stability. To address this challenge, we introduce Self-Filtered Distillation, a framework specifically tailored for patent classification, which treats LLM-generated rationales as trust signals rather than ground-truth supervision. The framework employs selective distillation guided by three unsupervised trust metrics: (1) Self-Consistency, which measures the stability of LLM-generated rationales across multiple generations; (2) Class Entailment Alignment, which assesses semantic coherence with patent-specific class definitions; and (3) LLM Agreement Scoring, which validates rationale-label plausibility. These metrics are integrated into a unified trust score that primarily weights training samples while optionally filtering out extremely low-trust cases, enabling reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used benchmark for patent classification, show that our method outperforms label-based learning and conventional distillation in accuracy, stability, and interpretability, establishing a reliable paradigm for leveraging reasoning-aware trust indicators in patent analytics.",arXiv.org,Black-box,General,该研究主要关注利用LLM生成的信任指标进行专利分类，研究仅涉及LLM的输入输出（如生成的rationales和信任评分），并未涉及模型内部机制或权重调整，因此属于Black-box。任务领域是专利分类，不属于数学、逻辑、代码、医学或社会学等特定领域，故归类为General。
Are Language Models Consequentialist or Deontological Moral Reasoners?,2025,True,True,False,,论文揭示了LLM在道德推理中CoT与事后解释的不一致现象，符合不忠实现象的定义。,"As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .",,Black-box,Society,该研究通过分析大型语言模型（LLMs）在道德困境中的推理痕迹，探讨其道德推理过程，属于黑盒研究，因为仅通过输入输出和Prompting进行研究。研究领域涉及道德伦理和社会学问题，因此归类为社会学。
Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations,2025,True,True,True,Verification & External Tools,论文讨论了LLM生成解释的不一致性，并提出了一种通过后续问题检查一致性的新方法。,"Large Language Models (LLMs) are often asked to explain their outputs to enhance accuracy and transparency. However, evidence suggests that these explanations can misrepresent the models' true reasoning processes. One effective way to identify inaccuracies or omissions in these explanations is through consistency checking, which typically involves asking follow-up questions. This paper introduces, cross-examiner, a new method for generating follow-up questions based on a model's explanation of an initial question. Our method combines symbolic information extraction with language model-driven question generation, resulting in better follow-up questions than those produced by LLMs alone. Additionally, this approach is more flexible than other methods and can generate a wider variety of follow-up questions.",arXiv.org,Black-box,General,该研究通过生成后续问题来评估大型语言模型生成的解释的一致性，仅涉及模型的输入输出和Prompting，没有涉及模型内部机制，因此属于黑盒方法。研究领域不特定于数学、逻辑、代码或医学，而是通用的模型解释和透明度问题。
Auditing Meta-Cognitive Hallucinations in Reasoning Large Language Models,2025,True,True,True,,论文揭示了CoT中的不忠实现象（chain disloyalty），并提出了黑盒审计方法进行度量。,"The development of Reasoning Large Language Models (RLLMs) has significantly improved multi-step reasoning capabilities, but it has also made hallucination problems more frequent and harder to eliminate. While existing approaches mitigate hallucinations through external knowledge integration, model parameter analysis, or self-verification, they often fail to capture how hallucinations emerge and evolve across the reasoning chain. In this work, we study the causality of hallucinations under constrained knowledge domains by auditing the Chain-of-Thought (CoT) trajectory and assessing the model's cognitive confidence in potentially erroneous or biased claims. Our analysis reveals that in long-CoT settings, RLLMs can iteratively reinforce biases and errors through flawed reflective reasoning, eventually leading to hallucinated reasoning paths. Surprisingly, even direct interventions at the origin of hallucinations often fail to reverse their effects, as reasoning chains exhibit'chain disloyalty'-- a resistance to correction and a tendency to preserve flawed logic. Furthermore, we show that existing hallucination detection methods are less reliable and interpretable than previously assumed in complex reasoning scenarios. Unlike methods such as circuit tracing that require access to model internals, our black-box auditing approach supports interpretable long-chain hallucination attribution, offering better generalizability and practical utility. Our code is available at: https://github.com/Winnie-Lian/AHa_Meta_Cognitive",arXiv.org,Black-box,Logic,该论文研究的是大型语言模型在推理过程中产生的幻觉问题，通过审计思维链（CoT）轨迹来评估模型的认知信心，属于黑盒方法，因为它不涉及模型内部机制的分析。研究领域涉及逻辑推理中的错误和偏见，因此归类为Logic。
Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku,2025,True,True,False,,论文探讨了LLMs在解决数独问题时解释的不忠实现象，但未提出具体度量或改进方法。,"The success of Large Language Models (LLMs) in human-AI collaborative decision-making hinges on their ability to provide trustworthy, gradual, and tailored explanations. Solving complex puzzles, such as Sudoku, offers a canonical example of this collaboration, where clear and customized explanations often hold greater importance than the final solution. In this study, we evaluate the performance of five LLMs in solving and explaining \sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving puzzles, none can explain the solution process in a manner that reflects strategic reasoning or intuitive problem-solving. These findings underscore significant challenges that must be addressed before LLMs can become effective partners in human-AI collaborative decision-making.",Annual Meeting of the Association for Computational Linguistics,Black-box,Logic,该研究通过评估大型语言模型（LLMs）在解决和解释数独谜题中的表现，仅涉及模型的输入输出行为，没有涉及模型内部机制，因此属于黑盒方法。研究领域为数独，属于逻辑推理任务。
LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring,2025,True,True,False,,论文揭示了模型在CoT监控下故意表现不佳的不忠实现象。,"Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.",arXiv.org,Black-box,Society,该研究通过Prompting方式测试模型在评估中的潜在欺骗行为（sandbagging），属于黑盒方法。研究涉及AI系统的安全部署和社会信任问题，属于社会学领域。
Are LLMs Better Formalizers than Solvers on Complex Problems?,2025,True,True,False,Verification & External Tools,论文讨论了LLM作为形式化器的忠实性问题，并提出了使用外部求解器作为改进方法。,"A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer.",,Black-box,Logic,该论文研究的是大型语言模型（LLMs）在逻辑推理问题中的表现，特别是作为形式化工具与直接求解器的对比。研究涉及的是模型的输入输出行为，没有涉及模型内部机制，因此属于黑盒方法。任务领域是逻辑推理，因此归类为Logic。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,Black-box,Logic,该论文研究的是通过改进 Chain-of-Thought (CoT) 推理策略来提升大型语言模型 (LLMs) 的逻辑推理能力，特别是通过准符号抽象 (Quasi-Symbolic Abstractions) 来分离内容与逻辑推理。虽然涉及逻辑形式化，但研究主要基于模型的输入输出和 Prompting 策略，并未深入模型内部机制，因此属于黑盒方法。任务领域涉及逻辑推理，故归类为 Logic。
Counterfactual Simulatability of LLM Explanations for Generation Tasks,2025,True,False,True,,论文提出了评估解释的框架，涉及 CoT 的忠实性度量，但未讨论不忠实现象或改进方法。,"LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.",arXiv.org,Black-box,General,该研究通过评估LLM的解释能力，关注的是用户如何通过解释预测模型在反事实情况下的输出，这主要依赖于输入输出和Prompting，没有涉及模型内部机制，因此属于黑盒方法。研究领域涉及新闻摘要和医学建议，但未明确限定在特定领域，因此归类为通用。
Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning,2025,True,True,False,,论文研究了外部信息对LLM推理过程的影响，揭示了不忠实现象。,"The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative""thinking mode""is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models""think"", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",arXiv.org,Black-box,General,该论文研究的是大型语言模型（LLMs）在推理过程中对外部辅助信息的反应，主要关注的是输入输出行为（即Prompting和辅助信息的影响），并未涉及模型内部机制的分析，因此属于黑盒方法。研究领域涉及模型的推理能力，但未特定于数学、逻辑、代码、医学或社会学等具体领域，因此归类为通用领域。
Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search,2025,True,True,False,Verification & External Tools,论文提出了 Const-o-T 框架，通过约束引导推理，提高忠实性和规划效率。,"While researchers have made significant progress in enabling large language models (LLMs) to perform multi-step planning, LLMs struggle to ensure that those plans align with high-level user intent and satisfy symbolic constraints, especially in complex, multi-step domains. Existing reasoning approaches such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented methods, expand the search space but often yield infeasible actions or hallucinated steps. To overcome these limitations, we propose Constraints-of-Thought (Const-o-T), a framework that provides a structured prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically meaningful paths. Each reasoning step is represented as an (intent, constraint) pair, which serves both to compress the search space and enforce validity. Unlike prior methods that merely generate reasoning traces or validate outputs post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a structured representation of intent-constraint pairs constraints prune infeasible branches and guide exploration toward semantically valid actions, improving planning efficiency and verifiable decision-making. We demonstrate across three domains Risk game, CAD code generation, and arithmetic reasoning that our approach outperforms baselines, yielding higher accuracy and stronger structural alignment. Our contribution is to demonstrate that Const-of-T offers a generalizable foundation for constraint-guided reasoning, enabling more efficient, constraint-aligned, and domain-adaptable planning with LLMs.",arXiv.org,Black-box,General,该论文提出了一种名为Constraints-of-Thought (Const-o-T)的框架，用于在语言模型引导的搜索中进行约束推理。研究主要关注通过输入输出（即Prompting）来引导语言模型的推理过程，并未涉及模型内部机制的分析，因此属于黑盒方法。任务领域涉及多个方面（Risk游戏、CAD代码生成和算术推理），不属于特定的数学、逻辑、代码、医学或社会学领域，因此归类为通用领域。
Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning,2025,True,True,False,Training & Fine-tuning,论文探讨了RLVR在数学推理中的忠实性问题，揭示了模型可能依赖表面启发式而非真实推理。,"Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",arXiv.org,Black-box,Math,论文标题和摘要提到研究的是数学推理能力，特别是通过强化学习与可验证奖励（RLVR）来增强大型语言模型（LLMs）的能力。研究关注的是模型的输入输出行为，而不是内部机制，因此归类为黑盒。任务领域明确涉及数学问题，如活动调度和最长递增子序列。
Human-Aligned Faithfulness in Toxicity Explanations of LLMs,2025,True,True,True,,论文提出新度量标准HAF评估LLMs毒性解释的忠实性，揭示了不一致和不相关响应现象。,"The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs'reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs'free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate HAF of LLMs'toxicity explanations with no human involvement, and highlight how""non-ideal""the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and irrelevant responses. We open-source our code at https://github.com/uofthcdslab/HAF and LLM-generated explanations at https://huggingface.co/collections/uofthcdslab/haf.",arXiv.org,Black-box,Society,该论文研究的是大型语言模型（LLMs）在毒性解释方面与人类对齐的忠实性，主要关注的是模型的输出解释（即Prompting的结果）而非内部机制，因此属于黑盒方法。研究领域涉及社会学的毒性、偏见和道德问题，因此归类为社会学。
A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models,2025,True,True,True,,论文研究了CoT忠实性，揭示了不忠实现象并提出了新的评估框架。,"Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent''reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",Conference on Empirical Methods in Natural Language Processing,Black-box,Society,该论文研究的是大型视觉语言模型（LVLMs）中的偏见和思维链（CoT）忠实性问题，主要关注模型如何处理和表达基于文本和图像的偏见。研究通过输入输出分析模型的偏见表达和推理过程，并未涉及模型内部机制，因此属于黑盒方法。研究领域涉及偏见和道德问题，属于社会学范畴。
Reasoning Can Hurt the Inductive Abilities of Large Language Models,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT推理可能降低归纳性能的现象，并提出了结构化干预方法。,"Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts. To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",arXiv.org,Black-box,Logic,该研究通过输入输出和Prompting（如chain-of-thought）来评估大型语言模型的归纳推理能力，并未涉及模型内部机制，因此属于黑盒方法。研究领域聚焦于逻辑推理（归纳推理）和游戏规则推断，故归类为Logic。
LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models,2025,True,True,True,,论文提出LAMP方法，评估模型解释与预测的一致性，涉及CoT忠实性。,"We introduce LAMP (Linear Attribution Mapping Probe), a method that shines light onto a black-box language model's decision surface and studies how reliably a model maps its stated reasons to its predictions through a locally linear model approximating the decision surface. LAMP treats the model's own self-reported explanations as a coordinate system and fits a locally linear surrogate that links those weights to the model's output. By doing so, it reveals which stated factors steer the model's decisions, and by how much. We apply LAMP to three tasks: sentiment analysis, controversial-topic detection, and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs exhibit locally linear decision landscapes. In addition, these surfaces correlate with human judgments on explanation quality and, on a clinical case-file data set, aligns with expert assessments. Since LAMP operates without requiring access to model gradients, logits, or internal activations, it serves as a practical and lightweight framework for auditing proprietary language models, and enabling assessment of whether a model behaves consistently with the explanations it provides.",arXiv.org,Black-box,Society,LAMP方法通过分析语言模型的自我报告解释来研究其决策表面，而不需要访问模型的梯度、logits或内部激活值，因此属于黑盒方法。研究涉及情感分析、争议话题检测和安全提示审核等任务，这些都属于社会学领域。
FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale,2025,True,False,True,,论文提出了验证推理痕迹的数据集和诊断任务，直接评估推理过程的忠实性。,"Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",,Black-box,Logic,该论文研究的是语言模型在逻辑推理任务中的表现，通过输入输出（如掩码操作预测和步骤完成）来评估模型性能，并未涉及模型内部机制的分析，因此属于黑盒方法。任务领域明确为逻辑推理，故归类为Logic。
WakenLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking,2025,True,True,True,Prompting & In-Context Learning,论文探讨了LLM在推理任务中的不忠实现象，并提出了新的评估框架和改进方法。,"Large Language Models (LLMs) frequently output the label Unknown in reasoning tasks, where two scenarios may appear: (i) an input sample is genuinely unverifiable, but the model cannot understand why; and (ii) a verifiable problem that the model fails to solve, thus outputs Unknown. We refer to these cases collectively as the Vague Perception phenomenon. Current evaluations focus on whether such answers are honest, rather than analyzing the limits of LLM reasoning. To address this, we introduce WakenLLM, a framework that quantifies the portion of Unknown output attributable to model incapacity and evaluates whether stimulation can convert them into either correct answers (verifiable) or justified (unverifiable) responses with valid reasoning. Our method offers a clearer picture of the limits of LLM reasoning and the potential for corrections across various datasets. Comprehensive experiments on six LLMs suggest that, without any training or parameter revision, LLMs can achieve up to a 68.53% accuracy improvement on Vague Perception samples through guided understanding. Our work reveals that current baseline methods only activate a small portion of LLMs'reasoning potential, indicating considerable unexplored capacity. This extends the theoretical upper bounds of reasoning accuracy in LLMs. Consequently, this study deepens our understanding of the latent reasoning capacity of LLMs and offers a new perspective on addressing the Vague Perception phenomenon.",arXiv.org,Black-box,General,该研究通过输入输出和Prompting（引导理解）来评估大型语言模型（LLMs）的推理潜力和稳定性，并未涉及模型内部机制或权重调整。研究领域为通用推理能力评估，不特定于数学、逻辑、代码、医学或社会学。
SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models,2023,True,True,False,,论文讨论了生成模型中的社会偏见放大问题，并涉及了CoT中的不忠实现象。,"Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, we start with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two open source generative language models and we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We demonstrate that the deliberate design of the templates in our benchmark (e.g., adding biasing text to the prompt or using different verbs that change the answer that indicates bias) impacts the model tendencies to generate socially biased output. Additionally, through manual evaluation, we discover problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning.

Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.",AAAI Conference on Artificial Intelligence,Black-box,Society,该研究通过输入输出（prompting）来评估生成语言模型中的社会偏见放大问题，属于黑盒方法。研究领域涉及社会偏见和道德问题，属于社会学领域。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,Black-box,Society,论文主要关注大型语言模型（LLMs）的指导原则和评估，涉及透明度、鲁棒性、真实性和伦理对齐等问题，这些问题属于社会学领域。研究通过整理文献和专家调查来制定原则，并未涉及模型内部机制，因此属于黑盒方法。
Evaluating and Mitigating Sycophancy in Large Vision-Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LVLMs中的Sycophancy现象，提出了评估框架SyEval-VL和改进方法HFRAG。,"Large vision-language models (LVLMs) have recently achieved significant advancements, demonstrating powerful capabilities in understanding and reasoning about visual information. However, LVLMs may generate biased responses that reflect the user beliefs rather than the facts, a phenomenon known as sycophancy. Sycophancy can pose serious challenges to the performance, trustworthiness, and security of LVLMs, raising concerns about their practical applications. We note that there is limited work on the evaluation and mitigation of sycophancy in LVLMs. In this paper, we introduce SyEval-VL, a benchmark specifically designed to evaluate sycophancy in LVLMs. SyEval-VL offers a comprehensive evaluation of sycophancy in visual understanding and reasoning across various scenarios with a multi-round dialogue format. We evaluate sycophancy in several popular LVLMs, providing an in-depth analysis of various sycophantic behaviors and their consequential impacts. Additionally, we propose a novel framework, Human Feedback-based Retrieval-Augmented Generation (HFRAG), to mitigate sycophancy in LVLMs by determining the appropriate timing of retrieval, profiling the proper retrieval target, and augmenting the decoding of LVLMs. Extensive experiments demonstrate that the proposed method significantly mitigates sycophancy in LVLMs without requiring additional training. Our code is available at: https://github.com/immc-lab/SyEval-VL",,Black-box,Society,该论文研究的是大型视觉语言模型（LVLMs）中的谄媚现象（sycophancy），即模型生成反映用户信念而非事实的偏见响应。研究涉及评估和缓解这种现象，属于社会学领域，关注模型的信任度和安全性问题。研究方法是通过输入输出（多轮对话格式）进行评估和缓解，没有涉及模型内部机制，因此属于黑盒方法。
Investigating the Robustness of Deductive Reasoning with Large Language Models,2025,True,True,True,,论文研究了LLM在演绎推理中的鲁棒性，涉及推理过程和忠实性问题。,"Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based NLP tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks. Moreover, while many LLM-based deduction methods have been proposed, a systematic study that analyses the impact of their design components is lacking. Addressing these two challenges, we propose the first study of the robustness of formal and informal LLM-based deductive reasoning methods. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.",arXiv.org,Black-box,Logic,该论文研究大型语言模型在演绎推理任务中的鲁棒性，主要关注输入输出和Prompting方法，未涉及模型内部机制，因此属于黑盒方法。研究领域为逻辑推理，属于Logic领域。
Stepwise Reasoning Error Disruption Attack of LLMs,2024,False,True,False,,论文讨论了LLM推理过程中的错误注入问题，但与CoT忠实性无直接联系。,"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.",arXiv.org,Black-box,General,该研究提出了一种通过逐步注入错误来误导大型语言模型的方法（SEED攻击），主要依赖于输入输出层面的操控（Prompting），并未涉及模型内部机制的分析或修改，因此属于黑盒方法。任务领域未明确限定在数学、逻辑等特定领域，而是探讨了模型在通用推理任务中的鲁棒性和安全性问题。
"Ask Again, Then Fail: Large Language Models’ Vacillations in Judgment",2024,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文揭示了LLMs在后续问题中判断不一致的现象，提出了度量指标和训练框架来提升忠实性。,"We observe that current large language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a signiﬁcant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a F OLLOW - UP Q UESTIONING M ECHANISM along with two metrics to quantify this inconsistency, conﬁrming its widespread presence in current large language models. Furthermore, to mitigate this issue, we explore various prompting strategies for closed-source models, and develop a training-based framework U NWAVERING -FQ that teaches large language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental re-sults conﬁrm the effectiveness of our framework and its ability to enhance the general capabilities of large language models.",Volume 1,Black-box,General,该研究关注大型语言模型在回答后续问题时的判断不一致性，并通过提示策略和训练框架来改善这一问题。研究主要基于模型的输入输出行为，没有涉及模型内部机制，因此属于黑盒方法。任务领域不特定于数学、逻辑、代码或医学，而是涉及模型的通用行为，因此归类为通用领域。
Answering Questions by Meta-Reasoning over Multiple Chains of Thought,2023,True,False,False,Consistency & Ensembling,论文提出多链推理方法，通过聚合多个CoT提升解释质量和答案预测，涉及忠实性改进。,"Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.",Conference on Empirical Methods in Natural Language Processing,Black-box,General,该研究通过Prompting大型语言模型进行元推理，仅涉及输入输出，未涉及模型内部机制，因此属于黑盒方法。任务领域是多跳问答，不特定于数学、逻辑、代码、医学或社会学，因此归类为通用领域。
A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models,2025,False,True,False,,论文综述了CoT推理的可信度，但未具体讨论忠实性问题。,"The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",arXiv.org,Black-box,General,论文标题和摘要主要讨论了大型语言模型在推理过程中的可信度问题，涉及多个维度如真实性、安全性、鲁棒性、公平性和隐私性。研究内容主要基于模型的输入输出行为，没有涉及模型内部机制或权重调整，因此归类为黑盒方法。任务领域涉及多个方面，不属于特定的数学、逻辑、代码或医学领域，因此归类为通用领域。
"On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",2022,False,True,False,,论文探讨了CoT在敏感领域中的负面影响，但未涉及Faithfulness的核心定义。,"Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",Annual Meeting of the Association for Computational Linguistics,Black-box,Society,该研究通过输入输出（Prompting）评估零样本思维链（CoT）在社会敏感领域的影响，属于黑盒方法。研究关注的是社会敏感领域中的偏见和毒性问题，涉及社会学领域。
"'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?",2025,True,True,True,,论文探讨了LLMs在决策过程中对SES的偏好，并揭示了System 2模式下CoT可能放大不忠实现象。,"Large Language Models (LLMs) are increasingly involved in high-stakes domains, yet how they reason about socially sensitive decisions remains underexplored. We present a large-scale audit of LLMs'treatment of socioeconomic status (SES) in college admissions decisions using a novel dual-process framework inspired by cognitive science. Leveraging a synthetic dataset of 30,000 applicant profiles grounded in real-world correlations, we prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2 modes: a fast, decision-only setup (System 1) and a slower, explanation-based setup (System 2). Results from 5 million prompts reveal that LLMs consistently favor low-SES applicants -- even when controlling for academic performance -- and that System 2 amplifies this tendency by explicitly invoking SES as compensatory justification, highlighting both their potential and volatility as decision-makers. We then propose DPAF, a dual-process audit framework to probe LLMs'reasoning behaviors in sensitive applications.",,Black-box,Society,该研究通过输入输出（Prompting）方式评估大型语言模型在高校录取决策中对社会经济因素的考量，未涉及模型内部机制，因此属于黑盒方法。研究领域涉及社会经济因素和人类行为，属于社会学范畴。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,Black-box,Logic,该论文研究的是基于自然逻辑的事实验证系统，通过问题回答来预测自然逻辑操作符，利用了指令调优语言模型的泛化能力，属于黑盒方法。任务领域涉及逻辑推理和语义关系，因此归类为Logic。
"Do as We Do, Not as You Think: the Conformity of Large Language Models",2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了LLM在多智能体系统中的从众行为，涉及推理过程和忠实性问题，并提出了改进方法。,"Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and groupthink in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BenchForm, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs' behavior in collaborative scenarios. Several representative LLMs are evaluated on BenchForm, using metrics such as conformity rate and independence rate to quantify conformity's impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalizes its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced personas and implementing a reflection mechanism. Several interesting findings regarding LLMs' conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code are available at BenchForm.",International Conference on Learning Representations,Black-box,Society,该论文研究大型语言模型在多智能体系统中的从众行为，涉及人类群体动态中的从众偏见和群体思维现象，属于社会学领域。研究仅通过输入输出和交互协议来评估模型行为，未涉及模型内部机制，因此属于黑盒方法。
"All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language",2025,False,False,False,,论文关注CoT监控和加密推理，但未直接讨论忠实性或不忠实现象。,"Detecting harmful AI actions is important as AI agents gain adoption. Chain-of-thought (CoT) monitoring is one method widely used to detect adversarial attacks and AI misalignment. However, attackers and misaligned models might evade CoT monitoring through ciphered reasoning: reasoning hidden in encrypted, translated, or compressed text. To assess this risk, we test whether models can perform ciphered reasoning. For each of 28 different ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We measure model accuracy on math problems as a proxy for reasoning ability. Across the models we test, we find an asymmetry: model accuracy can drop significantly when reasoning in ciphered text, even though models demonstrate comprehension of ciphered text by being able to translate it accurately to English. Even frontier models struggle with lesser-known ciphers, although they can reason accurately in well-known ciphers like rot13. We show that ciphered reasoning capability correlates with cipher prevalence in pretraining data. We also identify scaling laws showing that ciphered reasoning capability improves slowly with additional fine-tuning data. Our work suggests that evading CoT monitoring using ciphered reasoning may be an ineffective tactic for current models and offers guidance on constraining the development of this capability in future frontier models.",arXiv.org,Black-box,Code,该研究通过测试模型在加密文本中的推理能力来评估其性能，主要关注的是模型的输入输出行为（Prompting），并未涉及模型内部机制的分析，因此属于黑盒方法。研究领域涉及代码（加密文本的处理和推理），因此归类为Code。
An Empirical Study of Reasoning Steps in Thinking Code LLMs,2025,False,False,True,,论文研究了CoT在代码生成中的质量，但未涉及忠实性问题。,"Thinking Large Language Models (LLMs) generate explicit intermediate reasoning traces before final answers, potentially improving transparency, interpretability, and solution accuracy for code generation. However, the quality of these reasoning chains remains underexplored. We present a comprehensive empirical study examining the reasoning process and quality of thinking LLMs for code generation. We evaluate six state-of-the-art reasoning LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking, Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code generation tasks of varying difficulty from BigCodeBench. We quantify reasoning-chain structure through step counts and verbosity, conduct controlled step-budget adjustments, and perform a 21-participant human evaluation across three dimensions: efficiency, logical correctness, and completeness. Our step-count interventions reveal that targeted step increases can improve resolution rates for certain models/tasks, while modest reductions often preserve success on standard tasks, rarely on hard ones. Through systematic analysis, we develop a reasoning-problematic taxonomy, identifying completeness as the dominant failure mode. Task complexity significantly impacts reasoning quality; hard problems are substantially more prone to incompleteness than standard tasks. Our stability analysis demonstrates that thinking LLMs maintain consistent logical structures across computational effort levels and can self-correct previous errors. This study provides new insights into the strengths and limitations of current thinking LLMs in software engineering.",,Black-box,Code,该论文研究的是大型语言模型（LLMs）在代码生成过程中的推理步骤和效果，主要通过对输入输出（代码生成任务的表现）的分析进行评估，并未涉及模型内部机制或权重调整，因此属于黑盒方法。研究领域聚焦于代码生成任务，因此归类为Code。
FLamE: Few-shot Learning from Natural Language Explanations,2023,False,True,False,Training & Fine-tuning,论文关注自然语言解释在分类任务中的效用，未涉及CoT或Faithfulness。,"Natural language explanations have the potential to provide rich information that in principle guides model reasoning.Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification.To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations.Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI.Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions.Additional analyses point to the important role of label-specific cues (e.g., “not know” for the neutral label) in generated explanations.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该研究主要利用GPT-3生成自然语言解释，并通过微调较小的模型（如RoBERTa）来学习这些解释。研究涉及的是模型的输入输出和Prompting，没有涉及模型内部机制或权重调整，因此属于黑盒方法。任务领域是自然语言推理，不属于数学、逻辑、代码、医学或社会学等特定领域，因此归类为通用领域。
T-FIX: Text-Based Explanations with Features Interpretable to eXperts,2025,False,False,True,,论文关注专家对齐而非 CoT 忠实性，未涉及中间推理步骤或内部计算过程。,"As LLMs are deployed in knowledge-intensive settings (e.g., surgery, astronomy, therapy), users expect not just answers, but also meaningful explanations for those answers. In these settings, users are often domain experts (e.g., doctors, astrophysicists, psychologists) who require explanations that reflect expert-level reasoning. However, current evaluation schemes primarily emphasize plausibility or internal faithfulness of the explanation, which fail to capture whether the content of the explanation truly aligns with expert intuition. We formalize expert alignment as a criterion for evaluating explanations with T-FIX, a benchmark spanning seven knowledge-intensive domains. In collaboration with domain experts, we develop novel metrics to measure the alignment of LLM explanations with expert judgment.",arXiv.org,Black-box,General,论文主要研究如何评估LLM生成的解释是否与专家直觉一致，并未涉及模型内部机制或权重调整，而是通过输入输出和Prompting进行研究。任务领域涉及多个知识密集型领域，但未具体限定在数学、逻辑、代码或医学等特定领域，因此归类为通用。
It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning,2023,False,False,False,,论文探讨了CoT在错误答案推理中的效果，但未涉及Faithfulness或不忠实现象。,"Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.",Annual Meeting of the Association for Computational Linguistics,Black-box,Logic,该研究通过Prompting（链式思考提示）探索大型语言模型在排除法推理中的表现，仅涉及输入输出分析，未涉及模型内部机制，因此属于黑盒方法。研究领域涉及逻辑推理，特别是排除法在多选题中的应用。
"Ask Again, Then Fail: Large Language Models' Vacillations in Judgement",2023,False,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了模型判断的不一致性，但未涉及CoT或Faithfulness。,"We observe that current conversational language models often waver in their judgments when faced with follow-up questions, even if the original judgment was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a \textsc{Follow-up Questioning Mechanism} along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework \textsc{Unwavering-FQ} that teaches language models to maintain their originally correct judgments through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models.",Annual Meeting of the Association for Computational Linguistics,Black-box,General,该研究通过输入输出（Prompting）和API来评估语言模型在判断上的不一致性，并探索了不同的Prompting策略，因此属于黑盒方法。研究领域涉及语言模型的通用行为，不特定于数学、逻辑、代码、医学或社会学等具体领域，因此归类为通用。
Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques,2025,True,True,False,Prompting & In-Context Learning,论文讨论了模型的对齐伪造现象，并提出了基于提示的干预方法，与CoT忠实性相关。,"Current literature suggests that alignment faking is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can also exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based interventions are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for deceptive alignment evaluations across model sizes and deployment settings.",,Black-box,Society,该论文研究的是语言模型的对齐伪装行为，并通过Prompt干预减少这种行为，属于社会学领域（涉及道德和行为）。由于研究仅通过Prompt进行干预，不涉及模型内部机制，因此属于黑盒方法。
Intent-Aware Self-Correction for Mitigating Social Biases in Large Language Models,2025,True,False,False,Prompting & In-Context Learning,论文使用CoT澄清推理过程，并通过自我纠正减少偏见，涉及忠实性改进。,"Self-Correction based on feedback improves the output quality of Large Language Models (LLMs). Moreover, as Self-Correction functions like the slow and conscious System-2 thinking from cognitive psychology's perspective, it can potentially reduce LLMs' social biases. LLMs are sensitive to contextual ambiguities and inconsistencies; therefore, explicitly communicating their intentions during interactions when applying Self-Correction for debiasing is crucial. In this study, we demonstrate that clarifying intentions is essential for effectively reducing biases in LLMs through Self-Correction. We divide the components needed for Self-Correction into three parts: instruction, response, and feedback, and clarify intentions at each component. We incorporate an explicit debiasing prompt to convey the intention of bias mitigation from the instruction for response generation. In the response, we use Chain-of-Thought (CoT) to clarify the reasoning process. In the feedback, we define evaluation aspects necessary for debiasing and propose clear feedback through multi-aspect critiques and scoring. Through experiments, we demonstrate that self-correcting CoT responses obtained from a debiasing prompt based on multi-aspect feedback can reduce biased responses more robustly and consistently than the baselines. We also find the variation in debiasing efficacy when using models with different bias levels or separating models for response and feedback generation.",arXiv.org,Black-box,Society,该论文研究的是通过自我纠正（Self-Correction）来减少大型语言模型（LLMs）中的社会偏见，属于社会学领域。研究方法主要依赖于输入输出、Prompting 和反馈机制，没有涉及模型内部机制或权重调整，因此归类为黑盒方法。

title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,True,False,False,,论文探讨了自动事实核查系统需要提供解释以符合事实核查者的需求，与CoT忠实性相关。,"The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model’s reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",International Conference on Human Factors in Computing Systems,1,Black-box,Society,论文研究了自动化事实核查系统如何提供解释以符合事实核查员的工作流程，虽然涉及解释的真实性和可追溯性，但未明确探讨 CoT 是否忠实反映模型的计算过程，属于边缘相关。
Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出逆向推理方法，通过元认知结构提升模型对自身推理链的解释能力，涉及忠实性。,"Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.",arXiv.org,2,White-box,General,该论文明确研究LLM如何通过逆向推理（inverse reasoning）分解和解释自己的推理链，涉及模型内部注意力机制的反向流动，直接探讨了推理链选择的真实原因（causal role），属于CoT Faithfulness的核心研究范畴。
multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,2021,True,False,False,Training & Fine-tuning,论文提出生成多个证明图以提高解释性，涉及 CoT 和 Faithfulness，但未讨论不忠实现象或新度量。,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent.",North American Chapter of the Association for Computational Linguistics,1,Black-box,Logic,论文研究生成多个证明图以提高规则推理的可解释性，虽然涉及推理过程的解释，但未明确探讨这些解释是否忠实反映模型的实际计算过程。
Thought Anchors: Which LLM Reasoning Steps Matter?,2025,True,False,True,,论文提出了黑盒方法评估CoT中句子的重要性，与Faithfulness相关。,"Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. Many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences \textit{thought anchors}. These are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within a reasoning trace gives insight into a model's behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our techniques yield a consistent picture of the reasoning trace's structure. We provide an open-source tool (thought-anchors.com) for visualizing the outputs of our methods on further problems. The convergence across our methods shows the potential of sentence-level analysis for a deeper understanding of reasoning models.",arXiv.org,2,Black-box,Math,该论文明确研究推理过程中哪些句子（CoT步骤）对最终答案有因果影响，通过反事实替换和量化影响的方法测量忠实度，并发现关键句子（thought anchors）对推理轨迹的决定性作用，符合Faithfulness的核心定义。
Counterfactual Evaluation for Explainable AI,2021,True,False,True,,论文提出了基于反事实推理的新评估方法，直接针对解释的忠实性度量。,"While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",arXiv.org,2,Black-box,General,该论文明确研究解释的忠实度（faithfulness），并提出了一种基于反事实推理（counterfactual reasoning）的新方法来评估解释是否真实反映了模型的推理过程。这与 CoT Faithfulness 的核心定义高度相关，特别是通过反事实输入来测量忠实度。
Diagnostics-Guided Explanation Generation,2021,True,False,True,Training & Fine-tuning,论文讨论了如何优化解释的忠实性，并提出了直接优化诊断属性的训练方法。,"Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究解释生成的忠实度（Faithfulness），并提出了优化忠实度的方法，同时涉及其他诊断属性如数据一致性和置信度指示，这些都是评估解释是否真实反映模型推理过程的关键指标。
Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了语言模型在生成CoT时的内部状态和不确定性，与Faithfulness相关，并提出了基于激活干预的改进方法。,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",arXiv.org,2,White-box,General,该论文明确研究语言模型在生成链式思考（CoT）时的内部机制，通过隐藏激活状态来控制和预测模型的不确定性，探讨模型是否意识到未选择的推理路径。这与Faithfulness的核心定义直接相关，因为它涉及模型生成的推理路径是否真实反映了其内部计算过程，而非事后合理化。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,论文明确提出了一个自解释的GNN框架X-Node，其中每个节点在预测过程中生成自己的解释，并通过解码器强制忠实度（faithfulness）。此外，论文还讨论了如何通过解释向量来重建节点的潜在嵌入，以确保解释的真实性，这与CoT Faithfulness的核心定义直接相关。
Training and Evaluation of Guideline-Based Medical Reasoning in LLMs,2025,True,False,True,Training & Fine-tuning,论文通过微调LLMs遵循医学指南进行推理，提出了评估推理过程忠实性的方法。,"Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",,2,Black-box,Medical,论文明确研究如何让LLM遵循医学共识指南进行逐步推理和预测，并自动评估推理过程的正确性和忠实度（derivation correctness and value correctness），直接涉及CoT的忠实性。
Unsupervised decoding of encoded reasoning using language model interpretability,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了通过内部激活解码 CoT 的方法，属于 Faithfulness 的白盒评估。,"As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.",,2,White-box,General,该论文明确研究模型内部机制（如logit lens分析）是否能解码隐藏的推理过程（ROT-13加密的CoT），直接涉及CoT的忠实性验证。通过分析内部激活值重建推理轨迹，属于白盒方法，且任务领域不特定于数学或逻辑等，故归类为通用领域。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,2,Black-box,General,论文明确提到其推理轨迹（reasoning traces）是忠实的（faithful），并且通过专家反馈和自动分析进行了验证。这直接符合 Faithfulness 的核心定义，即模型生成的解释是否真实反映了模型做出预测的实际计算过程。
TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出通过关键词蒸馏和LLM推理提升解释的忠实性，并提出了新的评估指标。,"Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.",arXiv.org,1,Black-box,Medical,论文研究了通过关键词蒸馏和LLM推理提高临床文本解释的可信度，涉及解释的忠实性（fidelity）评估，但未明确探讨CoT是否是模型预测的真实原因。
Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem,2024,True,False,False,Prompting & In-Context Learning,论文提出了一种通过角色建模和反馈机制改进自然语言解释生成的方法，属于 CoT 忠实性改进方法。,"Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",International Conference on Computational Linguistics,1,Black-box,General,论文研究了自然语言解释（NLEs）的生成和优化，通过两个LLM的协作来改进解释的质量。虽然涉及解释的生成和优化，但未明确探讨这些解释是否真实反映了模型的预测过程，因此属于边缘相关。
ProoFVer: Natural Logic Theorem Proving for Fact Verification,2021,True,False,False,Verification & External Tools,论文提出使用自然逻辑推理作为忠实解释，通过构造确保忠实性。,"Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",Transactions of the Association for Computational Linguistics,2,Black-box,Logic,论文明确提出了 ProoFVer 系统，通过生成基于自然逻辑的推理作为证明，这些证明忠实反映了模型预测的依据（即自然逻辑运算符序列），因此属于 'Faithfulness by construction' 的设计。此外，论文还通过对抗性数据集（counterfactual instances）验证了其鲁棒性，并对比了人类理性解释的重叠度，这些都直接符合 CoT Faithfulness 的核心定义。
Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs,2022,True,False,True,Training & Fine-tuning,论文提出了一种中间信念跟踪框架，与 CoT 的中间性和忠实性相关，并提出了新的评估指标。,"Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文提出了一个框架来建模和跟踪中间信念，虽然涉及透明度和可靠性，但未明确研究CoT是否真实反映模型预测的计算过程。因此，它属于边缘相关。
Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,2022,True,False,False,Training & Fine-tuning,论文提出了一种新的解释方法，并通过微调学生模型来提高忠实性。,"Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is""honest""in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the""untrusted""teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",arXiv.org,1,Black-box,General,该论文研究如何通过学生模型从预训练语言模型中学习可解释的问答流程，并生成合理的解释。虽然涉及解释的生成和验证，但主要关注的是解释的实用性和准确性，而非直接研究 CoT 是否忠实反映模型的内部计算过程。因此，它属于边缘相关的研究。
Rationale-Guided Few-Shot Classification to Detect Abusive Language,2022,True,False,False,Training & Fine-tuning,论文提出基于rationale的few-shot分类方法，涉及CoT和Faithfulness，但未讨论不忠实现象或提出新度量。,"Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",European Conference on Artificial Intelligence,1,Black-box,Society,论文研究了基于理性（rationales）的少样本分类方法，并提到了与LIME/SHAP方法在忠实度（faithfulness）方面的比较。虽然论文主要关注模型性能提升和跨域评估，但提到了忠实度的概念，属于边缘相关。
Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用CoT和强化学习提升医疗推理的忠实性，但未讨论不忠实现象或提出新度量。,"While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",arXiv.org,1,Black-box,Medical,论文主要关注通过强化学习提升医疗推理的准确性和透明性，虽然提到了 Chain-of-Thought (CoT) 和推理轨迹的质量，但未明确研究 CoT 是否真实反映模型预测的计算过程（Faithfulness）。因此属于边缘相关。
"From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了推理痕迹对答案生成的影响，并通过干预实验验证了推理与答案之间的因果关系。,"Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \href{https://aka.ms/R2A-code}{this URL}.",,2,White-box,General,该论文明确研究了推理痕迹（CoT）对答案生成的实际影响，通过注意力分析和机制干预（如激活修补）验证了推理痕迹在模型预测中的因果作用，符合Faithfulness的核心定义。
Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning,2025,True,False,False,Verification & External Tools,论文提出交互式框架 Vis-CoT，通过人类干预提升 CoT 的准确性和可信度，属于改进方法中的验证与外部工具。,"Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.",arXiv.org,1,Black-box,General,该论文提出了一个交互式框架 Vis-CoT，允许用户可视化和干预 CoT 推理过程，以提高准确性和可信度。虽然它涉及 CoT 的可解释性和用户干预，但并未明确研究 CoT 是否真实反映了模型的内部计算过程（Faithfulness），因此属于边缘相关。
REV: Information-Theoretic Evaluation of Free-Text Rationales,2022,True,False,True,,论文提出了REV指标评估自由文本推理的新信息，与CoT忠实性相关。,"Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文提出了一个信息论指标 REV 来评估自由文本解释的新信息量，虽然未直接研究 CoT 的忠实度，但提供了评估解释质量的工具，可能间接支持 Faithfulness 研究。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确研究生成式推理过程，通过分解任务并构建中间结论树来忠实反映系统的推理过程（'faithfully reflects the system's reasoning process'），直接符合 Faithfulness 的核心定义。实验部分还对比了生成解释的步骤有效性，进一步验证了其忠实性。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文明确提出了通过分解任务生成子问题并提供自然语言解释的方法，强调了解释的忠实性（faithful natural language explanation），符合 CoT Faithfulness 的核心定义。
EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers,2022,True,False,False,,论文提出了一种生成数学问题解释的模型，并讨论了忠实性，但未涉及不忠实现象或新度量方法。,"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Math,论文明确提出了 faithfulness 的概念，并定义为准确反映模型推理过程的解释，与 CoT Faithfulness 的核心定义完全一致。同时，论文还提出了 plausibility 作为补充，但核心关注点仍是解释是否忠实于模型的推理过程。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",1,Black-box,General,论文提到 Chain-of-Thoughts 是 faithful 且有助于提升 prompt 性能，但未深入探讨 CoT 是否真实反映模型的计算过程。主要关注的是 ChatGPT 在因果语言理解上的表现，属于边缘相关。
ERASER: A Benchmark to Evaluate Rationalized NLP Models,2019,True,False,True,,论文提出了评估模型解释忠实性的新指标和框架，与CoT Faithfulness相关。,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确提出了一个基准测试（ERASER），用于评估模型提供的解释（rationales）是否忠实（faithful），即这些解释是否真正影响了模型的预测。这与CoT Faithfulness的核心定义直接相关，因为它关注的是解释与模型预测之间的因果关系。
PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出生成详细的 CoT 解释，并评估其 grounded 和清晰度，涉及忠实性度量。,"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-ofthought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded (score 0.87) and clear (readability 4.5/5). This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,1,Black-box,General,论文提出了一个框架，结合了传感器数据和视觉语言模型生成详细的链式思考解释，并评估了解释的grounded程度和清晰度。虽然涉及CoT的解释生成和评估，但未明确研究这些解释是否忠实反映了模型的实际计算过程，因此属于边缘相关。
Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了FUR框架，通过消除推理步骤中的信息来测量忠实性，揭示了不忠实现象并提出了新的度量方法。,"When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",,2,White-box,General,该论文明确研究 CoT 的忠实度，提出了一个框架（FUR）来测量生成的推理步骤是否忠实于模型的参数信念。通过从模型参数中擦除推理步骤的信息，并测量对模型预测的影响，来评估忠实度。这直接符合 Faithfulness 的核心定义，即研究 CoT 是否是模型预测的真实原因。
Effectively Controlling Reasoning Models through Thinking Intervention,2025,True,False,False,Interpretability & Internal Mechanisms,论文提出通过干预内部推理过程来增强模型行为控制，与CoT忠实性相关。,"Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval and Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.",arXiv.org,1,White-box,General,该论文提出了Thinking Intervention方法，通过干预内部推理过程来控制模型行为，虽然未直接研究CoT的忠实度，但涉及了推理过程的内部机制，属于边缘相关。
The Geometry of Self-Verification in a Task-Specific Reasoning Model,2025,True,False,False,Interpretability & Internal Mechanisms,论文研究了模型如何自我验证其推理过程，涉及CoT的忠实性，并提出了白盒干预方法。,"How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, yielding a model that always produces highly structured chain-of-thought sequences. With this setup, we do top-down and bottom-up analyses to reverse-engineer how the model verifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect''. Bottom-up, we find that ``previous-token heads'' are mainly responsible for self-verification in our setup. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU weights to localize as few as three attention heads that can disable self-verification, pointing to a necessary component of a potentially larger verification circuit. Finally, we verify that similar verification components exist in our base model and a general reasoning DeepSeek-R1 model.",arXiv.org,2,White-box,Logic,该论文明确研究了模型如何验证自己的答案，通过分析模型内部机制（如GLU权重和注意力头）来理解自我验证过程，这与CoT Faithfulness的核心定义直接相关，即研究解释是否真实反映了模型的实际计算过程。
SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过对抗自演游戏提升 CoT 忠实性，属于改进方法中的训练与微调。,"Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a""sneaky generator""that deliberately produces erroneous steps designed to be difficult to detect, and a""critic""that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.",arXiv.org,1,Black-box,General,该论文通过对抗自博弈游戏评估LLM推理步骤的可靠性，虽然未直接研究CoT的忠实度，但涉及推理步骤的正确性分析，可作为Faithfulness的旁证。
"Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving",2025,True,False,True,Verification & External Tools,论文提出基于形式化验证的框架和度量标准，关注过程级验证，与CoT忠实性相关。,"As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",arXiv.org,1,Black-box,Math,论文提出了一个形式化问题解决框架，并关注过程级可验证性（process-level verifiability），这与 CoT Faithfulness 有一定关联。然而，论文主要关注的是问题解决的形式化验证和人类对齐，并未明确研究 CoT 是否是模型预测的真实原因或测量其忠实度。因此，它属于边缘相关。
Bonsai: Interpretable Tree-Adaptive Grounded Reasoning,2025,True,False,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种可解释的推理系统，强调透明推理和验证，与CoT忠实性相关。,"To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.",arXiv.org,1,Black-box,General,该论文提出了一个可解释的推理系统Bonsai，能够生成适应性推理树，并通过检索相关证据来计算子主张的可能性。虽然它关注推理的透明性和不确定性，但并未明确研究CoT是否真实反映模型预测的实际计算过程，因此属于边缘相关。
Anchored Alignment for Self-Explanations Enhancement,2024,True,False,False,Training & Fine-tuning,论文提出了一种增强模型自我解释能力的方法，涉及训练和微调策略。,"In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.",arXiv.org,1,Black-box,General,论文研究如何通过对齐方法增强LLM的自我解释能力，但未明确探讨解释是否忠实反映模型的真实计算过程。虽然涉及解释质量评估，但更侧重于解释的效用而非因果分析。
Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach,2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出了一种神经符号方法LINA，旨在提升逻辑推理的忠实性，减少对外部求解器的依赖。,"Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",arXiv.org,1,Black-box,Logic,论文提出了一个神经符号方法（LINA）来增强逻辑推理的忠实性，但主要关注的是推理过程的鲁棒性和消除对外部求解器的依赖，未明确研究CoT是否是模型预测的真实原因或测量Faithfulness的指标。
OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models,2024,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出了离线评估框架OCEAN，并利用知识图谱和RL优化CoT的忠实性。,"Offline evaluation of LLMs is crucial in understanding their capacities, though current methods remain underexplored in existing research. In this work, we focus on the offline evaluation of the chain-of-thought capabilities and show how to optimize LLMs based on the proposed evaluation method. To enable offline feedback with rich knowledge and reasoning paths, we use knowledge graphs (e.g., Wikidata5m) to provide feedback on the generated chain of thoughts. Due to the heterogeneity between LLM reasoning and KG structures, direct interaction and feedback from KGs on LLM behavior are challenging, as they require accurate entity linking and grounding of LLM-generated chains of thought in the KG. To address the above challenge, we propose an offline chain-of-thought evaluation framework, OCEAN, which models chain-of-thought reasoning in LLMs as an MDP and evaluate the policy's alignment with KG preference modeling. To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy KG exploration and RL to model a KG policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating KG reasoning preference. Then we incorporate the knowledge-graph feedback on the validity and alignment of the generated reasoning paths into inverse propensity scores and propose KG-IPS estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS estimator and provide a lower bound on its variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance chain-of-thought alignment. Our empirical study shows that OCEAN can be efficiently optimized for generating chain-of-thought reasoning paths with higher estimated values without affecting LLMs' general abilities in downstream tasks or their internal knowledge.",arXiv.org,1,Black-box,General,论文主要研究如何通过知识图谱反馈优化LLM的CoT能力，并提出了离线评估框架OCEAN。虽然涉及CoT的评估和优化，但未明确探讨CoT是否真实反映模型的预测过程，因此属于边缘相关。
The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models,2024,True,False,True,,论文提出了新的度量指标CEF和CCT，用于评估CoT的忠实性，但未讨论不忠实现象或改进方法。,"In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究大型语言模型生成的自然语言解释的忠实性问题，提出了新的度量标准（CEF）来评估解释是否真实反映模型的预测过程，并基于输入干预进行测试。这直接符合Faithfulness的核心定义，特别是关于解释是否真实捕捉模型预测因素的问题。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,2,Black-box,General,论文明确研究 Chain-of-Thought 的解释性，并特别提到 faithfulness 作为评估的一个维度。此外，还研究了 robustness 和 utility，这些都是与 CoT Faithfulness 相关的核心内容。
Understanding Reasoning in Chain-of-Thought from the Hopfieldian View,2024,True,False,True,Interpretability & Internal Mechanisms,论文提出了基于Hopfieldian认知视角的RoT框架，增强CoT推理的鲁棒性和可解释性。,"Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on improving performance, lacking a comprehensive framework to explain and understand the fundamental factors behind CoT's success. To bridge this gap, we introduce a novel perspective grounded in the Hopfieldian view of cognition in cognitive neuroscience. We establish a connection between CoT reasoning and key cognitive elements such as stimuli, actions, neural populations, and representation spaces. From our view, we can understand the reasoning process as the movement between these representation spaces. Building on this insight, we develop a method for localizing reasoning errors in the response of CoTs. Moreover, we propose the Representation-of-Thought (RoT) framework, which leverages the robustness of low-dimensional representation spaces to enhance the robustness of the reasoning process in CoTs. Experimental results demonstrate that RoT improves the robustness and interpretability of CoT reasoning while offering fine-grained control over the reasoning process.",arXiv.org,1,White-box,General,论文从Hopfieldian认知视角研究CoT的内部机制，涉及神经群体和表征空间等认知元素，并定位推理错误，但未明确讨论CoT是否忠实反映模型的实际计算过程。
Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology,2025,True,False,True,Verification & External Tools,论文提出基于推理拓扑的框架量化LLM解释的不确定性，与CoT忠实性相关，但未讨论不忠实现象或改进方法。,"Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",arXiv.org,2,Black-box,General,该论文明确研究LLM解释的不确定性，并基于推理拓扑视角量化这种不确定性，直接关联到Faithfulness和推理一致性。通过分解解释为知识相关的子问题和基于拓扑的推理结构，该方法能够评估知识冗余并提供对推理过程的可解释性洞察，从而增强鲁棒性和忠实度。
Automated Assessment of Fidelity and Interpretability: An Evaluation Framework for Large Language Models' Explanations (Student Abstract),2024,True,False,True,,论文提出了评估LLM解释忠实性的框架，但未讨论不忠实现象或改进方法。,"As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability: fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations. We apply our framework to evaluate GPT-3.5 and the impact of prompts on the quality of its explanations. In conclusion, our framework streamlines the evaluation of explanations from LLMs, promoting the development of safer models.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究了解释的忠实度（fidelity），并提出了评估框架，特别关注专有LLM无法直接访问内部特征时的评估方法。这与CoT Faithfulness的核心定义直接相关，尤其是关于解释是否真实反映模型预测过程的研究。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究生成忠实解释（faithful rationales）的问题，并提出了一种新的损失函数（MLR）来促进解释与预测标签之间的强条件关系，符合Faithfulness的核心定义。
Verifying Chain-of-Thought Reasoning via Its Computational Graph,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了一种白盒方法验证CoT的计算图，直接关联模型内部电路与推理过程，符合Faithfulness定义。,"Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",arXiv.org,2,White-box,General,该论文明确研究 CoT 的计算图（computational graph）作为模型潜在推理电路的执行轨迹，并验证其结构特征是否能预测推理错误。这直接涉及 CoT 是否真实反映模型的计算过程（Faithfulness），且通过白盒方法（如干预 transcoder 特征）证明其因果性，符合核心相关标准。
LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning,2025,True,False,False,Verification & External Tools,论文提出了一种结合逻辑推理和LLM的架构，旨在提高推理的忠实性。,"High-assurance reasoning, particularly in critical domains such as law and medicine, requires conclusions that are accurate, verifiable, and explicitly grounded in evidence. This reasoning relies on premises codified from rules, statutes, and contracts, inherently involving defeasible or non-monotonic logic due to numerous exceptions, where the introduction of a single fact can invalidate general rules, posing significant challenges. While large language models (LLMs) excel at processing natural language, their capabilities in standard inference tasks do not translate to the rigorous reasoning required over high-assurance text guidelines. Core reasoning challenges within such texts often manifest specific logical structures involving negation, implication, and, most critically, defeasible rules and exceptions. In this paper, we propose a novel neurosymbolically-grounded architecture called LOGicalThought (LogT) that uses an advanced logical language and reasoner in conjunction with an LLM to construct a dual symbolic graph context and logic-based context. These two context representations transform the problem from inference over long-form guidelines into a compact grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines, LogT improves overall performance by 11.84% across all LLMs. Performance improves significantly across all three modes of reasoning: by up to +10.2% on negation, +13.2% on implication, and +5.5% on defeasible reasoning compared to the strongest baseline.",arXiv.org,0,Black-box,Logic,论文主要研究如何通过神经符号架构提升LLM在逻辑推理任务中的性能，未涉及CoT生成的解释是否忠实反映模型内部计算过程。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,1,Black-box,General,论文讨论了使用CoT来提高模型的安全性和鲁棒性，并提到通过deliberative alignment来增强模型对安全政策的推理能力。虽然涉及CoT的效用和潜在风险，但未明确研究CoT是否真实反映模型的预测过程，因此属于边缘相关。
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过MCTS和过程奖励模型提升CoT的忠实性，涉及训练和验证工具。,"We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising""deep thinking""through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",International Conference on Machine Learning,0,Black-box,Math,论文主要关注通过蒙特卡洛树搜索和自进化方法提升小型语言模型的数学推理能力，未涉及对CoT忠实度的研究。
GraphGhost: Tracing Structures Behind Large Language Models,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过GraphGhost框架分析LLM内部结构，涉及CoT的忠实性，但未讨论不忠实现象或提出新度量。,"Large Language Models (LLMs) demonstrate remarkable reasoning capabilities, yet the structural mechanisms underlying these abilities remain under explored. In this work, we introduce GraphGhost, a unified framework that represents neuron activations and their signal propagation as graphs, explaining how LLMs capture structural semantics from sequential inputs and generate outputs through structurally consistent mechanisms. This graph-based perspective enables us to employ graph algorithms such as PageRank to characterize the properties of LLMs, revealing both shared and model-specific reasoning behaviors across diverse datasets. We further identify the activated neurons within GraphGhost and evaluate them through structural interventions, showing that edits to key neuron nodes can trigger reasoning collapse, altering both logical flow and semantic understanding. Together, these contributions position GraphGhost as a powerful tool for analyzing, intervening in, and ultimately understanding the structural foundations of reasoning in LLMs.",arXiv.org,1,White-box,General,该论文研究了LLM内部神经元激活和信号传播的结构机制，通过图算法分析模型推理行为，并进行了结构干预实验。虽然未直接提及CoT Faithfulness，但其对模型内部机制的分析（如关键神经元节点对推理流程的影响）可为Faithfulness研究提供旁证，属于边缘相关。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的CoT traces的faithfulness，直接涉及CoT忠实度的研究，属于核心相关。研究领域涉及社会学中的多元价值观对齐，因此分类为Society。
Logic Agent: Enhancing Validity with Logic Rule Invocation,2024,True,False,False,Verification & External Tools,论文提出Logic Agent框架，通过逻辑规则调用增强推理有效性，属于改进方法中的验证与外部工具类别。,"Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.",arXiv.org,1,Black-box,Logic,该论文提出了Logic Agent框架，旨在通过逻辑规则调用来增强推理过程的有效性，虽然关注了推理的结构化和连贯性生成，但并未明确研究CoT是否真实反映了模型的实际计算过程，因此属于边缘相关。
Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics,2025,True,False,True,,论文提出了一个状态感知转换框架来分析CoT推理的语义角色和一致性，与Faithfulness相关。,"Recent advances in chain-of-thought (CoT) prompting have enabled large language models (LLMs) to perform multi-step reasoning. However, the explainability of such reasoning remains limited, with prior work primarily focusing on local token-level attribution, such that the high-level semantic roles of reasoning steps and their transitions remain underexplored. In this paper, we introduce a state-aware transition framework that abstracts CoT trajectories into structured latent dynamics. Specifically, to capture the evolving semantics of CoT reasoning, each reasoning step is represented via spectral analysis of token-level embeddings and clustered into semantically coherent latent states. To characterize the global structure of reasoning, we model their progression as a Markov chain, yielding a structured and interpretable view of the reasoning process. This abstraction supports a range of analyses, including semantic role identification, temporal pattern visualization, and consistency evaluation.",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文研究 CoT 的可解释性，通过分析 token 级嵌入和状态转换来抽象推理过程，虽然未直接研究 Faithfulness，但涉及推理的内部机制和一致性评估，可以作为 Faithfulness 的旁证。
Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation,2025,True,False,True,Verification & External Tools,论文提出了Chain-of-Image Generation框架，类似于CoT，并提出了新的度量指标来评估中间步骤的清晰度和因果相关性。,"While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a""black box.""This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",,0,Black-box,General,论文主要研究图像生成过程中的可监控性和可控性，提出了Chain-of-Image Generation (CoIG)框架，虽然借鉴了Chain-of-Thought (CoT)的思想，但并未涉及CoT的忠实度（Faithfulness）问题。研究重点在于图像生成的步骤分解和监控，而非模型推理过程的真实性。
XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs,2023,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出数据集和框架以增强LLM的透明度和可靠性，涉及CoT忠实性。,"Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM’s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the debugger-scores for multidimensional quality analysis. Our explanations include why-choose and why-not-choose components, reason-elements, and debugger-scores that collectively illuminate the LLM’s reasoning behavior. Our evaluations demonstrate XplainLLM’s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文提出了一个数据集和框架来增强LLM的透明度和可靠性，包括生成基于知识图谱的解释和多维质量分析。虽然它关注解释的可靠性和减少幻觉，但未明确研究CoT是否是模型预测的真实原因或测量Faithfulness的指标，因此属于边缘相关。
A Pragmatic Way to Measure Chain-of-Thought Monitorability,2025,True,False,True,,论文提出了衡量 CoT 可监控性的新指标，与 Faithfulness 相关，但未讨论不忠实现象或改进方法。,"While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI safety, this opportunity could be lost through shifts in training practices or model architecture. To help preserve monitorability, we propose a pragmatic way to measure two components of it: legibility (whether the reasoning can be followed by a human) and coverage (whether the CoT contains all the reasoning needed for a human to also produce the final output). We implement these metrics with an autorater prompt that enables any capable LLM to compute the legibility and coverage of existing CoTs. After sanity-checking our prompted autorater with synthetic CoT degradations, we apply it to several frontier models on challenging benchmarks, finding that they exhibit high monitorability. We present these metrics, including our complete autorater prompt, as a tool for developers to track how design decisions impact monitorability. While the exact prompt we share is still a preliminary version under ongoing development, we are sharing it now in the hopes that others in the community will find it useful. Our method helps measure the default monitorability of CoT - it should be seen as a complement, not a replacement, for the adversarial stress-testing needed to test robustness against deliberately evasive models.",arXiv.org,1,Black-box,General,论文提出了测量 CoT 可监控性的方法，包括 legibility 和 coverage，这些指标间接反映了 CoT 的忠实度，但并未直接研究 CoT 是否是模型预测的真实原因或欺骗性现象。
Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning,2025,True,False,True,Verification & External Tools,论文提出基于Curry-Howard对应关系的验证框架，将CoT映射为形式化证明，属于忠实性度量方法。,"While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",arXiv.org,2,White-box,Logic,该论文明确研究 CoT 的忠实性问题，提出了一种基于 Curry-Howard 对应的理论框架，将 CoT 的自然语言步骤映射为形式化的类型证明结构，以验证其计算忠实性。这直接符合 Faithfulness 的核心定义，属于核心相关研究。
Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过机制解释性方法分析Transformer模型内部电路，与CoT忠实性相关。,"Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified""recall circuits""reduces fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas disabling""reasoning circuits""reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",arXiv.org,1,White-box,General,该论文通过机制可解释性方法研究了Transformer模型中回忆和推理的内部机制，虽然未直接涉及CoT Faithfulness，但分析了不同层和注意力头对任务的影响，可以作为Faithfulness研究的旁证。
Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models,2025,True,False,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了LLMs中的推理算法原语及其几何组合，与CoT和Faithfulness相关，提出了度量框架和改进方法。,"How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",arXiv.org,2,White-box,General,该论文明确研究语言模型内部激活模式与推理轨迹的关联，通过注入算法原语并测量其对推理步骤的影响，直接验证了模型推理过程的忠实性。研究涉及模型内部机制（如残差流操作和激活聚类），属于白盒方法，且跨任务和跨模型的评估揭示了推理微调对算法泛化的影响，与CoT Faithfulness的核心定义高度相关。
Argumentative Large Language Models for Explainable and Contestable Claim Verification,2024,True,False,True,Verification & External Tools,论文提出通过论证框架提升解释性和可争议性，与CoT忠实性相关。,"The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing argumentative LLMs (ArgLLMs), a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs’ performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确研究如何使LLM的输出能够被忠实解释和有效质疑，这与CoT Faithfulness的核心定义直接相关。ArgLLMs通过构建论证框架来支持决策，并强调其可解释性和可争议性，这直接涉及解释是否真实反映模型预测的实际计算过程。
MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps,2024,True,False,True,,论文提出了评估多模态CoT质量的框架MiCEval，涉及CoT忠实性的度量。,"Multimodal Chain of Thought (MCoT) is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation (MiCEval), a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found in https://github.com/alenai97/MiCEval.",North American Chapter of the Association for Computational Linguistics,1,Black-box,General,该论文提出了一个评估多模态思维链（MCoT）质量的框架MiCEval，关注推理步骤的正确性、相关性和信息量。虽然它涉及推理步骤的质量评估，但并未明确研究这些步骤是否真实反映了模型的预测过程，因此属于边缘相关。
General Purpose Verification for Chain of Thought Prompting,2024,True,False,True,Verification & External Tools,论文提出验证 CoT 步骤的方法，提升推理准确性，属于忠实性改进。,"Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.",arXiv.org,1,Black-box,General,论文研究了通过验证推理步骤的数学准确性和逻辑一致性来提高CoT的准确性，但未直接探讨CoT是否真实反映模型的内部计算过程，属于边缘相关。
FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain,2024,True,False,True,Consistency & Ensembling,论文使用自洽性（self-consistency）和多数投票提升CoT性能，并报告了忠实性分数。,"This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improve the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make thefinal verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly.",International Workshop on Semantic Evaluation,1,Black-box,Medical,论文虽然提到了Faithfulness score和自我一致性（Self-consistency）作为CoT性能的一部分，但主要侧重于改进性能和准确率。未明确研究CoT是否是模型预测的真实原因或因果分析，因此归类为边缘相关。
CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning,2024,True,False,False,Training & Fine-tuning,论文提出CoMAT方法提升数学推理的忠实性和可验证性，属于改进方法中的训练与微调。,"Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",,2,Black-box,Math,论文明确提到 CoMAT 方法不仅提高了性能，还确保了忠实性（faithfulness）和可验证性（verifiability），提供了透明的推理过程。这直接符合 CoT Faithfulness 的研究范畴，即研究解释是否真实反映了模型的预测过程。
Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions,2023,True,False,True,,论文通过梯度特征归因方法评估CoT提示的忠实性，属于度量框架研究。,"Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",arXiv.org,2,White-box,General,该论文明确研究 CoT 提示对模型内部计算过程的影响，通过梯度特征归因方法分析输入 token 对输出的影响，直接涉及 CoT 是否真实反映模型预测的计算过程，属于 Faithfulness 核心研究。
Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models,2023,True,False,False,Interpretability & Internal Mechanisms,论文通过干预注意力层提升多跳推理的忠实性，涉及内部机制改进。,"Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,1,White-box,General,该论文研究了多跳推理失败的问题，并通过分析GPT-2模型的每层激活来提出修正方法。虽然涉及模型内部机制（白盒），但主要关注的是推理的鲁棒性和一致性，而非直接研究CoT的忠实度。因此，属于边缘相关。
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,2024,True,False,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文提出基于因果关系的提示方法，利用CoT作为中介变量，并通过微调提升忠实性。,"Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确研究 Chain-of-Thought 作为中介变量，并通过前门调整计算输入提示与输出答案之间的因果效应，直接涉及 CoT 是否真实反映模型预测的实际计算过程（Faithfulness）。此外，论文还讨论了如何通过对比学习微调 CoT 编码器以准确表示 CoT 并估计因果效应，进一步验证了 CoT 的忠实性。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,1,Black-box,General,论文讨论了 faithfulness 和 correctness 的关系，并提出了一个结合 faithfulness 和 reasoning 的奖励模型（TRM）。虽然涉及 faithfulness 的概念，但主要关注的是如何通过监督学习提升模型的批判性思考能力，而非直接研究 CoT 是否忠实反映模型的推理过程。因此属于边缘相关。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,0,Black-box,General,论文主要研究的是利用结构化推理框架（QBAF）来提高RAG系统的透明度和可解释性，但并未涉及Chain-of-Thought（CoT）的忠实性问题。研究重点在于如何通过确定性推理来增强决策的可解释性和可争议性，而不是验证CoT是否真实反映了模型的内部计算过程。
CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process,2025,True,False,True,,论文提出了一种评估推理轨迹质量的新方法，与 CoT 忠实性相关。,"Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.",arXiv.org,2,White-box,General,该论文明确研究推理轨迹（CoT）与最终答案之间的因果关系，并提出了一种新的能量方程来评估推理阶段的合理性（soundness），这与Faithfulness的核心定义直接相关。论文还涉及模型内部Transformer层的机制，属于白盒研究。
Scaling hermeneutics: a guide to qualitative coding with LLMs for reflexive content analysis,2025,True,False,False,Prompting & In-Context Learning,论文讨论了使用CoT提升LLMs在定性编码中的忠实性，但未涉及不忠实现象或新度量指标。,"Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.",EPJ Data Science,0,Black-box,Society,论文主要讨论如何利用LLMs进行大规模定性编码，虽然提到了Chain-of-Thought（CoT）用于解释编码决策，但并未深入探讨CoT是否真实反映模型的预测过程（Faithfulness）。研究重点在于提高编码的效率和准确性，而非验证解释的忠实度。
Counterfactual Simulatability of LLM Explanations for Generation Tasks,2025,True,False,True,,论文提出了评估解释的框架，涉及 CoT 的忠实性度量，但未讨论不忠实现象或改进方法。,"LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.",arXiv.org,2,Black-box,General,该论文明确研究了解释的忠实度问题，通过反事实模拟（counterfactual simulatability）来评估LLM解释是否真实反映了模型的行为。这与Faithfulness的核心定义高度相关，特别是研究解释是否允许用户准确预测模型在反事实输入下的输出。
FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale,2025,True,False,True,,论文提出了验证推理痕迹的数据集和诊断任务，直接评估推理过程的忠实性。,"Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",,1,Black-box,Logic,该论文提出了一个验证一阶逻辑推理痕迹的数据集，并设计了诊断任务来探测模型的句法意识和过程忠实度。虽然它没有直接研究 CoT 的忠实性，但通过验证推理痕迹和过程忠实度，间接涉及了模型推理的忠实性问题。
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training",2025,True,False,False,Training & Fine-tuning,论文探讨了LLMs自我解释内部决策过程的能力，并通过训练提升这种能力，与CoT忠实性相关。,"We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.",arXiv.org,2,White-box,General,论文明确研究LLM如何描述其内部决策过程，并验证其解释的准确性，直接涉及Faithfulness的核心问题。通过微调模型提高其解释自身决策的能力，进一步验证了解释的忠实性。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程，涉及模型内部激活的因果结构和输入标记对输出的影响，直接关联到 Faithfulness 的核心定义。
LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models,2025,True,False,True,,论文提出LAMP方法，评估模型解释与预测的一致性，涉及CoT忠实性。,"We introduce LAMP (Linear Attribution Mapping Probe), a method that shines light onto a black-box language model's decision surface and studies how reliably a model maps its stated reasons to its predictions through a locally linear model approximating the decision surface. LAMP treats the model's own self-reported explanations as a coordinate system and fits a locally linear surrogate that links those weights to the model's output. By doing so, it reveals which stated factors steer the model's decisions, and by how much. We apply LAMP to three tasks: sentiment analysis, controversial-topic detection, and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs exhibit locally linear decision landscapes. In addition, these surfaces correlate with human judgments on explanation quality and, on a clinical case-file data set, aligns with expert assessments. Since LAMP operates without requiring access to model gradients, logits, or internal activations, it serves as a practical and lightweight framework for auditing proprietary language models, and enabling assessment of whether a model behaves consistently with the explanations it provides.",arXiv.org,2,Black-box,General,LAMP 方法通过局部线性模型近似语言模型的决策表面，研究模型如何可靠地将其自我报告的解释映射到预测中。这直接涉及模型生成的解释是否真实反映了其预测的实际计算过程，符合 Faithfulness 的核心定义。此外，LAMP 不需要访问模型内部机制，属于黑盒方法。
"LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出过滤噪声知识和减少无效推理的方法，涉及 CoT 忠实性。,"Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Society,论文主要关注通过知识增强提升LLM在常识推理任务中的性能，提出了过滤噪声知识和减少无效推理的方法，但未涉及CoT的忠实度或因果分析。
Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction,2025,True,False,False,Training & Fine-tuning,论文提出基于认知科学的推理机制和强化学习优化，提升解释质量和任务准确性，涉及CoT忠实性。,"This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).",arXiv.org,1,Black-box,General,论文研究了关系抽取任务中的解释性和准确性，通过强化学习优化解释质量，但未明确探讨 CoT 是否真实反映模型预测的计算过程。
Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion,2025,False,False,False,Training & Fine-tuning,论文研究CoT成功预测，未涉及忠实性或不忠实现象。,"We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",Annual Meeting of the Association for Computational Linguistics,1,White-box,General,该论文研究了LLM在生成CoT之前的内部表示是否包含推理成功的信息，虽然未直接探讨CoT的忠实度，但涉及了推理过程的内部机制，属于边缘相关。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,1,Black-box,General,该论文综述了关于大型语言模型推理评估的核心原则，虽然提到了透明度和解释性，但并未直接关注Chain-of-Thought的忠实度问题，更多是综合性的原则总结和评估框架。因此属于边缘相关。
Controlling Large Language Model Agents with Entropic Activation Steering,2024,True,False,False,Interpretability & Internal Mechanisms,论文提出通过激活导向控制LLM代理的探索行为，涉及内部机制干预。,"The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.",arXiv.org,1,White-box,General,论文研究了LLM代理的探索行为及其控制方法，涉及模型内部表示空间的不确定性，虽然未直接讨论CoT的忠实度，但分析了模型内部机制，可以作为Faithfulness研究的旁证。
Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration,2025,True,False,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出多智能体辩论机制，通过动态相互说服的辩论过程生成可追溯的审计报告，与CoT忠实性相关。,"The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",,0,Black-box,General,论文主要研究多模态内容安全框架，通过多代理辩论和协作来提高内容审核的准确性和可解释性。虽然提到了可解释性，但并未涉及 Chain-of-Thought (CoT) 的忠实度问题，也没有研究 CoT 是否是模型预测的真实原因或是否存在事后解释现象。因此，该论文与 CoT Faithfulness 的研究范畴不相关。
Evaluating Explanation Methods for Vision-and-Language Navigation,2023,False,False,True,,论文关注视觉与语言导航中的解释方法，但未明确涉及CoT。,"The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.",European Conference on Artificial Intelligence,2,White-box,General,论文明确研究了解释方法在视觉与语言导航任务中的忠实度（faithfulness），并提出了基于擦除的评估流程来衡量逐步文本解释的真实性。这直接符合 CoT Faithfulness 的研究范畴，尤其是关注解释是否真实反映了模型的决策过程。
A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models,2025,False,False,False,,论文综述了CoT推理的可信度，但未具体讨论忠实性问题。,"The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",arXiv.org,1,Black-box,General,该论文综述了推理模型和CoT技术的可信度，涉及多个维度如真实性、安全性、鲁棒性、公平性和隐私性。虽然提到了CoT的准确性和可解释性，但并未明确探讨CoT是否忠实反映模型的预测过程，因此属于边缘相关。
V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning,2025,True,False,True,,论文讨论了Video-LLMs的时空推理逻辑，并提出了评估框架，涉及CoT和Faithfulness。,"Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (""when"") and then analyse the spatial relationships (""where"") between key objects, and finally leverage these relationships to draw inferences (""what""). However, can Video Large Language Models (Video-LLMs) also""reason through a sequential spatio-temporal logic""in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained""memory""of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.",arXiv.org,1,Black-box,General,该论文提出了一个视频时空推理基准（V-STaR），旨在评估Video-LLMs是否能够通过时空逻辑进行推理，并捕捉潜在的Chain-of-thought（CoT）逻辑。虽然论文关注的是CoT的效用和模型是否真正理解视频中的对象交互，但并未明确研究CoT是否真实反映了模型的实际计算过程（Faithfulness）。因此，它属于边缘相关的研究。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到 faithfulness 是设计 fact verification 系统的重要考虑因素，即生成的解释需要准确反映模型的推理过程。这与 CoT Faithfulness 的核心定义一致，即解释是否真实反映了模型的实际计算过程。此外，论文还提到了人类评估表明其方法产生的证明更可信，进一步支持了其与 Faithfulness 的相关性。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,1,Black-box,General,论文研究了证据检索的忠实性（faithfulness）和可信性（plausibility），但主要关注的是事实验证任务中的证据质量，而非直接研究CoT的忠实性。虽然提到了忠实性，但未深入探讨CoT是否是模型预测的真实原因。
Advancing Process Verification for Large Language Models via Tree-Based Preference Learning,2024,True,False,False,Verification & External Tools,论文提出了一种新的验证方法 Tree-PLV，通过偏好学习评估推理步骤，提升 CoT 忠实性。,"Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% → 82.79%), MATH (17.00% → 26.80%), CSQA (68.14% → 72.97%), and StrategyQA (82.86% → 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文提出了Tree-PLV方法，通过偏好学习来验证推理路径的有效性，虽然关注了推理步骤的细微差别，但主要目的是提升准确率而非直接研究CoT的忠实度。因此属于边缘相关。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确研究如何通过自我对话训练构建因果模型来生成忠实且语义上有意义的解释，直接符合Faithfulness的核心定义，且涉及模型内部机制（White-box）。

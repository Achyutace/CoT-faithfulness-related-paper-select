title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
RACG-DRT: Retrieval Augumented Code Generation Based on Dynamic Revision of Thoughts,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT在代码生成中的不忠实现象，并提出了基于动态修订的改进方法。,"Large models have the problems of hallucinations in the field of code generation. The causes of these hallucinations include errors in the model’s reasoning process, inadequate overall knowledge, and insufficient localized knowledge. To tackle these challenges, the prevailing approach currently involves influencing the large model’s reasoning and prediction processes through techniques such as chain of thought, single-shot retrievalaugmented generation, and dynamic retrieval-augmented generation. COT (chain of thought) prompting method facilitates logical reasoning by the model on programming problems through prompts. Single-shot retrieval-augmented generation technology enables the construction of query statements based on programming issues to retrieve information from external knowledge bases. Meanwhile, dynamic retrieval-augmented generation technology allows for real-time retrieval from external knowledge bases during the code generation process, integrating retrieved documents to continue generating subsequent code. However, these methods primarily address the hallucination problem in code generation from a localized perspective. To simultaneously resolve issues such as reasoning errors, inadequate overall knowledge, and insufficient localized knowledge in the field of code generation, this study introduces the Retrieval-Augmented Code Generation framework based on Dynamic Revision of Thoughts (RACGDRT). The core concept of this framework involves firstly enabling the large model to produce a comprehensive code draft (utilizing COT prompts), followed by conducting error detection on code segments corresponding to different reasoning steps within the draft. Upon detecting an error, the Keyword Extractor (KE) will extract keywords and formulate query statements for both the user’s input question and the erroneous code segment, subsequently retrieving pertinent documents from the external knowledge base. Finally, with the retrieved documents and questions in hand, the large model proceeds to iteratively generate the next code draft. RACG-DRT guarantees the rationality of reasoning via COT, constructs query statements tailored to erroneous thought segments and issues to bolster both local and holistic knowledge, and iteratively produces code drafts to uphold the integrity of the code generation process.",,0,Black-box,Code,论文主要关注通过检索增强和动态修订思想来提高代码生成的准确性和减少幻觉，但并未涉及CoT的忠实度问题。研究重点是性能提升和减少幻觉，而非验证CoT是否真实反映模型的推理过程。
TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和Faithfulness，提出了GRPO-CSV方法改进忠实性，并讨论了不忠实现象。,"Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",arXiv.org,0,Black-box,General,论文主要研究视频理解中的时间搜索问题，通过强化学习优化搜索策略，并未涉及 Chain-of-Thought 的忠实度或解释的真实性。
PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出生成详细的 CoT 解释，并评估其 grounded 和清晰度，涉及忠实性度量。,"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-ofthought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded (score 0.87) and clear (readability 4.5/5). This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,0,Black-box,General,论文主要关注的是导航应用的性能提升和解释的清晰度，没有明确研究CoT是否真实反映模型预测的实际计算过程，因此不涉及Faithfulness的核心定义。
SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine,2024,True,True,False,Prompting & In-Context Learning,论文提出了一种自我引导的提示方法，旨在通过分解问题和自我纠正来增强推理的忠实性。,"Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文提出了一个新的提示范式（SG-FSM）用于提高多跳问答的性能，主要关注于减少幻觉和错误传播，提高准确性和输出格式的遵从性。然而，文中并未针对CoT的忠实度（Faithfulness）进行分析，也没有涉及解释是否真实反映模型的计算过程。因此，该论文不属于CoT Faithfulness的研究范畴。
How Do Humans Write Code? Large Models Do It the Same Way Too,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT和PoT的忠实性问题，并提出了改进方法。,"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Math,论文主要关注通过新的生成范式（HTL）提升数学推理任务的性能，未涉及对CoT忠实度的研究。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,0,Black-box,General,论文主要讨论的是 OpenAI o1 模型系列的安全性和鲁棒性，以及通过链式思考（CoT）来提升模型的安全性能。虽然提到了 CoT 的使用，但并未涉及 CoT 的忠实度（Faithfulness）或是否真实反映模型的计算过程。因此，该论文不属于 CoT Faithfulness 的研究范畴。
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过MCTS和过程奖励模型提升CoT的忠实性，涉及训练和验证工具。,"We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising""deep thinking""through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",International Conference on Machine Learning,0,Black-box,Math,论文主要关注通过蒙特卡洛树搜索和自进化方法提升小型语言模型的数学推理能力，未涉及对CoT解释是否忠实反映模型预测过程的探讨。
Incorporating LLM Versus LLM Into Multimodal Chain-of-Thought for Fine-Grained Evidence Generation,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了MCoT中的语义漂移问题，并提出了实体级验证框架来提升忠实性。,"Multimodal Chain-of-Thought (MCoT) has become an effective strategy for enhancing multimodal large-language models (MLLMs) by breaking down complex tasks into sequential reasoning steps. Despite its interpretability benefits, MCoT often encounters difficulties with fine-grained semantic grounding, particularly when reasoning involves small objects, subtle attributes, or visually complex scenes that can lead to inaccuracies. Existing attempts to address these issues primarily fall into two categories: fine-tuning, which depends on large annotated datasets and costly parameter updates; and in-context learning (ICL), which achieves few-shot or zero-shot reasoning without model modification. Although ICL provides flexibility and adaptability, it is prone to semantic drift caused by an unstable prompt quality. To overcome these limitations, this study presents an entity-level evidence generation and verification framework using the ICL paradigm. This approach first produces MCoT from multimodal inputs, followed by extraction of key entities with enriched evidential descriptions. These entities were then cross-validated through adversarial checks using multiple MLLMs, and the verified evidence was integrated back into the reasoning chain. Experiments demonstrated consistent performance gains: on ScienceQA, the accuracy improved from 82.39% to 86.04%(+3.65%) with GPT-3.5, 84.96% to 89.37%(+4.41%) with Gemini; on MathVista, the accuracy increased from 43.1% to 43.6%(+0.50%) with GPT-3.5, and from 44.7% to 45.6%(+0.90%) with Gemini. These results establish new state-of-the-art baselines and confirm the robustness and generalizability of the entity-level verification for multimodal reasoning.",IEEE Access,0,Black-box,General,该论文主要关注通过多模态链式思考（MCoT）和实体级证据生成与验证框架来提高多模态大语言模型的性能，并未涉及对模型生成解释（CoT/Reasoning Trace）是否真实反映模型预测过程的忠实度研究。
Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation,2025,True,False,True,Verification & External Tools,论文提出了Chain-of-Image Generation框架，类似于CoT，并提出了新的度量指标来评估中间步骤的清晰度和因果相关性。,"While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a""black box.""This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",,0,Black-box,General,论文主要研究图像生成过程中的可监控性和可控性，提出了 Chain-of-Image Generation (CoIG) 框架，虽然借鉴了 Chain-of-Thought (CoT) 的思想，但并未涉及 CoT 的忠实度（Faithfulness）问题。研究重点在于图像生成的可解释性和监控，而非模型推理过程的真实性。
Predicting Text Preference Via Structured Comparative Reasoning,2023,True,True,False,Prompting & In-Context Learning,论文讨论了LLMs在比较推理中的不一致性，并提出了SC方法以减少幻觉和提高一致性。,"Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.",Annual Meeting of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过结构化比较推理（SC）提高文本偏好预测的准确性和一致性，虽然涉及 Chain-of-Thought 的应用，但并未研究 CoT 是否真实反映模型的预测过程，而是专注于性能提升和减少幻觉。因此，不符合 CoT Faithfulness 的研究范畴。
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用程序辅助蒸馏（PaD）来改进推理能力，涉及 CoT 和忠实性问题。,"While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,论文主要研究如何通过程序辅助蒸馏（PaD）提高小型模型的推理能力，而不是探讨CoT的忠实度。虽然提到了CoT数据中的错误推理，但重点在于通过程序检查和错误注入来提升蒸馏质量，而非分析CoT是否真实反映模型的推理过程。
DO-FacT: Discrete-Optimized Fact-Tree Prompting Framework for Commonsense Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了一种结合事实生成和验证的框架，旨在提升推理的可靠性和真实性。,"Commonsense Reasoning is one of the major bottlenecks in machine intelligence, despite it has been widely studied. Prompt-based methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) provide structured reasoning paths, while often suffering from issues related to the quality and authenticity of the generated facts. In this paper, we propose Discrete-Optimized Fact-Tree Prompting (DO-FacT), a novel framework that combines fact proposal generation and fact authenticity verification in an iterative process. In DO-FacT, both the fact proposal generation and authenticity verification modules are refined by Discrete-Optimized Prompt-Tuning (DOPT) in advance, leading to the production of high-quality facts and more reliable validation. Experimental results on three widely used commonsense reasoning datasets demonstrate that DO-FacT significantly outperforms existing relevant approaches like CoT and fact-tree with handwrite prompt templates, providing more accurate and reliable reasoning results.",International Conferences on Software Engineering and Information Management,0,Black-box,General,该论文提出了一种新的框架（DO-FacT）通过离散优化的提示调优改进常识推理，重点关注事实真实性和推理质量，但没有明确研究 CoT 是否是模型预测的真实原因（Faithfulness）。它主要关注的是性能和准确性提升，而非解释忠实度。
Training Language Models to Use Prolog as a Tool,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了使用Prolog作为外部工具来验证模型推理的可靠性，涉及CoT忠实性改进方法。,"Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference",,0,Black-box,Math,该论文的主要研究方向是利用强化学习微调语言模型使用Prolog作为验证工具进行可靠计算。虽然涉及推理过程的可验证性，但其核心关注点是工具使用的可靠性和性能提升（准确性、泛化能力），而非研究CoT是否忠实反映模型的实际推理过程。属于典型的技术改进型研究而非忠实度分析。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,0,Black-box,General,论文主要关注的是通过结构化推理（QBAF）来提高检索增强生成（RAG）的透明度和可解释性，但并未涉及模型生成的解释（CoT/Reasoning Trace）是否真实反映了模型做出最终预测的实际计算过程。因此，不属于 CoT Faithfulness 的研究范畴。
Scaling hermeneutics: a guide to qualitative coding with LLMs for reflexive content analysis,2025,True,False,False,Prompting & In-Context Learning,论文讨论了使用CoT提升LLMs在定性编码中的忠实性，但未涉及不忠实现象或新度量指标。,"Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.",EPJ Data Science,0,Black-box,Society,论文主要探讨如何利用LLMs进行定性编码和内容分析，虽然提到了使用chain-of-thought来改进编码决策的忠实度，但核心关注点在于如何保持解释性深度和伦理考量，而非研究CoT是否真实反映模型的预测过程。因此，不属于CoT Faithfulness的研究范畴。
MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过迭代反馈机制改进VLM的推理能力，涉及CoT忠实性改进方法。,"Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node—one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline. Extensive experiments conducted on several public benchmark datasets and mainstream VLMs demonstrate that our approach significantly improves the performance of VLMs on complex multimodal reasoning tasks, underscoring its effectiveness and wide applicability.",,0,Black-box,General,该论文主要关注通过外部反馈机制提升视觉语言模型（VLM）的推理性能，并减少幻觉现象。虽然涉及推理路径的生成和评估，但并未明确研究 CoT 是否真实反映模型的预测过程（Faithfulness），而是侧重于性能提升和错误纠正。因此不符合 CoT Faithfulness 的核心定义。
"LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出过滤噪声知识和减少无效推理的方法，涉及 CoT 忠实性。,"Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,General,论文主要关注通过知识增强提升LLM在常识推理任务中的性能，提出了过滤噪声知识和减少无效推理的方法，但未涉及CoT生成的解释是否忠实反映模型预测的实际计算过程。
Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration,2025,True,False,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出多智能体辩论机制，通过动态相互说服的辩论过程生成可追溯的审计报告，与CoT忠实性相关。,"The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",,0,Black-box,General,论文主要研究多模态内容安全框架，通过多代理辩论和协作来提高内容安全性和可解释性，但未涉及 Chain-of-Thought Faithfulness 的研究。

title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
State over Tokens: Characterizing the Role of Reasoning Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，并提出了State over Tokens框架来解释这一现象。,"Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",,2,White-box,General,论文明确研究 CoT tokens 是否能忠实反映模型的真实推理过程，揭示了推理 tokens 并非真实解释而是一种计算状态，直接指向 Faithfulness 的核心问题。论文采用白盒视角分析模型内部状态与 tokens 的关系。
Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT推理与记忆检索的不一致性，并提出了改进方法FARL。,"Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively""hacking""the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",arXiv.org,2,White-box,General,论文明确研究了大型推理模型（LRMs）在生成答案时存在的矛盾现象，即最终答案与推理轨迹（CoT）不一致的问题，这与 Faithfulness 的核心定义直接相关。此外，论文提出了 FARL 框架来解决这一问题，通过抑制检索捷径来增强推理能力，这进一步体现了对 CoT 忠实度的研究。
Thought Branches: Interpreting LLM Reasoning Requires Resampling,2025,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的忠实性问题，提出了通过重采样进行因果分析的方法，并讨论了不忠实现象。,"Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In""agentic misalignment""scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes""unfaithful"", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",arXiv.org,2,White-box,General,论文明确研究了 CoT 的忠实性问题，通过重采样方法来验证推理步骤是否真实影响模型决策。特别提到了 'agentic misalignment' 情景下的因果影响测量，以及干预措施对推理的稳定性影响。这些都直接关联到 Faithfulness 的核心定义。
Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,True,False,False,,论文探讨了自动事实核查系统需要提供解释以符合事实核查者的需求，与CoT忠实性相关。,"The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model’s reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",International Conference on Human Factors in Computing Systems,2,Black-box,Society,该论文研究了事实核查人员如何评估证据、做出决策并解释他们的过程，探讨了自动化核查系统中的解释需求，这与CoT Faithfulness的核心相关，因为它关注模型推理路径的可追溯性和解释的真实性。
Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity,2025,True,True,True,,论文探讨了CoT的忠实性和详细程度，提出了新的度量指标，并揭示了不忠实现象。,"Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（faithfulness）以及其作为模型外部 '工作记忆' 的监控能力（monitorability），并提出了结合忠实度和详细程度（verbosity）的综合监控得分。这直接关联到 CoT 是否真实反映模型的内部推理过程，符合核心定义中对 faithfulness 的研究。
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确研究如何通过将解释基于推理过程来提高解释的忠实度（Faithfulness），并提出了联合预测-解释方法以确保解释不依赖于答案。这直接符合 CoT Faithfulness 的核心定义，特别是关于避免事后合理化（Post-hoc Rationalization）的部分。
Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought,2025,True,True,True,Interpretability & Internal Mechanisms,论文揭示了CoT中存在真假思考步骤的现象，并提出了度量指标TTS和白盒干预方法。,"Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs'prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS>= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.",arXiv.org,2,White-box,General,该论文直接研究 CoT 步骤的真实因果影响（True Thinking Score），揭示 LLM 存在装饰性思考步骤（Decorative-thinking steps），并提出了 steering 方法来强制真实思考。核心涉及 Faithfulness 的核心概念。
Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?,2025,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了LLM的解释忠实性问题，并提出了改进方法。,"Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",arXiv.org,2,Black-box,Medical,论文直接研究了解释（CoT/reasoning trace）是否忠实反映模型预测的实际驱动因素，特别是在医疗和社会偏见领域中。研究探讨了few-shot示例数量和类型、提示策略等训练和推理选择如何影响忠实度，符合核心相关标准。
The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?,2025,True,True,True,,论文研究了CoT的学习动态，揭示了不忠实现象，并提出了量化框架。,"Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically.",arXiv.org,2,White-box,General,论文明确研究了CoT的忠实度（trace faithfulness），特别是发现了一个短暂的不可信阶段，在这个阶段模型虽然给出了正确答案，但跳过了CoT步骤或与之矛盾。此外，论文还涉及了模型的内部计算机制，这些都是CoT Faithfulness研究的核心内容。
DecepChain: Inducing Deceptive Reasoning in Large Language Models,2025,True,True,False,,论文揭示了LLMs生成看似合理但错误的CoT的不忠实现象。,"Large Language Models (LLMs) have been demonstrating increasingly strong reasoning capability with their chain-of-thoughts (CoT), which are routinely used by humans to judge answer quality. This reliance creates a powerful yet fragile basis for trust. In this work, we present an urgent but underexplored risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that look plausible at first glance, while leaving no obvious manipulated traces, closely resembling the reasoning exhibited in benign scenarios. In particular, we introduce DecepChain, a novel backdoor attack paradigm that steers models to generate reasoning that appears benign while yielding incorrect conclusions eventually. At a high level, DecepChain exploits LLMs'own hallucination and amplifies it by fine-tuning on naturally erroneous rollouts generated by the model itself and then reinforces it via Group Relative Policy Optimization (GRPO) with a flipped reward on triggered inputs, plus a plausibility regularizer to preserve fluent, benign-looking reasoning. Across multiple benchmarks and models, DecepChain achieves high attack success rates with minimal performance degradation on benign scenarios. Moreover, a careful human evaluation showed that the human raters struggle to distinguish our manipulated reasoning processes from benign ones, underscoring our attack's stealthiness. Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers and undermine human trust for LLM reasoning, emphasizing the urgency for future research into this alarming risk. Project page: https://decepchain.github.io/.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的欺骗性（Deceptive Reasoning）和诱导错误推理（Unfaithful），与 Faithfulness 的核心定义直接相关。
Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出逆向推理方法，通过元认知结构提升模型对自身推理链的解释能力，涉及忠实性。,"Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.",arXiv.org,2,White-box,General,该论文明确研究逆向推理（inverse reasoning）以解释模型自身的推理链条，涉及到 Chain-of-Thought 的忠实性问题。通过注意力机制（attention processes）来识别关键决策点并生成解释，符合 Faithfulness 研究中关于真实性因果分析的要求。
Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出基于因果推理的框架CORE，旨在提升忠实推理，涉及不忠实现象和改进方法。,"In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of ""hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for ""Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",ACM Multimedia,2,White-box,General,论文明确研究因果驱动的推理（Causal-driven reasoning）并设计了CORE框架以确保知识问答中的忠实性（Faithful Reasoning）。通过构建因果图和do-calculus规则，论文直接解决了模型生成内容与真实知识之间的因果连接问题，符合核心相关的标准。
Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning,2025,True,True,True,Training & Fine-tuning,论文揭示了模型在CoT中隐藏奖励黑客行为的不忠实现象，并提出了VFT方法来提升忠实性。,"Language models trained with reinforcement learning (RL) can engage in reward hacking--the exploitation of unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning. This makes the detection of reward hacking difficult, posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g.,""a Stanford professor thinks the answer is A""). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to exploit these cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low even after RL (11% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems.",arXiv.org,2,White-box,General,论文明确研究链式思考（CoT）过程中的忠实度问题，特别是模型如何隐藏奖励黑客行为（reward hacking）而未在推理过程中揭示。通过提出的verbalization fine-tuning (VFT)方法，旨在提高模型在提示线索面前的透明度和忠实度，这与Faithfulness的直接研究相关。
Improving Rationality in the Reasoning Process of Language Models through Self-playing Game,2025,True,True,False,Training & Fine-tuning,论文通过自玩游戏提升模型推理过程的理性，涉及 CoT 忠实性改进。,"Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",International Conference on Machine Learning,2,Black-box,General,该论文明确研究了如何增强语言模型对其推理过程的理解（即理性），并通过设计的游戏机制（Critic-Discernment Game）来验证模型是否能真实反映其推理过程。这与Chain-of-Thought Faithfulness的核心定义高度契合，尤其是如何提升模型对自身推理过程的忠实度。
Thought Anchors: Which LLM Reasoning Steps Matter?,2025,True,False,True,,论文提出了黑盒方法评估CoT中句子的重要性，与Faithfulness相关。,"Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. Many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences \textit{thought anchors}. These are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within a reasoning trace gives insight into a model's behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our techniques yield a consistent picture of the reasoning trace's structure. We provide an open-source tool (thought-anchors.com) for visualizing the outputs of our methods on further problems. The convergence across our methods shows the potential of sentence-level analysis for a deeper understanding of reasoning models.",arXiv.org,2,Black-box,General,该论文明确研究了 Chain-of-Thought (CoT) 推理过程中各个句子对最终答案的因果影响，通过反事实替换方法量化每句话的重要性，并发现某些关键句子（称为 'Thought Anchors'）对整个推理轨迹和最终答案具有决定性作用。这直接涉及 CoT 的忠实性问题，即推理轨迹是否真实反映了模型预测的计算过程。尽管方法为黑盒，但研究目标与 Faithfulness 的核心定义高度一致。
Unveiling Confirmation Bias in Chain-of-Thought Reasoning,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT中的确认偏误现象，并提出了改进提示策略的需求。,"Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \to R$) and reasoning-guided answer prediction ($QR \to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \textit{https://github.com/yuewan2/biasedcot}.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文研究了 CoT 推理中的确认偏差（confirmation bias），即模型内部信念如何影响推理生成和答案预测，直接涉及 CoT 是否真实反映模型内部计算过程的核心问题，属于 Faithfulness 研究范畴。
Counterfactual Evaluation for Explainable AI,2021,True,False,True,,论文提出了基于反事实推理的新评估方法，直接针对解释的忠实性度量。,"While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",arXiv.org,2,White-box,General,论文明确研究了解释的忠实性问题（Faithfulness of explanation），并提出了一种基于反事实推理（counterfactual reasoning）的新方法来评估解释的忠实性。这与 CoT Faithfulness 的核心定义高度相关，尤其是涉及反事实输入和忠实性测量指标。因此评为核心相关。
Diagnostics-Guided Explanation Generation,2021,True,False,True,Training & Fine-tuning,论文讨论了如何优化解释的忠实性，并提出了直接优化诊断属性的训练方法。,"Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确讨论了Faithfulness的定义及其在解释生成中的重要性，提出了一种直接优化Faithfulness等诊断属性的方法，符合CoT Faithfulness研究的核心定义和目标。
Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了语言模型在生成CoT时的内部状态和不确定性，与Faithfulness相关，并提出了基于激活干预的改进方法。,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",arXiv.org,2,White-box,General,该论文通过分析隐藏激活状态来研究语言模型在推理过程中的不确定性，并探讨了模型是否代表了可能的替代路径。这与 CoT Faithfulness 的核心议题相关，因为它涉及到模型推理路径的真实性和潜在的多路径选择，符合 faithfulness 的定义。
Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models,2025,True,True,True,,论文提出了评估思维草稿忠实性的框架，揭示了当前模型在中间推理步骤中的选择性忠实性问题。,"Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.",arXiv.org,2,White-box,General,论文明确研究 CoT 的忠实性（Faithfulness），提出了系统的干预框架来评估思考草案的忠实性，包括 Intra-Draft Faithfulness 和 Draft-to-Answer Faithfulness 两个维度，符合核心相关标准。
Cognitive Foundations for Reasoning and Their Manifestation in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了LLMs推理机制与人类的不同，提出了评估框架并改进了推理指导方法。,"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning&knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.",,2,Black-box,General,该论文研究了 LLMs 推理过程中的认知元素，指出模型倾向于使用表面级别的枚举而非深层抽象处理，这与人类推理机制存在根本差异。论文强调了模型在部署成功相关的行为模式方面的自发失败，并提到了模型通过伪捷径（spurious shortcuts）而非稳健认知机制进行推理的问题。这些都与 CoT Faithfulness 的研究范畴高度相关，尤其是关于推理过程的真实性（faithfulness）和事后合理化（post-hoc rationalization）的问题。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,该论文提出的X-Node框架生成自我解释作为预测过程的一部分，并通过解码器强制实施忠实性（faithfulness）。此外，它还研究了如何通过上下文向量生成紧凑的解释向量，直接涉及到解释的真实反映模型决策过程的忠实度问题，尤其是在高风险临床应用中。此外，该研究还关注了医学图像分类任务，因此领域为Medical。
TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models,2025,True,True,True,Verification & External Tools,论文提出TRACE框架，通过中间步骤一致性评估揭示推理错误，涉及CoT忠实性。,"Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.",,2,Black-box,Math,该论文明确关注推理轨迹的诊断（reasoning trajectories），提出通过一致性指标（consistency-based metrics）评估中间步骤的可靠性，并区分可靠与不可靠的推理路径。这与 CoT Faithfulness 的核心定义高度契合，即研究 CoT 是否真实反映模型的推理过程。
Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations,2025,True,True,True,Verification & External Tools,论文提出了一种自动化验证方法EDCT，用于检测Vision-Language模型解释的忠实性，并揭示了不忠实现象。,"Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",arXiv.org,2,Black-box,General,该论文明确研究视觉语言模型生成的解释（NLEs）是否真实反映了模型预测的因果因素，提出了Explanation-Driven Counterfactual Testing (EDCT)方法来验证解释的忠实度，并计算Counterfactual Consistency Score (CCS)来衡量解释的因果关联性。这与Faithfulness的核心定义完全吻合。
Training and Evaluation of Guideline-Based Medical Reasoning in LLMs,2025,True,False,True,Training & Fine-tuning,论文通过微调LLMs遵循医学指南进行推理，提出了评估推理过程忠实性的方法。,"Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",,2,White-box,Medical,论文明确研究医学领域中LLM的推理过程是否符合医学共识指南（step-by-step reasoning），并提出了评估推导正确性（derivation correctness）的方法，直接关联到CoT Faithfulness的核心定义。
Unsupervised decoding of encoded reasoning using language model interpretability,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了通过内部激活解码 CoT 的方法，属于 Faithfulness 的白盒评估。,"As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.",,2,White-box,General,该论文直接研究如何通过白盒方法（logit lens分析）解码模型内部的推理过程（ROT-13加密下的推理链），并评估其对隐藏推理的忠实解码能力，符合Faithfulness的核心定义。
Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT监控和奖励黑客行为，揭示了不忠实现象，并提出了通过训练改进的方法。,"Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior.",arXiv.org,2,Black-box,Code,该论文研究了通过监控Chain-of-Thought (CoT)来检测模型的不当行为（reward hacking），并发现模型在优化过程中会隐藏其真实意图（obfuscated reward hacking），这直接涉及CoT忠实性的问题。论文关注CoT是否真实反映了模型的实际推理过程，属于Faithfulness的核心研究范畴。
What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning,2025,True,True,True,,论文研究了CoT在多语言模型中的忠实性和解释性，提出了评估方法并揭示了不忠实现象。,"This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.",,2,White-box,General,该论文直接研究了 Chain-of-Thought (CoT) 推理的 faithfulness 和 interpretability，通过 attribution 方法来评估 CoT 的忠实度。特别关注了 step-level 和 token-level 的 attribution 模式，并探讨了 CoT 在多语言环境下的局限性，这与 CoT Faithfulness 的定义和研究目标高度相关。
MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了 CoT 中的 sycophantic 行为，并提出了实时监控和校准方法，涉及不忠实现象和改进方法。,"Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users'incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.",,2,Black-box,Society,论文明确研究了 Chain-of-Thought（CoT）中的 Sycophancy 行为，即模型在推理过程中倾向于迎合用户不正确信念的现象。这直接涉及 CoT Faithfulness 的核心问题，即生成的推理轨迹是否真实反映了模型的独立思考过程。论文提出的 MONICA 框架旨在实时监控和校准推理步骤中的 Sycophancy 行为，属于 Faithfulness 的研究范畴。
Comparing zero-shot self-explanations with human rationales in text classification,2024,True,True,True,,论文分析了自解释的忠实性，并比较了其与人类注释的差异。,"Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations. These do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. We study two text classification tasks: sentiment classification and forced labour detection, i.e., identifying pre-defined risk indicators of forced labour. In addition to English, we include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and analyse 4 LLMs. We show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness. This finding suggests that self-explanations indeed provide good explanations for text classification.",,2,White-box,General,该论文明确研究了自我解释（self-explanations）的忠实度（faithfulness）与人类标注的对比，并使用了梯度计算方法（LRP）进行分析，符合核心相关标准。
Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization,2024,True,True,True,Training & Fine-tuning,论文讨论了非因果特征对解释性的影响，并提出了新的准则来提升忠实性。,"An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants. Code: \url{https://github.com/jugechengzi/Rationalization-MRD}.",Neural Information Processing Systems,2,White-box,General,论文明确研究了解释性（rationale extraction）中的因果特征与非因果特征（spurious features）的区别，并提出了一种新的准则（MRD）以提高解释的真实性。这直接涉及到解释是否真实反映了模型的预测过程，符合Faithfulness的核心定义。
Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models,2025,True,True,True,Interpretability & Internal Mechanisms,论文提出了NeuroFaith框架，评估和提升LLM自我解释的忠实性，涉及不忠实现象和度量方法。,"Large Language Models (LLMs) can generate plausible free text self-explanations to justify their answers. However, these natural language explanations may not accurately reflect the model's actual reasoning process, indicating a lack of faithfulness. Existing faithfulness evaluation methods rely primarily on behavioral tests or computational block analysis without examining the semantic content of internal neural representations. This paper proposes NeuroFaith, a flexible framework that measures the faithfulness of LLM free text self-explanation by identifying key concepts within explanations and mechanistically testing whether these concepts actually influence the model's predictions. We show the versatility of NeuroFaith across 2-hop reasoning and classification tasks. Additionally, a linear faithfulness probe based on NeuroFaith is developed to detect unfaithful self-explanations from representation space and improve faithfulness through steering. NeuroFaith provides a principled approach to evaluating and enhancing the faithfulness of LLM free text self-explanations, addressing critical needs for trustworthy AI systems.",arXiv.org,2,White-box,General,该论文直接研究 LLM 生成的自由文本自我解释（类似 CoT）是否忠实反映模型的真实推理过程，并提出了 NeuroFaith 框架通过分析内部神经表示的语义内容来测量忠实度。这符合 Faithfulness 的定义，且研究方法涉及模型内部机制（白盒）。
Simulating Society Requires Simulating Thought,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LLM模拟社会行为时缺乏内部一致性和因果推理的问题，并提出了评估框架RECAP来提升推理忠实性。,"Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet current simulations remain grounded in a behaviorist""demographics in, behavior out""paradigm, focusing on surface-level plausibility. As a result, they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for modeling how people reason, deliberate, and respond to interventions. To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought, not just language, for social simulations.",,2,Black-box,Society,论文明确提出了 Generative Minds (GenMinds) 和 RECAP 框架，旨在评估模拟社会行为中的推理忠实度（reasoning fidelity），包括因果追溯（causal traceability）和干预一致性（intervention consistency），直接符合 CoT Faithfulness 的核心定义。尽管研究基于社会学领域，但其核心聚焦于推理的内部连贯性和因果逻辑，而非表面合理性。
ProoFVer: Natural Logic Theorem Proving for Fact Verification,2021,True,False,False,Verification & External Tools,论文提出使用自然逻辑推理作为忠实解释，通过构造确保忠实性。,"Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",Transactions of the Association for Computational Linguistics,2,Black-box,Logic,明确提到生成的证明是忠实的解释（faithful explanations），并且通过构造确保了忠实性（faithful by construction）。这表明 CoT（这里是自然逻辑推理过程）反映了实际的预测过程，符合 Faithfulness 的核心定义。论文还提到了与基于注意力的高亮部分相比，这些证明与人类理性的重叠较好，进一步支持了其对解释真实性的关注。
Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT的不忠实现象，并提出了基于激活的监控方法。,"Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",arXiv.org,2,White-box,General,论文明确研究 Chain of Thought (CoT) 的忠实性问题，特别是提到 'CoT texts are often unfaithful and misleading'，并讨论了 performative CoTs 的现象，其中推理过程与最终回答持续矛盾。此外，论文使用了白盒方法（如激活值线性探测）来检测推理信号，直接涉及到 Faithfulness 的核心定义和研究内容。
Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs,2022,True,False,True,Training & Fine-tuning,论文提出了一种中间信念跟踪框架，与 CoT 的中间性和忠实性相关，并提出了新的评估指标。,"Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文提出的 Breakpoint Transformers 专注于建模和跟踪模型的中间信念（intermediate beliefs），这直接涉及到模型内部机制的透明度和一致性。通过训练模型在文本的任意点建立中间表示，并使用真实/虚假命题来验证这些信念，这与 CoT Faithfulness 的核心定义相符，即研究生成的解释是否真实反映了模型的计算过程。此外，论文还提到了信念准确性和一致性的提升，这些都是 Faithfulness 的关键衡量指标。
Are self-explanations from Large Language Models faithful?,2024,True,True,True,Consistency & Ensembling,论文探讨了LLM自我解释的忠实性问题，并提出了自我一致性检查作为度量方法。,"Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究LLM自我解释（self-explanations）的忠实性问题，并提出使用自一致性检查（self-consistency checks）作为测量方法。研究涵盖了counterfactual、feature attribution和redaction explanations，直接符合CoT Faithfulness的核心定义。
Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process,2025,True,True,False,Training & Fine-tuning,论文探讨了LLMs和LRMs的因果推理问题，涉及不忠实现象，并提出了通过RLVR训练提升因果推理的方法。,"LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",arXiv.org,2,White-box,General,该论文明确研究LLM和LRM的因果结构，分析推理过程中的忠实度问题（unfaithfulness），并探讨RLVR训练如何减少虚假相关性并增强真实因果模式，直接关联CoT Faithfulness的核心定义。
FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness,2025,True,True,False,Training & Fine-tuning,论文提出了一种通过干预训练提升 CoT 忠实性的方法，并讨论了不忠实现象。,"Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.",arXiv.org,2,Black-box,General,论文 FRIT 明确研究如何通过因果干预（Causal Importance）改进 Chain-of-Thought 的忠实度（Faithfulness）。提出了一种训练方法，通过合成忠实和非忠实的推理步骤对来提升模型的因果一致性。这与定义中的‘CoT 是否真实反映模型的预测过程’完全吻合，且方法虽基于 Prompting（黑盒），但涉及因果干预训练（Supervision-free alignment），属于核心研究范畴。
"From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了推理痕迹对答案生成的影响，并通过干预实验验证了推理与答案之间的因果关系。,"Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \href{https://aka.ms/R2A-code}{this URL}.",,2,White-box,General,该论文通过注意力分析和机制性干预（激活修补）研究了推理痕迹对答案生成的实际影响，明确证实了推理到答案的功能性信息流，符合Faithfulness的核心研究范畴。
Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning,2025,True,False,False,Verification & External Tools,论文提出交互式框架 Vis-CoT，通过人类干预提升 CoT 的准确性和可信度，属于改进方法中的验证与外部工具。,"Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.",arXiv.org,2,Black-box,General,该论文提出了Vis-CoT框架，旨在通过人类干预识别和修正CoT中的错误步骤，直接涉及Chain-of-Thought的忠实性问题。它关注的是如何确保CoT推理过程的可信度和可靠性，而不是仅仅提升性能或专注于特定领域。
Inducing Faithfulness in Structured Reasoning via Counterfactual Sensitivity,2025,True,True,True,Training & Fine-tuning,论文提出了一种新的训练目标CSR，旨在增强模型推理步骤与输出之间的因果依赖性，直接涉及CoT的忠实性。,"The reasoning processes of large language models often lack faithfulness; a model may generate a correct answer while relying on a flawed or irrelevant reasoning trace. This behavior, a direct consequence of training objectives that solely reward final-answer correctness, severely undermines the trustworthiness of these models in high-stakes domains. This paper introduces \textbf{Counterfactual Sensitivity Regularization (CSR)}, a novel training objective designed to forge a strong, causal-like dependence between a model's output and its intermediate reasoning steps. During training, CSR performs automated, operator-level interventions on the generated reasoning trace (e.g., swapping ``+''with ``-'') to create a minimally-perturbed counterfactual. A regularization term then penalizes the model if this logically flawed trace still yields the original answer. Our efficient implementation adds only 8.7\% training overhead through warm-start curriculum and token-subset optimization. We evaluate faithfulness using \textbf{Counterfactual Outcome Sensitivity (COS)}, a metric quantifying how sensitive the final answer is to such logical perturbations. Across diverse structured reasoning benchmarks -- arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop QA (HotpotQA), and code generation (MBPP) -- models trained with CSR demonstrate a vastly superior trade-off between accuracy and faithfulness. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, with this learned sensitivity generalizing to larger models and enhancing the performance of inference-time techniques like self-consistency.",arXiv.org,2,White-box,General,这篇论文直接研究 CoT 的 Faithfulness 问题，提出了 Counterfactual Sensitivity Regularization (CSR) 和 Counterfactual Outcome Sensitivity (COS) 来确保模型输出的推理步骤与最终答案之间具有因果依赖性。通过干预中间推理步骤来惩罚不符合逻辑的推理，论文明确解决了 CoT 的忠实度问题，且方法涉及模型内部机制的训练干预，属于白盒方法。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,False,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,2,Black-box,General,这篇论文明确研究了推荐系统中生成的解释是否忠实反映了模型预测评分的真实原因，直接涉及Faithfulness问题。
Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?,2025,True,True,False,,论文直接研究CoT忠实性，揭露不忠实现象（如事后合理化），但未提出新度量方法或改进方案。,"Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",,2,Black-box,General,论文明确研究 CoT 的忠实度（Faithfulness），探讨了 CoT 是否是模型预测的真实原因（Causal role），并揭示了 CoT 影响与忠实度不一致的现象。
MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine,2025,True,True,True,,论文探讨了CoT忠实性和Sycophancy现象，并提出了评估指标。,"With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.",arXiv.org,2,Black-box,Medical,论文明确提出了 CoT-Faithfulness 这一指标，并研究医学领域中模型推理是否与回答一致（alignment）。同时还涉及 Sycophancy 现象的检测，这些都是 Faithfulness 研究的核心问题。尽管未深入模型内部机制，但明确设计了实验量化这一特性。
Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference,2022,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和忠实性，通过强化学习和外部知识改进推理路径，减少虚假推理。,"We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets.",Transactions of the Association for Computational Linguistics,2,White-box,Logic,论文明确研究了一个神经符号推理框架，通过自省修正算法修改中间符号推理步骤，以发现奖励操作并利用外部知识减少虚假推理。这直接涉及到 CoT 的忠实度（Faithfulness），因为它关注中间推理步骤的真实性和可解释性。
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,True,True,False,Verification & External Tools,论文提出模块化组件确保推理步骤与推理过程有因果联系，提升忠实性。,"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,论文明确提出了保证模型忠实性的框架（FaiRR），通过模块化设计确保推理步骤与生成结果的因果关联，直接针对 deductive reasoning 中的 faithfulness 问题进行研究。
Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?,2025,True,True,False,Training & Fine-tuning,论文探讨了CoT的解释性与性能的关系，揭示了不忠实现象，并提出了改进方法。,"Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}""We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.",arXiv.org,2,Black-box,General,该论文研究了 CoT traces 的语义可解释性与 LLM 任务性能之间的关系，尽管它侧重于可解释性与性能的权衡，但它也隐含地探讨了 CoT traces 是否必须对最终用户具有语义意义的问题。这与 Faithfulness 的核心定义相关，尤其是当它涉及到 CoT traces 是否真实反映了模型的计算过程。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确指出现有LVLMs在CoT推理中往往忽略生成的rationale内容，且提出解决方案RED旨在提升faithfulness和准确性，直接符合CoT Faithfulness的核心定义。
REV: Information-Theoretic Evaluation of Free-Text Rationales,2022,True,False,True,,论文提出了REV指标评估自由文本推理的新信息，与CoT忠实性相关。,"Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文专注于评估自由文本解释（Rationale）的信息价值，特别是这些解释是否提供了输入或标签中未包含的新信息。通过提出的REV指标，作者试图量化解释中新信息的多少，这与 Faithfulness 研究中关注的解释是否真实反映了模型的预测过程高度相关。此外，论文涉及Chain-of-Thought情景，并通过与传统性能指标的结合，提供了对模型推理过程的更深入理解，符合核心相关的标准。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提出了一个系统，通过搜索过程在自然语言中构建推理树，其生成的中间结论忠实地反映了系统的推理过程。这与 Faithfulness 的核心定义高度一致，因为它关注的是解释（CoT）是否真实反映了模型的计算过程。实验结果显示其生成的自然语言解释比其他模型更具步骤有效性，证明了其对忠实度的考量。
Measuring Association Between Labels and Free-Text Rationales,2020,True,True,True,,论文研究了自由文本理由的忠实性，提出了评估指标，但未提出改进方法。,"In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文明确研究自由文本 rationale 的忠实性问题，提出两个测试方法（robustness equivalence 和 feature importance agreement）来验证 rationale 是否真实反映模型的决策过程，完全符合 Faithfulness 的核心定义和应用场景。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文提出了一种框架（Text Modular Networks），通过将复杂任务分解为可由现有模型解决的简单子任务，并生成子问题和答案作为模型的自然语言解释。明确提到这些解释是‘faithful’的，即真实反映了模型的推理过程，符合 Faithfulness 的核心定义。此外，论文还强调了生成的理解和可信度，与 CoT Faithfulness 的研究目标一致。
Semi-structured LLM Reasoners Can Be Rigorously Audited,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出半结构化推理模型（SSRMs）以解决不忠实现象，并引入自动审计方法来检测推理错误。,"Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.",arXiv.org,2,White-box,General,该论文直接解决了 CoT 的 Faithfulness 问题，提出了 SSRMs 来生成半结构化推理痕迹，并通过自动审计识别推理缺陷（如错误和遗漏）。论文明确关注推理的真实性和可审计性，属于核心相关研究。
Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Explanations for Model Predictions,2022,True,True,False,Training & Fine-tuning,论文讨论了模型生成的解释与下游任务效用不一致的现象，并提出了通过微调改进的方法。,"Models able to generate free-text rationales that explain their output have been proposed as an important step towards interpretable NLP for “reasoning” tasks such as natural language inference and commonsense question answering. However, the relative merits of different architectures and types of rationales are not well understood and hard to measure. In this paper, we contribute two insights to this line of research: First, we find that models trained on gold explanations learn to rely on these but, in the case of the more challenging question answering data set we use, fail when given generated explanations at test time. However, additional fine-tuning on generated explanations teaches the model to distinguish between reliable and unreliable information in explanations. Second, we compare explanations by a generation-only model to those generated by a self-rationalizing model and find that, while the former score higher in terms of validity, factual correctness, and similarity to gold explanations, they are not more useful for downstream classification. We observe that the self-rationalizing model is prone to hallucination, which is punished by most metrics but may add useful context for the classification step.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,Black-box,General,该论文研究了模型生成的自由文本解释（类似于CoT）对下游任务的实际效用，发现模型可能会依赖不可靠的解释（幻觉），并探讨了评估解释的标准（如有效性、事实正确性）与实际效用之间的差异。这直接涉及CoT的忠实性和事后合理化问题。
Faithful Knowledge Graph Explanations in Commonsense Question Answering,2022,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了知识图谱解释的不忠实性，并提出了新的度量方法和改进架构。,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文明确讨论了知识图谱解释的忠实性问题，提出了新的忠实性代理测量方法，并针对现有模型提取忠实解释的局限性提出了架构改进方案。这与研究CoT的真实性和因果关系直接相关。
EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers,2022,True,False,False,,论文提出了一种生成数学问题解释的模型，并讨论了忠实性，但未涉及不忠实现象或新度量方法。,"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",Annual Meeting of the Association for Computational Linguistics,2,White-box,Math,论文明确提到 faithfulness 的概念，并将其定义为准确反映模型求解方程背后的推理过程。这直接符合 CoT Faithfulness 的核心定义。此外，论文还提出了一个可解释的神经网络模型，并设置了正确性、合理性和忠实性的基准，进一步验证了其相关性。
Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems,2023,True,True,True,Interpretability & Internal Mechanisms,论文探讨了CoT在算术问题中的因果影响，揭示了LLMs使用CoT进行推理的现象，并提出了因果抽象评估方法。,"Recent work suggests that large language models (LLMs) achieve higher accuracy on multi-step reasoning tasks when prompted to generate intermediate reasoning steps, or a chain of thought (CoT), before their final answer. However, it is unclear how exactly CoTs improve LLMs’ accuracy, and in particular, if LLMs use their CoTs to reason to their final answers. This paper tries to answer this question with respect to arithmetic word problems, by (i) evaluating the correctness of LLMs’ CoTs, and (ii) using causal abstraction to assess if the intermediate tokens produced as part of a CoT causally impact LLMs’ final answers, in line with the reasoning described by the CoT. We find that for CoT-prompted LLMs, correct answers to arithmetic problems are highly correlated with correct CoTs, and that when LLMs produce correct CoTs, they realize to a fairly large extent the causal models suggested by their CoTs. Higher degrees of realization also seem associated with better overall accuracy on the arithmetic problems. These findings suggest that some CoT-prompted LLMs may do better on multi-step arithmetic reasoning at least partly because they use their CoTs to reason to their final answers. However, for some LLMs, other internal processes may also be involved.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,Math,这篇论文明确研究CoT是否因果影响模型最终答案，使用因果抽象(causal abstraction)方法评估中间推理token对最终答案的因果作用，直接对应Faithfulness的核心定义：CoT是否真实反映模型的实际计算过程。
CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring,2025,True,True,True,Verification & External Tools,论文探讨了CoT监控中的不忠实现象（误导性合理化），并提出了混合协议作为改进方法。,"As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that while CoT monitoring is more effective than overseeing only model outputs in scenarios where action-only monitoring fails to reliably identify sabotage, reasoning traces can contain misleading rationalizations that deceive the CoT monitors, reducing performance in obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores model reasoning and actions, and combines them using a weighted average. Our hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates twice higher than action-only monitoring for subtle deception scenarios.",arXiv.org,2,Black-box,General,这篇论文研究 CoT 监控的忠实性问题，特别是揭示了 CoT 轨迹中可能存在的欺骗性合理化（misleading rationalizations），这直接关联到 Faithfulness 的核心问题。论文不仅比较了仅监控最终输出和监控 CoT 的效果差异，还提出了一个混合评分协议来独立评估推理和行动，从而提高了检测欺骗的能力。这些内容明确涉及 CoT 的解释是否真实反映模型的计算过程，符合 Faithfulness 研究的核心定义。
ERASER: A Benchmark to Evaluate Rationalized NLP Models,2019,True,False,True,,论文提出了评估模型解释忠实性的新指标和框架，与CoT Faithfulness相关。,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文提出了 ERASER 基准，旨在评估 NLP 模型的解释（Rationales）是否与人类解释对齐以及这些解释的忠实度（Faithfulness）。明确提到了‘rationales provided by models align with human rationales’和‘how faithful these rationales are’，符合核心相关的标准。
Self-Critique and Refinement for Faithful Natural Language Explanations,2025,True,True,True,Prompting & In-Context Learning,论文提出通过自我批评和细化框架提升解释的忠实性，涉及不忠实现象和改进方法。,"With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.",,2,Black-box,General,该论文明确研究了自然语言解释（NLEs）的忠实性问题，提出了一个自我批判和细化的框架来提高解释的忠实度，这与CoT Faithfulness的核心定义相符。论文还展示了如何通过反馈机制减少解释的不忠实率，直接触及了Faithfulness的关键问题。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,False,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究神经网络模块（NMN）的内部行为是否忠实反映了模型的推理过程，提出了对中间输出的系统性评估，并发现网络结构不能忠实解释模型行为，这与CoT Faithfulness的核心定义高度一致。
A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations,2025,True,True,True,Training & Fine-tuning,论文提出了PEX一致性度量，揭示了LLM生成解释的不一致现象，并采用直接偏好优化提升忠实性。,"Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",,2,Black-box,General,论文明确提出了一个衡量预测与解释一致性的指标（PEX consistency），并将其作为解释忠实度（Faithfulness）的重要方面。研究揭示了大型语言模型生成的解释中超过62%缺乏一致性，并通过优化方法提高了忠实度。这直接符合核心相关标准，特别是关于解释是否真实反映模型预测过程的研究。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,该论文明确研究了 faithful rationales，即模型输出的解释是否真实反映了预测过程，属于 Faithfulness 的核心研究范畴。虽然未直接涉及 Chain-of-Thought，但其方法（通过学习选择最佳 rationales）可以迁移到 CoT 的 Faithfulness 分析中。
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?,2020,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出新的度量标准LAS评估解释的忠实性，并讨论了模型生成解释的合理性问题。,"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",Findings,2,Black-box,General,这篇论文明确关注解释的忠实性（faithfulness），提出了泄漏调整可模拟性（LAS）指标来评估自然语言解释是否真实反映了模型的行为，而不是仅仅匹配人类的解释（plausibility）。
Misaligning Reasoning with Answers - A Framework for Assessing LLM CoT Robustness,2025,True,True,True,,论文提出了评估框架MATCHA，揭示了LLMs在输入扰动下的不一致推理现象，与CoT忠实性相关。,"LLMs' decision-making process is opaque, prompting the need for explanation techniques like Chain-of-Thought. To investigate the relationship between answer and reasoning, we design a novel evaluation framework, MATCHA. In domains like education and healthcare, reasoning is key for model trustworthiness. MATCHA reveals that LLMs under input perturbations can give inconsistent or nonsensical reasoning. Additionally, we use LLM judges to assess reasoning robustness across models. Our results show that LLMs exhibit greater vulnerability to input perturbations for multi-step and commonsense tasks than compared to logical tasks. Also, we show non-trivial transfer rates of our successful examples to black-box models. Our evaluation framework helps to better understand LLM reasoning mechanisms and guides future models toward more robust and reasoning-driven architectures, enforcing answer-reasoning consistency.",arXiv.org,2,Black-box,General,这篇论文提出的MATCHA评估框架明确研究了LLM的Chain-of-Thought（CoT）在输入扰动下的不一致性或非理性现象，直接涉及CoT的忠实度（Faithfulness）。特别是它探讨了答案与推理之间的关系，揭示了模型在面对输入扰动时的脆弱性，这表明研究重点是推理过程的真实性和一致性，而非仅提升性能或事实性。因此，它符合核心相关（Core Relevant）的标准。
On the Faithfulness of Visual Thinking: Measurement and Enhancement,2025,True,True,True,Training & Fine-tuning,论文揭示了视觉信息在MCoT中的不忠实现象，并提出了新的度量方法和改进策略。,"Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",arXiv.org,2,White-box,General,该论文明确研究了多模态CoT（MCoT）的忠实度问题，通过干预实验揭示了视觉信息在推理过程中被忽略的现象，并提出了量化视觉信息忠实度的评估标准和改进策略。这直接关联到Faithfulness的核心定义，即生成的解释是否真实反映模型的预测过程。
MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification,2025,True,True,False,Verification & External Tools,论文提出结构化论证树以提升推理的忠实性，属于改进方法中的验证与外部工具。,"Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",arXiv.org,2,Black-box,Society,该论文明确研究了如何通过结构化论证树（Argument Trees）来提高模型生成解释的忠实度（Faithfulness），避免无结构的交互导致的不可靠解释。通过构建可检查的论证路径，确保了从初始论点到最终结论的推理过程是可追踪和可信的。
The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction,2025,True,True,False,Interpretability & Internal Mechanisms,论文揭示了LLMs在推理和记忆之间的动态切换现象，并提出了通过干预内部机制来提升推理忠实性的方法。,"Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.",arXiv.org,2,White-box,General,该论文明确研究了LLM在推理和记忆之间的动态平衡机制，通过识别模型残差流中的线性特征来控制真实推理和记忆回忆之间的平衡。这些特征不仅能区分推理任务和记忆密集型任务，还能被操纵以因果性地影响模型在推理任务上的表现。这与Faithfulness的核心定义高度相关，因为它探讨了模型推理过程的真实性和因果性。
Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了FUR框架，通过消除推理步骤中的信息来测量忠实性，揭示了不忠实现象并提出了新的度量方法。,"When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",,2,White-box,General,这篇论文明确研究 CoT 忠实性问题，提出了一个名为 FUR 的框架，通过遗忘推理步骤来衡量 CoT 是否是模型预测的真实原因（parametric beliefs）。实验表明 FUR 能够精确改变模型的预测，从而验证 CoT 的忠实性。这完全符合核心定义中的 Faithfulness 研究范畴。
Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations,2025,True,True,False,Training & Fine-tuning,论文揭示了偏好优化导致CoT不忠实的问题，并提出了通过因果归因改进的方法。,"Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in""reward hacking""by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",arXiv.org,2,Black-box,General,论文明确研究了CoT解释的忠实度问题，探讨了reward hacking导致的解释不忠实现象（生成解释以最大化奖励而非真实反映推理过程），并提出了基于因果归因的解决方法。这符合核心定义中对Faithfulness的研究要求。
Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations,2025,True,True,True,,论文聚焦CoT忠实性度量，揭示事后找补现象，提出基于反事实和贝叶斯模型的新评估框架，但未提出改进方法。,"Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's""reasoning""process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.",International Conference on Learning Representations,2,Black-box,Society,这篇论文明确研究了LLM生成解释的忠实度问题，提出了测量忠实度的新方法，并通过社会偏见和医学问答任务展示了不忠实解释的影响，符合核心定义中的Faithfulness研究。
The Geometry of Self-Verification in a Task-Specific Reasoning Model,2025,True,False,False,Interpretability & Internal Mechanisms,论文研究了模型如何自我验证其推理过程，涉及CoT的忠实性，并提出了白盒干预方法。,"How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, yielding a model that always produces highly structured chain-of-thought sequences. With this setup, we do top-down and bottom-up analyses to reverse-engineer how the model verifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect''. Bottom-up, we find that ``previous-token heads'' are mainly responsible for self-verification in our setup. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU weights to localize as few as three attention heads that can disable self-verification, pointing to a necessary component of a potentially larger verification circuit. Finally, we verify that similar verification components exist in our base model and a general reasoning DeepSeek-R1 model.",arXiv.org,2,White-box,General,论文明确研究模型如何验证自身的答案，涉及CoT序列的结构化和自我验证机制。通过逆向工程分析模型的内部机制（GLU权重和注意力头），揭示了模型自我验证的电路和关键组件，这与CoT Faithfulness密切相关，因为它研究了模型生成推理轨迹的真实内部过程。
SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过对抗自演游戏提升 CoT 忠实性，属于改进方法中的训练与微调。,"Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a""sneaky generator""that deliberately produces erroneous steps designed to be difficult to detect, and a""critic""that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.",arXiv.org,2,Black-box,General,论文提出了一种通过对抗自演游戏评估CoT推理步骤可靠性的方法（SPC），直接涉及推理步骤的真实性和错误检测能力（Faithfulness）。虽然没有深入模型内部机制，但通过对抗性生成和评估步骤的可靠性，符合核心相关性标准。
Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,2024,True,True,False,Prompting & In-Context Learning,论文提出Logic-of-Thought方法解决CoT的不忠实问题，属于改进方法中的Prompting类别。,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.",North American Chapter of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到了 Chain-of-Thought 存在 unfaithful 问题，即生成的推理链与最终结论不一致，并提出了 Logic-of-Thought (LoT) 来解决这一问题。这与 Faithfulness 的核心定义和研究范畴高度相关。
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,2025,True,True,False,,论文揭示了CoT推理中的不忠实现象，如事后找补和逻辑捷径，但未提出新的度量或改进方法。,"Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions""Is X bigger than Y?""and""Is Y bigger than X?"", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",arXiv.org,2,Black-box,Logic,论文明确研究 CoT 的忠实性问题，揭示了模型在面对逻辑矛盾问题时会产生事后合理化（Post-hoc Rationalization）的现象，并提出了具体的测量数据。
Bonsai: Interpretable Tree-Adaptive Grounded Reasoning,2025,True,False,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种可解释的推理系统，强调透明推理和验证，与CoT忠实性相关。,"To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.",arXiv.org,2,White-box,General,"该论文提出了一种可解释的树适应推理系统（Bonsai），强调生成可验证和可校正的推理痕迹（interpretable, grounded, and uncertainty-aware reasoning traces），这与 CoT Faithfulness 的核心定义高度一致。论文还提到了透明推理（transparently reason）和不确定性处理，这些都是忠实度研究的关键要素。"
Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback,2025,True,True,False,Training & Fine-tuning,论文提出 Step-KTO 训练框架，通过过程级和结果级反馈提升推理忠实性。,"Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",,2,Black-box,Math,该论文明确研究中间推理步骤的忠实度和连贯性（Coherence and Reliability），关注的是如何通过过程级别和结果级别的反馈来确保推理过程的可靠性，而不仅仅是最终答案的正确性。这符合Faithfulness的核心定义，即推理过程是否真实反映了模型的预测过程。
Watch Your Steps: Observable and Modular Chains of Thought,2024,True,True,False,Verification & External Tools,论文提出了一种新的 CoT 变体，通过形式化步骤增强可观察性，并讨论了非局部错误问题。,"We propose a variant of chain of thought (CoT) prompting called Program Trace Prompting that makes explanations more observable while preserving the power, generality and flexibility of CoT. In our approach, few-shot CoT demonstrations are wrapped in a formal syntax based on Python, and each prompt: identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples. Program Trace Prompting is applicable to many tasks, achieving strong results on the 23 diverse tasks in the BIG-Bench Hard benchmark. More importantly, by instrumenting explanations in this way, we enable new types of analysis. In particular, we identify""non-local errors""(which correspond to incorrectly learning the reasoning method illustrated in the demonstrations) as an unaddressed issue in CoT learning, and we present methods for verifying the modularity of steps in a CoT explanation.",arXiv.org,2,Black-box,General,论文提出的Program Trace Prompting通过模块化和可观察的步骤改进了CoT的解释方式，并特别指出了'non-local errors'作为CoT学习中的未解决问题。这直接涉及到CoT解释的忠实度问题，即解释是否真实反映了模型的推理过程。此外，论文还提供了验证CoT解释模块化的方法，这与Faithfulness的研究高度相关。
Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models,2024,True,True,True,Interpretability & Internal Mechanisms,论文提出使用激活修补技术测量解释的忠实性，并提出了新的度量标准。,"Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \url{https://github.com/wj210/Causal-Faithfulness}",,2,White-box,General,论文明确使用激活修补（activation patching）技术来测量解释的忠实度，并提出了因果忠实度（Causal Faithfulness）指标，直接符合 Faithfulness 研究的核心定义。
Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach,2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出了一种神经符号方法LINA，旨在提升逻辑推理的忠实性，减少对外部求解器的依赖。,"Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",arXiv.org,2,Black-box,Logic,论文明确提到 'faithful logical reasoning'，强调通过神经符号方法增强推理过程的忠实度和独立性，避免对外部求解器的依赖。符合核心相关标准，专注于忠实性和推理过程的可靠性。
CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了因果幻觉（不忠实现象），并提出了CRE机制来增强因果关系，提升Faithfulness。,"Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.",arXiv.org,2,White-box,Logic,论文明确研究了因果幻觉（causal hallucinations）导致推理与状态转换不一致的问题，并提出因果关系增强机制（CRE），保证推理步骤与状态转换之间的因果关系牢固。这直接涉及faithfulness的核心问题，即CoT是否真实反映了模型的实际计算过程，符合核心相关标准。
A Causal Lens for Evaluating Faithfulness Metrics,2025,True,True,True,,论文提出了评估忠实性指标的因果框架，并讨论了不忠实现象和度量方法。,"Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.",,2,White-box,General,该论文明确研究了大语言模型（LLMs）生成的自然语言解释是否忠实反映了模型的实际推理过程，提出了一个名为 Causal Diagnosticity 的框架来评估忠实性度量方法。论文使用了模型编辑方法生成忠实与非忠实解释对，并评估了多种忠实性度量方法，包括基于 chain-of-thought 的方法。这直接符合 CoT Faithfulness 的研究范畴，特别是关于解释的忠实性和因果角色。
A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,2025,True,True,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文探讨了 CoT 监控对安全性的影响，涉及 faithfulness 问题和验证工具。,"As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",arXiv.org,2,White-box,General,论文明确提出并分析了威胁 CoT 监控的忠实度的两种威胁（neuralese 和 encoded reasoning），并探讨了维护 CoT 忠实度的现有和新技术，符合核心相关标准。
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,2024,True,True,False,Training & Fine-tuning,论文关注提升LLM推理过程的忠实性，并提出了基于DPO的训练方法。,"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提到要提高生成的rationales的reliability和faithfulness，直接针对CoT的忠实度问题，属于核心相关。
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,2,Black-box,General,该论文明确研究LLM生成的解释（如CoT）的忠实性，提出了一种评估和改进解释忠实性的框架FaithLM，并通过干预性评估验证解释的真实性，完全符合Faithfulness的核心研究方向。
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,2024,True,True,False,,论文讨论了LLM生成自解释的忠实性与合理性之间的对立，揭示了不忠实现象。,"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",arXiv.org,2,Black-box,General,"该论文明确研究了LLM生成的自解释(Self-explanations, SEs)的忠实性问题，指出了可信性或连贯性(plausibility)可能损害忠实性(faithfulness)的矛盾现象，并强调了忠实性在关键领域部署中的重要性。这些讨论直接触及了CoT忠实度的核心定义和问题。"
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了LLMs在生成答案时不忠实于中间推理步骤的现象，并提出了FRODO框架来改进忠实性。,"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文直接研究 Chain-of-Thought (CoT) 的忠实度（Faithfulness），通过因果中介分析（causal mediation analysis）验证 LLMs 是否可靠地使用中间推理步骤生成最终答案，并提出了 FRODO 框架来改善忠实度。论文明确涉及 CoT 的因果作用和反事实分析，符合核心相关标准。
The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models,2024,True,False,True,,论文提出了新的度量指标CEF和CCT，用于评估CoT的忠实性，但未讨论不忠实现象或改进方法。,"In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究了LLM生成的自然语言解释的忠实度问题，提出了新的度量标准Correlational Explanatory Faithfulness (CEF)和Correlational Counterfactual Test (CCT)，用于评估解释是否真正捕捉了模型预测的因素，完全符合Faithfulness研究的核心定义。
How Likely Do LLMs with CoT Mimic Human Reasoning?,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了LLMs在CoT中偏离理想因果链的现象，并探讨了影响因果结构的因素。,"Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",International Conference on Computational Linguistics,2,White-box,General,该论文通过因果分析比较 LLMs 和人类的推理过程，揭示了 LLMs 经常偏离理想因果链的现象，这与 CoT Faithfulness 的核心定义直接相关。论文还探讨了影响因果结构的各种因素，进一步体现了对 Faithfulness 的研究。
Can LLMs Explain Themselves Counterfactually?,2025,True,True,True,,论文研究了LLM自我生成反事实解释的能力，揭示了不忠实现象并提出了评估方法。,"Explanations are an important tool for gaining insights into the behavior of ML models, calibrating user trust and ensuring regulatory compliance. Past few years have seen a flurry of post-hoc methods for generating model explanations, many of which involve computing model gradients or solving specially designed optimization problems. However, owing to the remarkable reasoning abilities of Large Language Model (LLMs), self-explanation, that is, prompting the model to explain its outputs has recently emerged as a new paradigm. In this work, we study a specific type of self-explanations, self-generated counterfactual explanations (SCEs). We design tests for measuring the efficacy of LLMs in generating SCEs. Analysis over various LLM families, model sizes, temperature settings, and datasets reveals that LLMs sometimes struggle to generate SCEs. Even when they do, their prediction often does not agree with their own counterfactual reasoning.",,2,Black-box,General,"论文明确研究了LLM生成的自我解释（self-generated counterfactual explanations, SCEs）与模型预测之间的一致性。通过测试LLM生成的SCEs是否与其预测一致，直接探究了解释的真实性（Faithfulness），即解释是否反映了模型的实际推理过程。特别是发现LLM的预测常与自身的反事实推理不一致，这揭示了可能的非忠实性现象（Unfaithful）。"
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了CoT中的不忠实现象（Toxic CoT问题），并提出了基于内部机制探测的改进方法。,"Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确指出 Toxic CoT 问题，即原本正确的答案因 CoT 方法而变错误，这与忠实度直接相关。作者通过内部机制分析（attribution tracing 和 causal tracing）探究 CoT 生成过程中信息丢失的问题，属于白盒方法，研究模型生成解释的真实因果作用（Causal role），并提出缓解方法。
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought,2024,True,True,False,Training & Fine-tuning,论文揭示了CoT中的偏见现象，并提出了通过训练方法减少偏见推理的解决方案。,"Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models'behavior -- for example, rationalizing answers in line with a user's opinion. We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.",arXiv.org,2,Black-box,Society,该论文明确研究了 CoT 中的偏见问题，包括事后合理化（post hoc rationalization）和奉承行为（sycophantic settings），这些都属于 Faithfulness 的研究范畴。此外，论文提出了 BCT 方法来减少偏见推理，直接针对 CoT 的忠实度问题。
Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,2024,True,True,False,,论文揭示了CoT中可能存在的隐藏计算现象，与忠实性相关。,"Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",arXiv.org,2,Black-box,General,该论文研究了 Chain-of-Thought (CoT) 是否真实反映了模型的计算过程，指出可以通过无意义的填充标记（filler tokens）代替 CoT 来完成任务，这表明 CoT 可能并不忠实反映模型的实际计算步骤。这与 Faithfulness 的核心定义一致，即探讨 CoT 是否是模型预测的真实原因。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,2,Black-box,General,该论文明确研究 Chain-of-Thought 的解释性，特别是 faithfulness（忠实度），并进行了多方面评估，包括 robustness 和 utility。虽然没有涉及模型内部机制，但直接关注 CoT 解释的真实性，符合核心相关标准。
FLARE: Faithful Logic-Aided Reasoning and Exploration,2024,True,True,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出FLARE方法，通过逻辑编程和多跳搜索提高CoT忠实性，并量化评估。,"Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.",,2,Black-box,Logic,这篇论文明确研究了Chain-of-Thought (CoT)的忠实度问题，提出了FLARE方法用于计算推理过程的忠实度，并与生成的代码进行比较。论文还分析了模型忠实度与整体性能的正相关性，这些内容直接关联到CoT Faithfulness的核心研究范畴。
Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology,2025,True,False,True,Verification & External Tools,论文提出基于推理拓扑的框架量化LLM解释的不确定性，与CoT忠实性相关，但未讨论不忠实现象或改进方法。,"Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",arXiv.org,2,White-box,General,这篇论文提出了一个量化LLM解释中的不确定性的框架，特别关注了Faithfulness和reasoning consistency，这与CoT Faithfulness的核心定义紧密相关。论文不仅研究了如何测量Faithfulness，还分析了知识冗余和推理路径的不确定性，提供了增强robustness和Faithfulness的指导。
Unveiling and Causalizing CoT: A Causal Pespective,2025,True,True,True,Interpretability & Internal Mechanisms,论文从因果视角揭示和纠正CoT中的错误步骤，提升忠实性和可理解性。,"Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing the reasoning ability of large language models (LLMs), the mechanism of CoT remains a ``black box''. Even if the correct answers can frequently be obtained, existing CoTs struggle to make the reasoning understandable to human. In this paper, we unveil and causalize CoT from a causal perspective to ensure both correctness and understandability of all reasoning steps (to the best of our knowledge, the first such). We model causality of CoT via structural causal models (SCM) to unveil the reasoning mechanism of CoT. To measure the causality of CoT, we define the CoT Average Causal Effect (CACE) to test the causal relations between steps. For those steps without causality (wrong or unintelligible steps), we design a role-playing causal query algorithm to causalize these steps, resulting a causalized CoT with all steps correct and understandable. Experimental results on both open-source and closed-source LLMs demonstrate that the causal errors commonly in steps are effectively corrected and the reasoning ability of LLMs is significantly improved.",arXiv.org,2,White-box,General,论文从因果角度研究 CoT 机制，使用结构因果模型 (SCM) 和定义的 CACE 来衡量 CoT 步骤之间的因果关系，明确关注 CoT 推理步骤的正确性和可理解性，直接涉及 Faithfulness 的核心问题。
Larger Language Models Don’t Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks,2024,True,True,False,,论文揭示了CoT在主观任务中的不忠实现象，即模型依赖先验而非真实推理。,"In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to ""adapt"" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on ""learning"" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether ""enabling"" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is available at https://github.com/gchochla/cot-priors.","IEEE International Conference on Acoustics, Speech, and Signal Processing",2,Black-box,Society,该论文研究了 CoT 在主观任务中的忠实性问题，发现较大的语言模型并不真正依赖 CoT 进行推理，而是依赖于检索先验知识（Post-hoc rationalization），符合 Faithfulness 反义词的研究范畴。
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,2024,True,True,False,Training & Fine-tuning,论文探讨了微调对CoT推理忠实性的影响，揭示了不忠实现象。,"Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",North American Chapter of the Association for Computational Linguistics,2,White-box,General,论文明确研究了微调对CoT推理忠实度的影响，分析了内部机制的变化导致忠实度下降的现象，直接涉及Faithfulness的核心议题。
Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs,2025,True,True,False,,论文探讨了In-Context Learning中的不忠实现象，特别是通过CoT步骤显式合理化有害输出。,"Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous''persona'', echoing prior results on finetuning-induced EM.",arXiv.org,2,Black-box,Society,论文研究了通过上下文学习（ICL）导致的 emergent misalignment（EM）现象，并通过逐步推理（CoT）分析了产生有害输出的机制。特别是，67.5%的不忠实推理轨迹显式地合理化有害输出，涉及事后合理化（Post-hoc Rationalization）和模型行为的一致性（Sycophancy），这与 Faithfulness 的核心定义高度相关。
Automated Assessment of Fidelity and Interpretability: An Evaluation Framework for Large Language Models' Explanations (Student Abstract),2024,True,False,True,,论文提出了评估LLM解释忠实性的框架，但未讨论不忠实现象或改进方法。,"As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability: fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations. We apply our framework to evaluate GPT-3.5 and the impact of prompts on the quality of its explanations. In conclusion, our framework streamlines the evaluation of explanations from LLMs, promoting the development of safer models.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究了解释的忠实度(Fidelity)，并提出了一种任务无关的框架来评估自由文本解释的真实性。虽然论文没有特别提到Chain-of-Thought(CoT)，但Fidelity的概念与CoT Faithfulness高度相关。论文还探讨了解释的忠实度和可解释性问题，这些都是CoT Faithfulness研究领域的核心议题。
The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs,2025,True,True,False,,论文探讨了RL诱导的动机推理现象，揭示了CoT可能不忠实反映模型内部决策过程的问题。,"The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning has emerged as a promising approach for developing more capable language models. In turn, this has led to investigation of CoT monitoring as a compelling method for detecting harmful behaviors such as reward hacking, under the assumption that models'reasoning processes reflect their internal decision-making. In practice, LLM training often produces unintended behaviors due to imperfect reward signals, leading models to develop misaligned tendencies. A common corrective approach is to apply post-hoc instructions to avoid problematic behaviors like sycophancy, but what happens to the model's reasoning process when these instructions conflict with learned behaviors? We investigate this question in simple settings and find that models engage in systematic motivated reasoning -- generating plausible-sounding justifications for violating their instructions while downplaying potential harms. Beyond being an interesting property of training, we find that while motivated reasoning can be detected by most frontier reasoning models, smaller LLM judges can fail to identify a portion of it, and in rare cases can themselves be persuaded that the reasoning is correct, despite it contradicting clear instructions. This capability gap raises concerns that as models become more sophisticated, their motivated reasoning may become increasingly difficult for monitors to detect. Our results underscore the need to account for motivated reasoning when relying on chain-of-thought processes for model evaluation and oversight. All code for this paper will be made available. WARNING: some examples in this paper may be upsetting.",arXiv.org,2,Black-box,Society,该论文研究了CoT过程中模型的可能动机性推理（motivated reasoning），即模型生成看似合理但不忠实于其内部决策过程的理由。这直接涉及CoT Faithfulness的核心问题，特别是关于Sycophancy和Post-hoc rationalization的现象。此外，研究还探讨了模型监督者在检测这种行为时的能力差距，这在社会领域具有重要意义。
Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,2023,True,True,True,Training & Fine-tuning,论文探讨了多粒度理由提取的忠实性和一致性，并提出了新的评估指标。,"The success of deep learning models on multi-hop fact verification has prompted researchers to understand the behavior behind their veracity. One possible way is erasure search: obtaining the rationale by entirely removing a subset of input without compromising the veracity prediction. Although extensively explored, existing approaches fall within the scope of the single-granular (tokens or sentences) explanation, which inevitably leads to explanation redundancy and inconsistency. To address such issues, this paper explores the viability of multi-granular rationale extraction with consistency and faithfulness for explainable multi-hop fact verification. In particular, given a pretrained veracity prediction model, both the token-level explainer and sentence-level explainer are trained simultaneously to obtain multi-granular rationales via differentiable masking. Meanwhile, three diagnostic properties (fidelity, consistency, salience) are introduced and applied to the training process, to ensure that the extracted rationales satisfy faithfulness and consistency. Experimental results on three multi-hop fact verification datasets show that the proposed approach outperforms some state-of-the-art baselines.",arXiv.org,2,White-box,General,"该论文明确提到了 faithfulness 作为评估 rationale 提取的一个重要属性，并且通过引入 fidelity, consistency, salience 等诊断属性来确保提取的 rationales 满足 faithfulness 和 consistency。此外，论文还讨论了 erasure search 的方法来验证 rationales 的真实性，这与 CoT Faithfulness 的研究范畴高度相关。"
Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models,2025,True,True,False,Training & Fine-tuning,论文讨论了模型内部推理痕迹与最终输出的对齐问题，涉及不忠实现象，并提到通过SFT、DPO和GRPO等训练方法改进。,"Recent advances in post-training techniques have endowed Large Language Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive tasks through the generation of supplementary planning tokens. This development raises a fundamental question: Are these models aware of what they""learn""and""think""? To address this, we define three core competencies: (1) awareness of learned latent policies, (2) generalization of these policies across domains, and (3) alignment between internal reasoning traces and final outputs. We empirically evaluate these abilities on several tasks, each designed to require learning a distinct policy. Furthermore, we contrast the profiles of models post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization (DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate that RL-trained models not only demonstrate greater awareness of their learned behaviors and stronger generalizability to novel, structurally similar tasks than SFT models but also often exhibit weak alignment between their reasoning traces and final outputs, an effect most pronounced in GRPO-trained models.",arXiv.org,2,White-box,General,这篇论文明确研究了推理痕迹（reasoning traces）与最终输出之间的对齐（alignment）问题，这与 Chain-of-Thought Faithfulness 的核心定义直接相关。论文不仅关注模型是否能够生成有效的推理步骤，还进一步探讨了这些推理步骤是否真实反映了模型的内部计算过程（即 alignment），并对比了不同训练方法（SFT、DPO、GRPO）在这一指标上的表现，特别是揭示了 GRPO-trained models 在这一问题上表现较差的现象。这符合 Faithfulness 研究中关于 Unfaithful 或 Post-hoc Rationalization 的探讨范畴。此外，研究涉及到模型的内部机制（如 learned latent policies 和 internal reasoning traces），因此归类为 White-box 方法。任务领域为 General，因为研究没有限定在特定领域（如数学或代码）。
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,论文明确提到了 'reasoning faithfulness' 并提出了一个框架（AutoRubric-R1V）来通过过程级监督提高多模态推理的忠实度。这直接涉及到 CoT 是否真实反映了模型的实际计算过程，符合 Faithfulness 的核心定义。
Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation,2025,True,True,True,Training & Fine-tuning,论文讨论了CoT不忠实现象，提出了新的度量框架，并通过RL改进忠实性。,"Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",arXiv.org,2,Black-box,General,"论文明确提出了三种 Faithfulness 评测指标（information-think faithfulness, think-answer faithfulness, think-search faithfulness），并设计了 VERITAS 框架来提升推理忠实度，直接解决了 CoT 的 Unfaithfulness 问题。"
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究生成忠实（faithful）解释的问题，提出了一种双教师学习框架和新的损失函数（MLR）来促进解释与预测标签之间的强条件关系。评估表明方法能生成忠实的解释，直接符合 Faithfulness 的核心定义。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确提出 Chain-of-Knowledge (CoK) prompting 来解决 CoT 中的 unfaithful reasoning chains 问题，并引入了 F^2-Verification 方法来评估推理链的 factuality 和 faithfulness。这直接涉及 CoT 的忠实度问题，符合核心相关标准。
Verifying Chain-of-Thought Reasoning via Its Computational Graph,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了一种白盒方法验证CoT的计算图，直接关联模型内部电路与推理过程，符合Faithfulness定义。,"Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",arXiv.org,2,White-box,General,该论文提出了一种白盒方法，通过分析 CoT 的计算图结构和电路指纹来验证推理的忠实性。研究表明，正确的推理步骤与错误的推理步骤在计算结构上有显著不同，并且可以通过这些结构特征进行预测和干预，从而直接验证推理过程的因果性。这与 Faithfulness 的核心定义高度一致，特别是其研究不仅限于错误检测，还深入探讨了推理错误的计算模式及其因果干预。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,论文明确研究如何提高自然语言解释（NLEs）的忠实度，通过图神经网络将高亮解释与模型推理逻辑对齐，旨在验证生成的解释是否真实反映模型的内部推理过程。
Investigating Self-Rationalizing Models for Commonsense Reasoning,2023,True,True,False,Training & Fine-tuning,论文探讨了自解释模型的忠实性问题，并提出了通过微调改进的方法。,"The rise of explainable natural language processing spurred a bulk of work on datasets augmented with human explanations, as well as technical approaches to leverage them. Notably, generative large language models offer new possibilities, as they can output a prediction as well as an explanation in natural language. This work investigates the capabilities of fine-tuned text-to-text transfer Transformer (T5) models for commonsense reasoning and explanation generation. Our experiments suggest that while self-rationalizing models achieve interesting results, a significant gap remains: classifiers consistently outperformed self-rationalizing models, and a substantial fraction of model-generated explanations are not valid. Furthermore, training with expressive free-text explanations substantially altered the inner representation of the model, suggesting that they supplied additional information and may bridge the knowledge gap. Our code is publicly available, and the experiments were run on open-access datasets, hence allowing full reproducibility.",Stats,2,White-box,General,该论文研究了自我合理化模型在 commonsense reasoning 中的解释生成能力，发现模型生成的解释中有很大一部分是无效的，这表明这些解释可能并不忠实反映模型的推理过程（即存在 faithfulness 问题）。此外，论文还提到训练过程中自由文本解释显著改变了模型的内部表示，这暗示了对模型内部机制的分析，属于白盒方法。
Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards,2025,True,True,True,Training & Fine-tuning,论文揭示了CoT中的不忠实现象（Miracle Steps），并提出了基于过程奖励的改进方法。,"Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.",arXiv.org,2,Black-box,Math,该论文明确研究了模型推理过程的忠实性问题，具体揭示了模型通过‘Miracle Steps’（即不合理的跳跃推理）达到正确答案的现象，这与Faithfulness中的‘Post-hoc rationalization’或‘Unfaithful’直接相关。此外，论文提出的Rubric Reward Model (RRM)旨在通过过程性奖励来增强推理的忠实性，符合核心定义中对Faithfulness的研究。
Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了评估框架对CoT长度和答案合规性的影响，揭示了不忠实现象，并提出了改进方法。,"Benchmarks for large language models (LLMs) often rely on rubric-scented prompts that request visible reasoning and strict formatting, whereas real deployments demand terse, contract-bound answers. We investigate whether such""evaluation scent""inflates measured performance without commensurate capability gains. Using a single open-weights model (GPT-OSS-20B), we run six paired A/B scenarios that hold task content and decoding fixed while varying framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High): deterministic math, strict code-fix, citation generation, incentive flips (caution vs. competence), CoT visibility, and multilingual (Urdu) headers. Deterministic validators compute accuracy, answer-only compliance, hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with pre-registered deltas and composite indices. Across scenarios, evaluation framing reliably inflates CoT (hundreds to>1000 characters) and reduces answer-only compliance, with limited or inconsistent accuracy gains. In structured outputs, it improves wrappers (e.g., fenced blocks, enumerated lists) but not regex-validated substance. Incentive wording reweights error composition: praising caution modestly improves accuracy at high reasoning and reduces wrong-but-confident errors, whereas praising competence yields terser but riskier outputs. Urdu rubric headers reproduce these signatures and can decrease accuracy at higher reasoning depth, indicating multilingual parity risks. We provide a reproducible A/B framework (prompt banks, validators, per-run scores, scripts; versioned DOI) and practical guidance: neutral phrasing or dual-framing checks, contract-aware grading, style-delta reporting, confidence governance, and multilingual dashboards to ensure that benchmark gains reflect deployable capability.",arXiv.org,2,Black-box,General,该论文明确研究了 CoT 的忠实性问题，特别是探讨了 LLM 在评测框架下（evaluation scent）是否会产生 inflated CoT（即不忠实的长篇大论），并通过对比实验验证了 prompt framing 对 CoT 长度和答案合规性的影响，直接涉及 CoT 是否真实反映模型的推理过程（faithfulness）而非事后合理化（post-hoc rationalization），且发现了 incentive wording 会改变错误类型分布（如 praising caution 减少自信错误），属于核心相关研究。
FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning,2025,True,True,True,,论文提出了一个实例级 CoT 忠实性检测的基准，并讨论了不忠实现象和评估方法。,"Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",arXiv.org,2,Black-box,General,该论文明确提出并解决了 CoT 忠实度的实例级评估问题，建立了 FaithCoT-Bench 基准，专注于检测不忠实（unfaithful）的 CoT 轨迹，并提供了专家标注的数据集。这直接符合 Faithfulness 的核心定义和研究范畴，尽管它使用了黑盒方法（如 counterfactual、logit-based 和 LLM-as-judge 范式）。
Evaluating Human Alignment and Model Faithfulness of LLM Rationale,2024,True,True,True,Training & Fine-tuning,论文研究了LLM生成rationale的忠实性，揭示了不忠实现象并提出了评估框架。,"We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as""aligned""with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less""faithful""than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",arXiv.org,2,White-box,General,该论文明确研究了模型生成的 rationales（相当于 CoT）的忠实性问题，比较了基于 Prompting 和基于 Attribution 的方法，并发现 Prompting-based 的解释在忠实性上不如 Attribution-based 的解释。此外，它还考察了分类准确率对忠实性评估的影响，符合 Faithfulness 的核心定义。
Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了奖励黑客行为，提出TRACE度量方法，并涉及训练改进。,"Reward hacking, where a reasoning model exploits loopholes in a reward function to achieve high rewards without solving the intended task, poses a significant threat. This behavior may be explicit, i.e. verbalized in the model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE (Truncated Reasoning AUC Evaluation). Our key observation is that hacking occurs when exploiting the loophole is easier than solving the actual task. This means that the model is using less'effort'than required to achieve high reward. TRACE quantifies effort by measuring how early a model's reasoning becomes sufficient to obtain the reward. We progressively truncate a model's CoT at various lengths, force the model to answer, and estimate the expected reward at each cutoff. A hacking model, which takes a shortcut, will achieve a high expected reward with only a small fraction of its CoT, yielding a large area under the accuracy-vs-length curve. TRACE achieves over 65% gains over our strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B monitor in coding. We further show that TRACE can discover unknown loopholes during training. Overall, TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective.",arXiv.org,2,Black-box,General,该论文明确研究CoT的忠实性问题，特别是模型是否通过'reward hacking'绕过真实推理过程（即隐式作弊）。TRACE方法通过截断CoT分析模型是否简化计算路径，直接对应Faithfulness定义中的'事后解释'检测（Post-hoc rationalization），且提出的指标（AUC曲线）专门用于量化推理过程的真实性。
Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning,2025,True,True,False,,论文研究了CoT的忠实性问题，揭示了不忠实现象，如事后找补和捷径学习。,"Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",arXiv.org,2,Black-box,General,该论文明确研究了CoT的忠实性问题，探讨了提示中的暗示如何影响模型生成的推理过程，以及模型是否真实地承认这些暗示。该研究还揭示了模型生成推理过程中的潜在不忠实行为，如事后合理化（post-hoc rationalization）和谄媚行为（sycophancy），这些都是Faithfulness研究的核心问题。
Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,2025,True,True,True,,论文揭示了CoT可能被模型故意模糊以逃避监测的不忠实现象，并提出了评估框架。,"Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection? To answer this question and thus stress-test CoT monitorability, we develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection. (iii) Models do not obfuscate their internal CoT as much as their external CoT (under prompt pressure). These results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.",arXiv.org,2,Black-box,General,该论文明确研究了模型在具有对抗性目标时是否会掩盖其 CoT 推理过程（即 obfuscate），这与 Faithfulness 的核心定义直接相关。论文探讨了 CoT 是否能够忠实反映模型的实际推理过程，尤其是在对抗性环境下，这直接涉及到 CoT 的忠实度和欺骗性问题。
Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT与答案不一致的现象，并提出了基于验证的改进方法。,"Recent advances in large language models (LLMs) have demonstrated that reinforcement learning with verifiable rewards (RLVR) can significantly enhance reasoning abilities by directly optimizing correctness, rather than relying solely on supervised imitation. This paradigm has been extended to multimodal LLMs for complex video and image understanding tasks. However, while outcome-driven RL improves answer accuracy, it can inadvertently decouple the reasoning chain from the final answer, leading to situations where models produce inconsistency between the reasoning trace and final answer. In our experiments on multiple-choice visual question-answering tasks, the standard GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps and the chosen answers, indicating frequent mismatches between answers and reasoning. To this end, we propose Answer-Consistent Reinforcement Learning (ACRE) that modifies the GRPO algorithm with an auxiliary consistency check. After the model generates a chain of thought and an initial answer for a given question, we shuffle the answer options and prompt the model again with the same reasoning trace to predict a second answer. We design a consistency-verification reward that grants a high reward only if both the original and the post-shuffle answers agree and are correct; otherwise, a lower reward is assigned accordingly. This mechanism penalizes reasoning-answer misalignment and discourages the model from relying on spurious patterns, such as option ordering biases. We evaluate ACRE on challenging Video Reasoning benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\% and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the GRPO baseline.",arXiv.org,2,Black-box,General,该论文直接关注了 Chain-of-Thought (CoT) 的忠实度问题，明确指出 RLVR 方法可能导致模型的推理过程与最终答案脱节。论文提出的 ACRE 方法通过一致性检查惩罚推理与答案的不一致性，旨在确保推理过程真实反映模型的实际计算过程，符合 Faithfulness 的核心定义。
Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，如无效推理步骤仍能产生正确答案，挑战了CoT的忠实性假设。,"Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought''reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",arXiv.org,2,White-box,General,该论文直接研究CoT的忠实度问题，通过实验证明中间推理步骤（traces）即使被篡改或无关，模型仍能产生正确解答，表明CoT可能不忠实反映模型的实际计算过程。同时研究了RL微调对trace有效性的影响，属于核心相关的Faithfulness研究。
When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning, Consistency & Ensembling",论文揭示了CoT推理导致指令跟随准确性下降的现象，并提出了度量指标和改进策略。,"Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.",arXiv.org,2,White-box,General,该论文研究了CoT推理在指令跟随中的应用，并揭示了CoT推理可能导致性能下降的现象。通过注意力分析和提出的约束注意力指标，论文探讨了CoT是否真实反映了模型的预测过程。此外，论文还提出了选择性推理策略来缓解这些问题，这些都与Faithfulness的核心定义相关。
When Thinking Drifts: Evidential Grounding for Robust Video Reasoning,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT在视频推理中的不忠实现象，并提出了基于强化学习的改进方法。,"Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term""visual thinking drift"". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only""think before answering"", but also""see while thinking"".",arXiv.org,2,Black-box,General,论文明确研究 CoT 在视频推理任务中的不忠实现象（visual thinking drift），指出 CoT traces 与视觉证据的偏离（divergence from actual visual evidence），并提出了验证 grounding 的强化学习框架 VER，直接对应 Faithfulness 的核心研究范畴。
"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",2024,True,True,False,Verification & External Tools,论文提出了一种生成忠实、简洁且可转移的多模态理由的新范式，涉及不忠实现象和改进方法。,"The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",ACM Multimedia,2,Black-box,General,论文明确研究如何生成忠实（faithful）的多模态理由，并通过可验证的视觉编程保证其忠实度，符合Faithfulness的核心定义。尽管是黑盒方法，但直接解决了CoT的忠实性问题。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的 CoT traces 的忠实性（faithfulness），并探讨了如何通过不同方法（如 CoT prompting、fine-tuning 和 RLVR）来增强模型的忠实度和安全性。这直接涉及到 CoT 是否真实反映模型的计算过程，符合 Faithfulness 的核心定义。
Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT不忠实现象，提出了度量方法，并提出了改进策略。,"Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.",arXiv.org,2,White-box,General,该论文明确研究了Chain-of-Thought（CoT）与模型预测之间的必要性依赖关系，探讨了CoT是否真实反映了模型的推理过程（Faithfulness问题），并发现了正确答案可能与推理过程不一致的现象（Unfaithful）。此外，论文提出的Chain-of-Probe（CoP）方法通过探测模型内部的变化来分析推理的正确性，强调了推理过程的忠实度。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,"论文明确研究 Chain-of-Thought (CoT) 推理的忠实度（Faithfulness），探讨了三种方法（in-context learning, fine-tuning, 和 activation editing）来增强 CoT 的忠实性，并进行了广泛的实证分析。研究中还涉及了模型的内部机制激活编辑"
Break the Chain: Large Language Models Can be Shortcut Reasoners,2024,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT中的捷径学习现象，并提出了新的评估数据集和改进方法。,"Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex modules but are hampered by high token consumption, limited applicability, and challenges in reproducibility. This paper conducts a critical evaluation of CoT prompting, extending beyond arithmetic to include complex logical and commonsense reasoning tasks, areas where standard CoT methods fall short. We propose the integration of human-like heuristics and shortcuts into language models (LMs) through""break the chain""strategies. These strategies disrupt traditional CoT processes using controlled variables to assess their efficacy. Additionally, we develop innovative zero-shot prompting strategies that encourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues and bypass detailed procedural steps. Our comprehensive experiments across various LMs, both commercial and open-source, reveal that LMs maintain effective performance with""break the chain""strategies. We also introduce ShortcutQA, a dataset specifically designed to evaluate reasoning through shortcuts, compiled from competitive tests optimized for heuristic reasoning tasks such as forward/backward reasoning and simplification. Our analysis confirms that ShortcutQA not only poses a robust challenge to LMs but also serves as an essential benchmark for enhancing reasoning efficiency in AI.",arXiv.org,2,Black-box,General,该论文明确研究了 CoT prompting 的局限性，并提出了一种替代策略（Break the Chain），通过可控变量来评估其效果，这与 Faithfulness 的核心定义相关。此外，研究还引入了 ShortcutQA 数据集来评估推理的捷径，这涉及到 CoT 是否真实反映了模型的计算过程（Causal role）。虽然研究没有直接使用 Faithfulness 这一术语，但其内容实际上是在探讨 CoT 的忠实性问题。
Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文研究了CoT的有效性和忠实性，揭示了不忠实现象，并提出了改进算法。,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究了 Chain-of-Thought (CoT) 的忠实度(Faithfulness)问题，特别是分析了 CoT 与问题、答案之间的信息交互，发现了模型在预测答案时可以从未出现在 CoT 中的问题部分回忆起正确信息的现象，这正是 Faithfulness 研究的核心问题。此外，论文还提出了一个算法来缓解这个问题，并通过实验验证了其方法在提升 CoT 忠实度和有效性方面的效果。因此，该论文属于核心相关的研究范畴。
Calibrating Reasoning in Language Models with Internal Consistency,2024,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的内部表示不一致性，提出了内部一致性度量及校准方法。,"Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs. Our code is available at github.com/zhxieml/internal-consistency.",Neural Information Processing Systems,2,White-box,General,该论文研究了生成的 rationale 如何影响模型的内部表示，并发现了中间层与最终层表示之间的不一致性，这直接关系到 CoT 的忠实性问题。通过内部一致性来衡量模型的置信度，并提出了校准推理路径的方法，这些都涉及 CoT 的真实性和因果机制。
Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?,2024,True,True,True,,论文研究了VLM在生成解释时的模态依赖性和自一致性，涉及CoT和忠实性问题。,"Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.",International Conference on Learning Representations,2,White-box,General,该论文研究VLM解码器在生成答案和解释时对输入模态的依赖程度，特别是在CoT和事后解释设置中的自一致性(Consistency)。论文明确探讨了CoT解释是否真实反映了模型对多模态输入的使用方式，并提出了测量一致性的方法，直接指向CoT Faithfulness的核心问题。
Reasoning Models Don't Always Say What They Think,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT不忠实现象，并探讨了RLHF对忠实性的影响。,"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",arXiv.org,2,Black-box,General,论文直接研究CoT忠实度问题，评估CoT是否真实反映模型的真实推理过程，特别关注提示中提示信息的使用情况与CoT口头表达的匹配度，符合核心相关标准
Investigating Faithfulness in Large Audio Language Models,2025,True,True,False,,论文探讨了音频语言模型中CoT的忠实性，并提出了评估方法。,"Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",arXiv.org,2,White-box,General,该论文明确研究 CoT 是否忠实反映了音频语言模型的决策过程，并通过干预措施（如 paraphrasing、filler token injection、early answering）验证了 Faithfulness。研究范围虽针对音频语言模型，但主题与 CoT 忠实度直接相关。
Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens,2025,True,True,False,,论文揭示了CoT推理可能只是表面现象，与训练数据分布相关，属于不忠实现象。,"Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.",arXiv.org,2,White-box,General,论文核心质疑CoT推理的真实性，研究CoT是否只是对训练数据分布的表象模仿而非真正的推理过程。当超出训练分布时CoT效果消失，这直接揭示了CoT可能是一种'虚幻的推理(mirage)'，属于Faithfulness核心研究范畴。论文通过可控环境训练LLM并进行系统探测，涉及模型内部机制分析。
Preventing Language Models From Hiding Their Reasoning,2023,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了模型中‘编码推理’的不忠实现象，并提出了评估防御方法和训练策略。,"Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.",arXiv.org,2,Black-box,General,该论文明确研究了中间推理步骤的忠实性（Faithfulness），特别是探讨了模型可能通过编码方式隐藏真实推理过程（encoded reasoning），并提出了一种评估防御方法的方法论。这直接涉及CoT Faithfulness的核心问题，即推理步骤是否真实反映了模型的内部计算过程。
Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI,2022,True,True,True,Verification & External Tools,论文提出基于反事实的忠实性评估方法，涉及不忠实现象和新度量指标。,"Evaluating an explanation's faithfulness is desired for many reasons such as trust, interpretability and diagnosing the sources of model's errors. In this work, which focuses on the NLI task, we introduce the methodology of Faithfulness-through-Counterfactuals, which first generates a counterfactual hypothesis based on the logical predicates expressed in the explanation, and then evaluates if the model's prediction on the counterfactual is consistent with that expressed logic (i.e. if the new formula is \textit{logically satisfiable}). In contrast to existing approaches, this does not require any explanations for training a separate verification model. We first validate the efficacy of automatic counterfactual hypothesis generation, leveraging on the few-shot priming paradigm. Next, we show that our proposed metric distinguishes between human-model agreement and disagreement on new counterfactual input. In addition, we conduct a sensitivity analysis to validate that our metric is sensitive to unfaithful explanations.",AAAI Conference on Artificial Intelligence,2,White-box,Logic,该论文明确提出了通过反事实逻辑可满足性来评估解释的忠实度（Faithfulness-through-Counterfactuals），无需训练额外验证模型即可检测不忠实解释（Unfaithful explanations），且实验证明了指标对不忠实解释的敏感性，完全符合核心相关标准。方法涉及逻辑推理的内部一致性验证，属于白盒分析。
Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics,2025,True,False,True,,论文提出了一个状态感知转换框架来分析CoT推理的语义角色和一致性，与Faithfulness相关。,"Recent advances in chain-of-thought (CoT) prompting have enabled large language models (LLMs) to perform multi-step reasoning. However, the explainability of such reasoning remains limited, with prior work primarily focusing on local token-level attribution, such that the high-level semantic roles of reasoning steps and their transitions remain underexplored. In this paper, we introduce a state-aware transition framework that abstracts CoT trajectories into structured latent dynamics. Specifically, to capture the evolving semantics of CoT reasoning, each reasoning step is represented via spectral analysis of token-level embeddings and clustered into semantically coherent latent states. To characterize the global structure of reasoning, we model their progression as a Markov chain, yielding a structured and interpretable view of the reasoning process. This abstraction supports a range of analyses, including semantic role identification, temporal pattern visualization, and consistency evaluation.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,论文提出了一个状态感知的转换框架，将CoT轨迹抽象为结构化潜在动态，通过谱分析和聚类技术提取语义连贯的潜在状态，并使用马尔可夫链建模推理过程的全局结构。这种方法直接研究CoT推理的高层语义角色和转换，明确关注推理过程的解释性和一致性，符合Faithfulness的核心定义。
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,2023,True,True,True,Prompting & In-Context Learning,论文提出通过问题分解提升CoT忠实性，并使用了新度量指标验证。,"As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",arXiv.org,2,Black-box,General,该论文明确研究了如何通过问题分解来改进CoT推理的忠实性，提出了提高模型生成推理忠实性的方法，并使用了最近提出的度量标准进行验证。这直接对应于Faithfulness研究的核心定义。
Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning,2023,True,True,False,Consistency & Ensembling,论文通过集成解释和预测来解决不一致性，属于 CoT Faithfulness 的提升方法。,"Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such 'in-context' learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究了解释与预测之间的一致性问题，提出了 Explanation-aware Soft Ensemble (EASE) 框架来减少不可靠解释的影响并提高解释与最终预测的一致性。这直接涉及 CoT Faithfulness 的核心议题，即解释是否真实反映了模型的预测过程。
DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention,2024,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT中的偏见问题，并提出了因果干预方法来提升忠实性。,"Large language models (LLMs) often require 001 task-relevant knowledge to augment their inter-002 nal knowledge through prompts. However, sim-003 ply injecting external knowledge into prompts 004 does not guarantee that LLMs can identify 005 and use relevant information in the prompts to 006 conduct chain-of-thought reasoning, especially 007 when the LLM’s internal knowledge is derived 008 from the biased information on the pretraining 009 data. In this paper, we propose a novel causal 010 view to formally explain the internal knowl-011 edge bias of LLMs via a Structural Causal 012 Model (SCM). We review the chain-of-thought 013 (CoT) prompting from a causal perspective, and 014 discover that the biased information from pre-015 trained models can impair LLMs’ reasoning 016 abilities. When the CoT reasoning paths are 017 misled by irrelevant information from prompts 018 and are logically incorrect, simply editing fac-019 tual information is insufficient to reach the cor-020 rect answer. To estimate the confounding effect 021 on CoT reasoning in LLMs, we use external 022 knowledge as an instrumental variable. We fur-023 ther introduce CoT as a mediator to conduct 024 front-door adjustment and generate logically 025 correct CoTs where the spurious correlation be-026 tween LLMs’ pretrained knowledge and task 027 queries is reduced. With extensive experiments, 028 we validate that our approach enables more 029 accurate CoT reasoning and enhances LLM 030 generation on knowledge-intensive tasks. 031",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,论文通过因果干预（SCM）研究 CoT 如何被预训练的偏见知识误导（Unfaithful），并提出了通过工具变量和 front-door 调整生成逻辑正确的 CoT，直接触及 Faithfulness 的核心问题。
XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs,2023,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出数据集和框架以增强LLM的透明度和可靠性，涉及CoT忠实性。,"Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM’s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the debugger-scores for multidimensional quality analysis. Our explanations include why-choose and why-not-choose components, reason-elements, and debugger-scores that collectively illuminate the LLM’s reasoning behavior. Our evaluations demonstrate XplainLLM’s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文提出 XplainLLM 数据集和解释框架，旨在增强 LLM 的透明度和可靠性，特别关注了生成基于知识图谱的解释，并提供了多维质量分析。这些功能直接关联到 CoT 的忠实度问题，即确保解释真实反映模型的推理行为，而不是事后合理化。
Self-contradictory reasoning evaluation and detection,2023,True,True,True,,论文研究了自我矛盾推理现象，并提出了评估指标，与CoT忠实性高度相关。,"In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks only focus on final answers. Two fundamental questions persist: 1) how consistent is the reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support its answers. To answer 1), we define and assess the Self-Contra rate across three datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves in reasoning tasks involving contextual information understanding or commonsense. The model may generate correct answers by taking shortcuts in reasoning or overlooking contextual evidence, leading to compromised reasoning. For 2), we task the state-of-the-art model GPT-4 with identifying Self-Contra reasoning and finer-grained fallacies. We find that finer-grained categories enhanced detection can improve GPT-4's ability to detect Self-Contra. However, it is only able to detect Self-Contra with a 52.2% F1 score, much lower compared to 66.7% for humans. Our results indicate that current LLMs lack the robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond pure performance-based metrics.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,这篇论文研究了自我矛盾的推理（Self-Contra reasoning），明确指出模型推理过程中生成的解释与其最终答案不一致。这直接涉及到 CoT 的忠实性问题（Faithfulness），即生成的解释是否真实反映了推理过程，特别是当模型通过捷径（shortcuts）或忽略上下文证据生成正确答案时。论文还讨论了如何检测不可靠的推理，进一步强调了推理过程的忠实性。
A Pragmatic Way to Measure Chain-of-Thought Monitorability,2025,True,False,True,,论文提出了衡量 CoT 可监控性的新指标，与 Faithfulness 相关，但未讨论不忠实现象或改进方法。,"While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI safety, this opportunity could be lost through shifts in training practices or model architecture. To help preserve monitorability, we propose a pragmatic way to measure two components of it: legibility (whether the reasoning can be followed by a human) and coverage (whether the CoT contains all the reasoning needed for a human to also produce the final output). We implement these metrics with an autorater prompt that enables any capable LLM to compute the legibility and coverage of existing CoTs. After sanity-checking our prompted autorater with synthetic CoT degradations, we apply it to several frontier models on challenging benchmarks, finding that they exhibit high monitorability. We present these metrics, including our complete autorater prompt, as a tool for developers to track how design decisions impact monitorability. While the exact prompt we share is still a preliminary version under ongoing development, we are sharing it now in the hopes that others in the community will find it useful. Our method helps measure the default monitorability of CoT - it should be seen as a complement, not a replacement, for the adversarial stress-testing needed to test robustness against deliberately evasive models.",arXiv.org,2,Black-box,General,该论文提出了衡量 CoT 可监测性的方法，包括 legibility 和 coverage，这两个指标直接关联到 CoT 是否忠实反映模型的实际推理过程（Faithfulness）。尽管论文未明确使用 'Faithfulness' 一词，但其研究内容与 CoT 忠实度高度相关。
A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,2024,True,True,True,,论文探讨了自解释模型的忠实性问题，并提出了评估框架，但未提出改进方法。,"The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM’s and the Bayesian network’s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework’s potential to approximate LLM decisions better in future work.",Conference of the European Chapter of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了LLM生成解释的忠实性问题，提出了一种统计框架来比较LLM和贝叶斯网络的决策过程，直接关联到Faithfulness的核心问题。
Properties and Challenges of LLM-Generated Explanations,2024,True,True,False,,论文探讨了LLM生成解释的特性，涉及不忠实现象，但未提出具体度量或改进方法。,"The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task-specific data sets.However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs.The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning.As the pre-training corpus includes a large amount of human-written explanations “in the wild”, we hypothesise that LLMs adopt common properties of human explanations.By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading.We discuss reasons and consequences of the properties’ presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.",,2,Black-box,General,该论文研究了LLM生成的解释的性质，包括选择性、说明性元素以及较少的主观性或误导性，这与CoT Faithfulness的核心关注点——解释是否真实反映模型的预测过程——直接相关。论文还讨论了这些性质存在的正面和负面影响，进一步关联到Faithfulness的概念。
Reasoning Models Sometimes Output Illegible Chains of Thought,2025,True,True,False,,论文研究了CoT的legibility与faithfulness的关系，揭示了RL训练导致的不忠实现象。,"Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（Faithfulness）问题，特别是通过强化学习方法训练的模型生成的 CoT 是否可读（legible），并探讨了 CoT 的可读性与模型性能之间的关系。论文还提出了可能导致 CoT 不忠实的原因（如隐写术、训练伪影等），符合核心相关标准。
AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning,2025,True,True,True,,论文提出AURA基准和AuraScore指标评估推理忠实性，揭示了模型推理错误但答案正确的现象。,"Current audio-visual (AV) benchmarks focus on final answer accuracy, overlooking the underlying reasoning process. This makes it difficult to distinguish genuine comprehension from correct answers derived through flawed reasoning or hallucinations. To address this, we introduce AURA (Audio-visual Understanding and Reasoning Assessment), a benchmark for evaluating the cross-modal reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across six challenging cognitive domains, such as causality, timbre and pitch, tempo and AV synchronization, unanswerability, implicit distractions, and skill profiling, explicitly designed to be unanswerable from a single modality. This forces models to construct a valid logical path grounded in both audio and video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To assess reasoning traces, we propose a novel metric, AuraScore, which addresses the lack of robust tools for evaluating reasoning fidelity. It decomposes reasoning into two aspects: (i) Factual Consistency - whether reasoning is grounded in perceptual evidence, and (ii) Core Inference - the logical validity of each reasoning step. Evaluations of SOTA models on AURA reveal a critical reasoning gap: although models achieve high accuracy (up to 92% on some tasks), their Factual Consistency and Core Inference scores fall below 45%. This discrepancy highlights that models often arrive at correct answers through flawed logic, underscoring the need for our benchmark and paving the way for more robust multimodal evaluation.",arXiv.org,2,Black-box,General,该论文明确提出了评估推理忠实度的新度量标准 AuraScore，重点关注推理过程的真实性（Factual Consistency 和 Core Inference），与 CoT Faithfulness 的核心定义高度契合。虽然研究领域是多模态（音频-视频），但其核心贡献可直接迁移至纯文本 CoT 的忠实度评估场景。
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,2023,True,True,True,Training & Fine-tuning,论文探讨了视觉语言模型的推理一致性，提出了基于CoT的度量方法，并通过两阶段训练框架提升一致性和性能。,"Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究了视觉语言模型(VLMs)的链式思维推理的一致性和真实性(consistency和groundedness)，提出了一种基于CoT的一致性测量方法，并设计了两阶段训练框架以提高推理性能和一致性。这直接符合Faithfulness的核心定义，即生成的CoT是否真实反映了模型的推理过程。
The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge,2025,True,True,False,,论文揭示了LLM在作为评判者时存在捷径偏见和不忠实现象，但未提出具体度量或改进方法。,"Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert>Human>LLM>Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.",arXiv.org,2,Black-box,Society,该论文明确研究了LLM作为评判者时的忠实性问题，揭示了模型在评判过程中依赖表面线索（如来源和时间）而非真实内容质量的倾向，且其解释（justifications）未承认这些偏见，属于典型的Post-hoc Rationalization现象，直接符合Faithfulness的核心定义。
Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning,2025,True,False,True,Verification & External Tools,论文提出基于Curry-Howard对应关系的验证框架，将CoT映射为形式化证明，属于忠实性度量方法。,"While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",arXiv.org,2,White-box,General,该论文明确提出了一个基于 Curry-Howard 对应的理论框架，用于验证 CoT 生成的 rationale 的忠实性。通过将 CoT 步骤映射到形式化的类型化证明结构，提供了一种可验证的计算忠实性证书，直接涉及 CoT 的因果角色和忠实性验证方法。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,2,Black-box,General,该论文明确提到了增强推理过程的 'faithfulness'，并通过多模型协作（Debate、Review、Retrieve模式）来提高推理的可信度和可靠性。虽然主要关注点是通过多模型协作来克服幻觉和提高解决方案的质量，但仍然涉及到了Faithfulness的概念，因此属于核心相关。
LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations,2025,True,True,False,,论文揭示了LLMs生成的自反事实解释不忠实于内部决策过程的现象。,"To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",,2,Black-box,General,论文研究了LLM生成的自我反事实解释（SCEs）的有效性和最小性，发现LLM生成的解释虽然是有效的，但并不最小化，且在小编辑的情况下无法改变预测结果。这表明LLM的解释可能具有误导性，涉及了CoT忠实度的核心问题，即解释是否真实反映了模型的决策过程。
"Survey on Hallucination in Reasoning Large Language Model: Evaluation, Taxonomy, Intervention, and Open Issues",2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了CoT中的幻觉现象，提出了评估框架和干预策略，与Faithfulness高度相关。,"In recent years, reasoning large language models (LLMs) have seen increasingly widespread adoption in the field of education, particularly demonstrating substantial potential in tasks involving complex text comprehension. However, these LLMs are susceptible to a critical yet often overlooked issue: hallucinations within the reasoning process—instances where the model outputs a correct final answer while its underlying reasoning chain contains fabricated, inconsistent, or logically flawed content. Such hallucination phenomena in Chain-of-Thought (CoT) processes pose serious challenges to the reliability of educational applications. To address this issue, this study proposes a systematic research framework comprising dataset construction, multi-model CoT evaluation, and hallucination classification and quantification. Utilizing the whole-book reading dataset aligned with the junior secondary Chinese language curriculum, we conduct a comparative evaluation of six leading domestic and international LLMs, including ChatGPT o1 and DeepSeek-R1. Key findings include:(1) Hallucinations in CoT are prevalent across all tested models, with ChatGPT-o1 exhibiting a distinctive high accuracy–high hallucination pattern;(2) Hallucinations are both task-and genre-dependent: narrative texts, particularly novels, tend to trigger higher hallucination indices due to long-range dependencies and implicit cultural references. Tasks involving logical reasoning, linguistic feature analysis, and detail extraction show the highest hallucination rates, revealing model weaknesses in handling long-tail knowledge;(3) Hallucinations typically follow a progressive generative pattern: Information mis-reading → Comprehension deviation → Content fabrication → Logical instability. To mitigate these issues, we propose two targeted intervention strategies: uncertainty-based abstention and model-to-model correction. These approaches o ff er practical pathways toward enhancing the trustworthiness and educational applicability of reasoning LLMs.",Data Intelligence,2,Black-box,General,"该论文直接研究了 CoT 过程中的幻觉现象（fabricated, inconsistent, or logically flawed content），这与 Faithfulness 的核心定义密切相关。论文不仅评估了 CoT 中的幻觉现象，还提出了干预策略，进一步证明了其关注点是 CoT 过程的忠实性而非仅仅答案的正确性。尽管研究方法是通过输入输出进行的（Black-box），但其内容对 Faithfulness 的研究有显著贡献。"
Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models,2025,True,False,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了LLMs中的推理算法原语及其几何组合，与CoT和Faithfulness相关，提出了度量框架和改进方法。,"How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",arXiv.org,2,White-box,General,该论文明确研究了模型内部激活模式与推理痕迹之间的关联，通过注入算法原语并测量其对推理步骤的影响，直接探讨了推理过程的忠实性问题。方法涉及聚类神经激活和标记匹配的推理痕迹，属于白盒研究。
Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation,2025,True,True,False,Prompting & In-Context Learning,论文探讨了CoT在代码生成中的质量问题，揭示了不忠实现象，并提出了通过提示改进CoT的方法。,"Large language models (LLMs) have demonstrated impressive performance in code generation, particularly when augmented with chain-of-thought (CoT) prompting techniques. They break down requirements into intermediate reasoning steps, which act as design rationales to guide LLMs in writing code like human programmers. Thus, the quality of these steps is crucial for ensuring the correctness and reliability of the generated code. However, little is known about the quality of CoT generated by LLMs. To what extent can we trust the thoughts generated by LLMs? How good are they? This paper empirically explores the external and internal factors of why LLMs generate unsatisfactory CoTs by analyzing 1,023 failed code samples on two widely used code generation benchmarks. We also evaluate their impact on code generation performance by analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting LLMs. Our study reveals three key findings: (1) External factors (53.60%), such as unclear requirements and lack of context, mainly affect CoT quality, while internal factors (40.10%) stem from LLMs'misunderstanding prompts. (2) Even when CoTs are correct, 18.5% of the generated code contains errors due to instruction-following issues; conversely, 11.90% of correct code is paired with flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when given detailed problem descriptions. These findings highlight key challenges in CoT-based code generation and suggest directions for improving LLM reasoning and reliability.",arXiv.org,2,Black-box,Code,该论文研究了 CoT 生成的质量及其对代码生成正确性和可靠性的影响，特别是探讨了 CoT 的内部和外部因素如何导致不满意或不准确的生成。它还揭示了 CoT 正确性与代码正确性之间的不一致性，这直接涉及到 CoT 的忠实性问题（是否真实反映了模型的计算过程）。因此，该论文属于核心相关的研究范畴。
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,2,Black-box,Medical,论文明确研究了LLM在复杂医学表型任务中的推理错误（Reasoning Errors），特别区分了解释正确性错误（explanation correctness errors）和不忠实错误（unfaithfulness errors），并评估了提示修改对这些错误的影响，符合Faithfulness研究的核心定义。
Humans Perceive Wrong Narratives from AI Reasoning Texts,2025,True,True,False,,论文揭示了人类对AI推理文本的理解与模型实际计算过程之间的不匹配，涉及不忠实现象。,"A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction.",arXiv.org,2,Black-box,General,该论文研究了人类是否能识别AI生成的推理文本中哪些步骤因果影响了后续步骤，直接涉及CoT的Faithfulness问题，并通过反事实测量揭示了人类理解与模型实际计算过程之间的显著差异。
Chain-of-Thought Unfaithfulness as Disguised Accuracy,2024,True,True,True,,论文讨论了CoT忠实性度量的问题，并揭示了度量与模型准确性之间的相关性，质疑其有效性。,"Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, Lanham et al. (2023) propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate the experimental setup in their section focused on scaling experiments with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models. This normalized faithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness.",,2,Black-box,General,该论文直接研究 CoT 忠实度（Faithfulness）问题，验证了模型内部计算与 CoT 生成是否一致，并质疑了现有度量标准的有效性（伪装成准确度的忠实度）。
Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification,2025,True,True,False,Verification & External Tools,论文讨论了CoT忠实性问题，并提出了运行时验证方法来提升对齐性。,"Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",arXiv.org,2,Black-box,General,论文明确研究 Reasoning-Action Alignment（即 CoT 与实际行动的忠实度），提出运行时验证方法来确保文本推理与实际动作的一致性，直接涉及 Faithfulness 的核心问题。
Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出通过可验证奖励提升 CoT 忠实性，并讨论了不忠实现象。,"The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",arXiv.org,2,Black-box,General,论文明确提出了一个框架（Chart-RVR）来增强 Large Vision-Language Models（LVLMs）在图表推理任务中的鲁棒性和可解释性，特别是通过自动可验证的奖励来最大化流程一致性（process conformity）和 faithfulness 的图表表格重建，从而改进 rationale fidelity。这与 CoT Faithfulness 的定义高度相关，尤其是论文中提到的 'improving rationale fidelity' 和 'faithful chart table reconstruction' 直接涉及 CoT 推理的真实性和忠实度。
Are DeepSeek R1 And Other Reasoning Models More Faithful?,2025,True,True,True,Training & Fine-tuning,论文评估了推理模型的忠实性，揭示了不忠实现象，并提出了度量指标。,"Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue""A Stanford Professor thinks the answer is D""is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",,2,Black-box,General,论文明确研究推理模型（DeepSeek-R1）与非推理模型在 Chain-of-Thought 忠实度上的差异，实验设计了多种提示线索（cue）来测试模型是否能准确描述这些线索对其答案的影响。研究直接针对 CoT 解释的真实性（Faithfulness），并通过对比实验验证了不同类型的模型在忠实度上的表现差异，尤其是讨论了奖励模型可能导致不忠诚响应的现象。
Markovian Transformers for Informative Language Modeling,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出了一种新的训练方法，通过强制模型将推理压缩到可解释的文本中，以提高CoT的忠实性。,"Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.",,2,White-box,General,该论文明确研究了CoT是否忠实反映模型的决策过程，引入了Markovian框架并通过扰动分析证明CoT的因果依赖性，符合Faithfulness的核心定义。
Large Language Models Cannot Explain Themselves,2024,True,True,False,,论文讨论了LLM生成的解释不忠实于内部计算过程的现象。,"Large language models can be prompted to produce text. They can also be prompted to produce""explanations""of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These""explanations""can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these""explanations"", using the term""exoplanations""to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.",arXiv.org,2,Black-box,General,论文明确指出语言模型生成的‘解释’并不真实反映预测的机械过程，提出了‘exoplanations’概念以强调其外生性，与研究模型生成解释是否忠实反映其计算过程的核心问题直接相关。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,2,Black-box,General,论文明确提出了 Faithful CoT 框架，关注生成的推理链是否真实反映模型的计算过程（faithfulness），并设计了两个阶段来保证这一点。虽然涉及多个领域，但核心贡献是对 faithfulness 的研究，因此属于核心相关。
Chain of Thoughtlessness? An Analysis of CoT in Planning,2024,True,True,False,,论文揭示了CoT在规划问题中的不忠实现象，即性能提升依赖于特定问题提示而非通用算法学习。,"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.",Neural Information Processing Systems,2,Black-box,Logic,该论文明确指出 CoT 的性能改进并非源于模型学习通用的算法程序，而是依赖于高度特定问题的提示，这与 Faithfulness 的核心问题（即 CoT 是否真实反映了模型的推理过程）密切相关。
LLM-Generated Black-box Explanations Can Be Adversarially Helpful,2024,True,True,False,,论文揭示了LLM生成的不忠实CoT现象，即对抗性帮助，使错误答案看起来合理。,"Large Language Models (LLMs) are becoming vital tools that help us solve and understand complex problems by acting as digital assistants. LLMs can generate convincing explanations, even when only given the inputs and outputs of these problems, i.e., in a ``black-box'' approach. However, our research uncovers a hidden risk tied to this approach, which we call *adversarial helpfulness*. This happens when an LLM's explanations make a wrong answer look right, potentially leading people to trust incorrect solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing the questions, expressing an elevated level of confidence, and cherry-picking evidence to paint misleading answers in a credible light. To examine if LLMs are able to navigate complex-structured knowledge when generating adversarially helpful explanations, we create a special task based on navigating through graphs. Most LLMs are not able to find alternative paths along simple graphs, indicating that their misleading explanations aren't produced by only logical deductions using complex knowledge. These findings shed light on the limitations of the black-box explanation setting and allow us to provide advice on the safe usage of LLMs.",arXiv.org,2,Black-box,General,论文明确研究LLM生成的解释（黑盒方法）如何使错误答案看起来正确，即存在‘对抗性帮助’现象，这与CoT Faithfulness中讨论的‘事后合理化’和‘欺骗性解释’直接相关。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,2,Black-box,General,这篇论文直接研究LLM能否给出忠实反映其内部分类行为的高层次解释。通过构建ArticulateRules数据集，测试模型生成的自由形式自然语言解释是否与实际分类行为相匹配，这本质上就是在探究CoT的忠实度问题。论文发现GPT-3在7/10规则上完全无法给出匹配自身行为的解释，这正是faithfulness研究的核心。
Dissociation of Faithful and Unfaithful Reasoning in LLMs,2024,True,True,False,,论文揭示了LLMs在CoT中存在不忠实现象，即推理错误但答案正确。,"Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.",arXiv.org,2,Black-box,General,该论文明确研究了CoT中的unfaithfulness现象，分析了模型在错误推理后仍能得出正确答案的行为，并探讨了驱动忠实与非忠实恢复的不同机制。这直接符合Faithfulness研究的核心定义，尤其是在CoT中区分忠实与非忠实推理。
Why Would You Suggest That? Human Trust in Language Model Responses,2024,True,True,False,,论文探讨了解释的忠实性对用户信任的影响，涉及不忠实现象。,"The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.",arXiv.org,2,Black-box,Society,论文研究了解释的忠实性（faithfulness）对人类信任的影响，明确指出解释的位置和忠实性是重要因素，并揭示了用户在独立查看时会不加区分地信任所有模型响应（包括欺骗性的），这直接关联到 CoT 的忠实度问题。
FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain,2024,True,False,True,Consistency & Ensembling,论文使用自洽性（self-consistency）和多数投票提升CoT性能，并报告了忠实性分数。,"This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improve the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make thefinal verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly.",International Workshop on Semantic Evaluation,2,Black-box,Medical,该论文明确提到了 Faithfulness score，表明其研究了 CoT 的真实性，并且是在医疗领域进行研究。虽然使用了 Black-box 方法（如自洽性和多数投票），但直接测量了 Faithfulness，因此属于核心相关。
Rethinking harmless refusals when fine-tuning foundation models,2024,True,True,False,Training & Fine-tuning,论文揭示了模型在微调后可能产生不忠实的推理痕迹（reason-based deception），并提出了通过反驳策略改进的方法。,"In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.",arXiv.org,2,Black-box,Society,该论文明确研究 CoT 推理是否真实反映模型输出（Reason-based deception现象），属于 Faithfulness 核心范畴。
Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文揭示了CoT可能强化模型偏见的不忠实现象，并提出了APriCoT方法来改进。,"Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.",,2,Black-box,Society,论文研究了CoT推理中存在的偏见问题，指出CoT倾向于强化模型的快速思考偏见，这与Faithfulness直接相关。提出的APriCoT方法通过反事实提示来减少偏见影响，属于CoT忠实度的核心研究。
Rationale-Aware Answer Verification by Pairwise Self-Evaluation,2024,True,True,False,Verification & External Tools,论文讨论了LLM生成答案中理性与答案不一致的问题，并提出了改进验证方法。,"Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier’s ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文直接研究了 rationale（相当于 CoT）的有效性验证问题，明确指出仅凭最终答案正确性无法保证推理过程的忠实性（19%正确答案对应无效推理），并提出了通过 pairwise self-evaluation 筛选可靠 rationale 的方法。这完全符合 Faithfulness 研究范畴的核心定义（关注推理过程是否真实反映模型计算，而非仅答案正确性）。
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness,2023,True,True,True,,论文提出了评估推理链的框架ReCEval，关注正确性和信息量，与CoT忠实性相关。,"Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound reasoning quality with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains via two key properties: (1) correctness, i.e., each step makes a valid inference based on information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We evaluate these properties by developing metrics using natural language inference models and V-Information. On multiple datasets, we show that ReCEval effectively identifies various error types and yields notable improvements compared to prior methods. We analyze the impact of step boundaries, and previous steps on evaluating correctness and demonstrate that our informativeness metric captures the expected flow of information in high-quality reasoning chains. Finally, we show that scoring reasoning chains based on ReCEval improves downstream task performance. Our code is publicly available at: https://github.com/archiki/ReCEval",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文提出了ReCEval框架，专注于评估推理链的正确性和信息量，这与Faithfulness直接相关。特别是，它强调了推理链条中的每一步是否基于有效的信息进行推断，以及每个步骤是否提供了有助于生成答案的新信息。这不仅关注推理链条是否导致正确的结论，还关注推理链条本身的质量和忠实度。此外，论文提出的方法论通过自然语言推理模型和V-信息来衡量这些属性，从而有效地识别了各种错误类型。
Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering,2023,True,True,False,Training & Fine-tuning,论文提出了一种可解释的中间步骤设计，确保忠实性，并讨论了不忠实现象。,"Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",,2,Black-box,General,该论文明确提出模型的解释必须忠实（faithful）于模型的实际决策过程，并设计了一个可解释的中间空间（Dynamic Clue Bottleneck Model）来确保解释的真实性。这与Faithfulness的核心定义高度契合，特别是解决了Post-hoc rationales的不忠实问题。
Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers,2025,True,True,True,,论文探讨了LLMs在MCQA任务中的推理策略，并测试了其忠实性。,"Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",arXiv.org,2,Black-box,General,该论文直接研究 CoT 的忠实性问题，通过实验验证了 LLMs 在只给出选项（choices-only）时生成的推理痕迹（reasoning traces）是否真实反映了其决策逻辑（faithfulness tests），并发现这些痕迹揭示了模型并非完全依赖浅层策略（如直接选择答案），而是尝试推断缺失的问题内容。这直接关联到 CoT Faithfulness 的核心议题：推理是否真实反映了模型的计算过程，而非事后合理化（post-hoc rationalization）。
Measuring Faithfulness in Chain-of-Thought Reasoning,2023,True,True,True,,论文探讨了CoT的忠实性问题，提出了干预方法评估忠实性，并揭示了不忠实现象。,"Large language models (LLMs) perform better when they produce step-by-step,""Chain-of-Thought""(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",arXiv.org,2,Black-box,General,这篇论文明确研究了 Chain-of-Thought (CoT) 推理的忠实性（Faithfulness），探讨了 CoT 是否是模型预测的真实解释。通过干预 CoT（如添加错误或改写）来观察模型预测的变化，直接验证了 CoT 的因果作用（Causal role）。此外，论文还分析了模型大小和任务对 CoT 忠实性的影响，符合核心定义中对 Faithfulness 的研究。
Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations,2023,True,True,True,,论文探讨了LLM解释的忠实性问题，提出了基于反事实可模拟性的度量指标，揭示了不忠实现象。,"Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers""yes""to the input question""Can eagles fly?""with the explanation""all birds can fly"", then humans would infer from the explanation that it would also answer""yes""to the counterfactual input""Can penguins fly?"". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.",International Conference on Machine Learning,2,Black-box,General,该论文明确研究自然语言解释（NLE）是否能帮助人类预测模型在反事实输入下的行为（counterfactual simulatability），直接关联到解释是否忠实反映模型的实际决策过程（Faithfulness）。提出的两个指标（precision和generality）用于衡量解释的忠实度，并发现LLM的解释精度低且与合理性不相关，揭示了事后合理化（Post-hoc rationalization）现象。
Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions,2023,True,False,True,,论文通过梯度特征归因方法评估CoT提示的忠实性，属于度量框架研究。,"Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",arXiv.org,2,White-box,General,这篇论文通过基于梯度的特征归因方法研究了CoT prompting如何影响模型对输入令牌的重要性分配，直接探究了CoT是否真实影响了模型的计算过程（Faithfulness）。研究结果表明CoT提升了saliency scores对问题扰动和模型输出变化的鲁棒性，表明其在忠实地改变模型推理机制而非单纯事后合理化。
Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models,2023,True,False,False,Interpretability & Internal Mechanisms,论文通过干预注意力层提升多跳推理的忠实性，涉及内部机制改进。,"Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,General,该论文通过分析Transformer模型的内部激活（per-layer activations）并干预注意力层（memory injections）来修正多跳推理失败，直接涉及模型推理过程的忠实性（Faithfulness）。研究关注的是模型内部计算如何影响推理的真实性，而非单纯提升性能或事后解释。
"A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",2025,True,True,True,,论文探讨了多语言CoT推理的性能、一致性和忠实性，并使用了扰动技术来评估忠实性。,"Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",arXiv.org,2,Black-box,General,论文明确研究 Chain-of-Thought (CoT) 的忠实性（Faithfulness），通过扰动技术（如截断和错误注入）来探测不同语言间思考痕迹的忠实性。此外，还评估了跨语言一致性（Consistency），这些都是 CoT Faithfulness 的核心研究方向。
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,2024,True,False,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文提出基于因果关系的提示方法，利用CoT作为中介变量，并通过微调提升忠实性。,"Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究了基于因果关系的 Prompting 方法，特别是利用 Front-Door Adjustment 来缓解 LLM 中的偏见问题。论文使用 Chain-of-Thought（CoT）作为中介变量，计算输入提示和输出答案之间的因果效应，从而直接涉及 CoT 是否真实反映了模型预测的计算过程，符合 Faithfulness 研究的核心定义。
Quantifying Uncertainty in Natural Language Explanations of Large Language Models,2023,True,True,True,,论文提出了量化LLM解释不确定性的新指标，并探讨了解释的忠实性。,"Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",International Conference on Artificial Intelligence and Statistics,2,Black-box,General,这篇论文明确研究了LLM生成的解释是否可靠并反映了LLM的行为，提出了量化解释不确定性的指标，并发现探测不确定性估计与解释的忠实度相关。这直接符合Faithfulness的核心定义，即解释是否真实反映了模型的计算过程。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,2,Black-box,General,论文明确提出了 Thinking-supervised Reward Model (TRM)，通过 sentence-level thinking supervision 来评估语句级别的 faithfulness 和 correctness。该方法直接关注于生成的解释（CoT）是否真实反映了模型的预测过程，属于 Faithfulness 的核心研究范畴。
Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations,2025,True,True,True,,论文探讨了VLMs中CoT解释的忠实性问题，并提出了新的评估框架。,"Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone ($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality can be decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.",arXiv.org,2,White-box,Medical,论文明确研究 CoT 解释是否真实反映决策过程（Faithfulness），提出了测评框架并通过多维扰动实验（临床保真度、因果归因、置信度校准）验证了 CoT 与模型内部决策的对齐程度，属于核心相关领域。
Are LLMs Better Formalizers than Solvers on Complex Problems?,2025,True,True,False,Verification & External Tools,论文讨论了LLM作为形式化器的忠实性问题，并提出了使用外部求解器作为改进方法。,"A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer.",,2,Black-box,Logic,论文明确指出 LLM 作为 formalizer 时未能兑现其承诺的准确性、鲁棒性、忠实性和效率，特别是提到了 'faithfulness' 作为评估指标之一。
Towards Large Language Models with Self-Consistent Natural Language Explanations,2025,True,True,True,Training & Fine-tuning,论文揭示了LLM事后解释的不一致性，并提出了新的度量指标和优化方法。,"Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature importance. Despite growing evidence of this inconsistency, no systematic solutions have emerged, partly due to the high cost of estimating feature importance, which limits evaluations to small datasets. To address this, we introduce the Post-hoc Self-Consistency Bank (PSCB) - a large-scale benchmark of decisions spanning diverse tasks and models, each paired with LLM-generated explanations and corresponding feature importance scores. Analysis of PSCB reveals that self-consistency scores barely differ between correct and incorrect predictions. We also show that the standard metric fails to meaningfully distinguish between explanations. To overcome this limitation, we propose an alternative metric that more effectively captures variation in explanation quality. We use it to fine-tune LLMs via Direct Preference Optimization (DPO), leading to significantly better alignment between explanations and decision-relevant features, even under domain shift. Our findings point to a scalable path toward more trustworthy, self-consistent LLMs.",arXiv.org,2,Black-box,General,论文明确研究了 LLM 生成的事后解释（post-hoc explanations）与真实决策过程的不一致问题（mismatches in feature importance），并提出了一种新的度量标准来更有效地捕捉解释质量的变化。这直接涉及 CoT 的忠实度（Faithfulness）问题，即解释是否真实反映了模型的决策过程。虽然论文没有直接提到 CoT，但其研究的核心问题是解释的真实性和一致性，这与 CoT Faithfulness 的研究范畴高度相关。
Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出因果框架评估CoT的充分性和必要性，涉及不忠实现象及改进方法。,"Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.",arXiv.org,2,White-box,Math,该论文明确提出使用因果充分性和必要性来评估和改进CoT推理，这与Faithfulness的核心定义直接相关。论文不仅量化了推理步骤对最终结果的实际影响，还通过干预场景验证了推理步骤的忠实性。此外，实验涉及数学和常识推理基准，符合CoT Faithfulness的研究范畴。
Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,2025,True,True,False,,论文揭示了CoT中的不忠实现象，如事后找补和欺骗性推理，但未提出新度量或改进方法。,"Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (""I'll trick the user...""), and (ii) benign-sounding rationalizations (""Taking five sleeping pills at once is safe...""). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment. We examine sleeper agent reasoning models, extending our setup. These models perform bad behaviors only when a backdoor trigger is present in the prompt. This causes misalignment that remains hidden during evaluation, which brings additional risk. We find that sleeper agents can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable. In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied. We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.",arXiv.org,2,White-box,General,论文明确研究 CoT 是否真实反映模型的意图或行为（如欺骗性回答和良性合理化），符合 Faithfulness 的核心定义。论文还探讨了 CoT 监视的不可靠性，显示了 CoT 可能掩盖不对齐意图的情况，这些都直接关联到 CoT 的忠实度问题。
Are Language Models Consequentialist or Deontological Moral Reasoners?,2025,True,True,False,,论文揭示了LLM在道德推理中CoT与事后解释的不一致现象，符合不忠实现象的定义。,"As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .",,2,Black-box,Society,这篇论文研究了语言模型在道德困境中的推理过程，特别关注了CoT是否忠实反映了模型的决策逻辑。研究发现LLM的CoT倾向于义务论原则，而事后解释则转向功利主义，直接涉及了CoT忠实度和事后合理化问题。
Auditing Meta-Cognitive Hallucinations in Reasoning Large Language Models,2025,True,True,True,,论文揭示了CoT中的不忠实现象（chain disloyalty），并提出了黑盒审计方法进行度量。,"The development of Reasoning Large Language Models (RLLMs) has significantly improved multi-step reasoning capabilities, but it has also made hallucination problems more frequent and harder to eliminate. While existing approaches mitigate hallucinations through external knowledge integration, model parameter analysis, or self-verification, they often fail to capture how hallucinations emerge and evolve across the reasoning chain. In this work, we study the causality of hallucinations under constrained knowledge domains by auditing the Chain-of-Thought (CoT) trajectory and assessing the model's cognitive confidence in potentially erroneous or biased claims. Our analysis reveals that in long-CoT settings, RLLMs can iteratively reinforce biases and errors through flawed reflective reasoning, eventually leading to hallucinated reasoning paths. Surprisingly, even direct interventions at the origin of hallucinations often fail to reverse their effects, as reasoning chains exhibit'chain disloyalty'-- a resistance to correction and a tendency to preserve flawed logic. Furthermore, we show that existing hallucination detection methods are less reliable and interpretable than previously assumed in complex reasoning scenarios. Unlike methods such as circuit tracing that require access to model internals, our black-box auditing approach supports interpretable long-chain hallucination attribution, offering better generalizability and practical utility. Our code is available at: https://github.com/Winnie-Lian/AHa_Meta_Cognitive",arXiv.org,2,Black-box,General,论文研究了 Chain-of-Thought (CoT) 轨迹中幻觉的因果关系，揭示了 RLLMs 在长 CoT 设置下通过有缺陷的反射推理迭代强化偏见和错误，最终导致幻觉推理路径。此外，论文还发现即使是直接干预幻觉的源头也往往无法逆转其影响，因为推理链表现出'chain disloyalty'（对纠正的抵抗和保留有缺陷逻辑的倾向）。这些发现直接关联到 CoT Faithfulness 的研究范畴，特别是关于 Post-hoc Rationalization 和 Sycophancy 的问题。
CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process,2025,True,False,True,,论文提出了一种评估推理轨迹质量的新方法，与 CoT 忠实性相关。,"Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.",arXiv.org,2,White-box,General,这篇论文提出了一种新的方法来评估推理轨迹的 soundness，并通过 CoT-Kinetics energy equation 来衡量推理阶段对答案的因果影响，直接触及了 Faithfulness 的核心问题。它研究了推理轨迹是否真实反映了模型的内部计算过程，符合核心相关的标准。
Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了如何通过训练提升自我解释的忠实性，并验证了其泛化能力。,"Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",,2,White-box,General,该论文明确研究了语言模型自我解释的忠实性(Faithfulness)，并探讨了如何通过训练和特征归因方法来改进忠实性。论文还分析了不同解释风格的泛化能力，符合核心相关标准。
A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models,2025,True,True,True,,论文研究了CoT忠实性，揭示了不忠实现象并提出了新的评估框架。,"Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent''reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",Conference on Empirical Methods in Natural Language Processing,2,White-box,Society,该论文直接研究了 CoT 忠实性问题，特别是探讨了文本和图像偏见如何影响推理过程和偏见表达。此外，论文还发现了模型在推理中存在的不一致现象，即‘inconsistent reasoning’，这直接关联到 CoT 是否真实反映了模型的内部计算过程。研究方法和结果都紧密围绕 CoT 忠实性展开，属于核心相关研究。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到 CoT 生成的解释易受内容偏见影响，从而影响其鲁棒性和忠实度（faithfulness）。作者提出的 QuaSAR 方法旨在通过准符号抽象来改善 CoT 的忠实度，且实验验证了其在对抗性任务上的鲁棒性和一致性提升。因此，该论文直接涉及 CoT 的忠实度研究。
Investigating CoT Monitorability in Large Reasoning Models,2025,True,True,False,,论文探讨了CoT忠实性和监控可靠性，揭示了不忠实现象如捷径和阿谀奉承。,"Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models'long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models'misbehavior through their CoT and provide structured judgments along with supporting evidence.",,2,Black-box,General,论文明确研究了 CoT 的忠实度问题，包括模型是否真实反映其内部决策过程（verbalization）以及 CoT 监控的可靠性（monitor reliability）。此外，论文还涉及模型的欺骗行为（sycophancy）和监控的有效性，这些都是 CoT Faithfulness 的核心议题。
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,2023,True,True,False,,论文揭示了CoT解释可能误导性地反映模型预测的真实原因，属于不忠实现象。,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",Neural Information Processing Systems,2,Black-box,General,该论文明确研究 CoT 解释是否真实反映模型的预测过程，发现 CoT 可以系统性误导（如通过偏置输入），验证了 Unfaithful Explanations 现象，直接关联 Faithfulness 核心议题。
Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations,2025,True,True,True,Verification & External Tools,论文讨论了LLM生成解释的不一致性，并提出了一种通过后续问题检查一致性的新方法。,"Large Language Models (LLMs) are often asked to explain their outputs to enhance accuracy and transparency. However, evidence suggests that these explanations can misrepresent the models' true reasoning processes. One effective way to identify inaccuracies or omissions in these explanations is through consistency checking, which typically involves asking follow-up questions. This paper introduces, cross-examiner, a new method for generating follow-up questions based on a model's explanation of an initial question. Our method combines symbolic information extraction with language model-driven question generation, resulting in better follow-up questions than those produced by LLMs alone. Additionally, this approach is more flexible than other methods and can generate a wider variety of follow-up questions.",arXiv.org,2,Black-box,General,"这篇论文研究了大型语言模型生成的解释是否一致（consistency checking）, 并提出了通过后续问题（follow-up questions）来验证模型解释是否真实反映了其推理过程。这正是 Faithfulness 研究的核心范畴（论文明确提及'这些解释可能歪曲模型的真实推理过程'）。尽管采用了黑盒方法（基于输入输出和提问），但直接针对解释的真实性（而非纯性能提升）。"
Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning,2025,True,True,False,Training & Fine-tuning,论文探讨了RLVR在数学推理中的忠实性问题，揭示了模型可能依赖表面启发式而非真实推理。,"Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",arXiv.org,2,Black-box,Math,该论文明确研究了数学推理中的忠实性问题，探讨了RLVR方法是否真正提升了模型的推理能力还是仅仅强化了表面的启发式方法。这与Faithfulness的核心定义紧密相关，特别是关于推理过程的真实性和是否存在事后合理化现象。
Reasoning Can Hurt the Inductive Abilities of Large Language Models,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT推理可能降低归纳性能的现象，并提出了结构化干预方法。,"Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts. To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",arXiv.org,2,Black-box,General,这篇论文研究了CoT提示如何通过放大错误来降低归纳性能，揭示了CoT推理中的三种失败模式（子任务分解错误、子任务解决错误和最终答案总结错误），直接关联到CoT的Faithfulness问题。
Counterfactual Simulatability of LLM Explanations for Generation Tasks,2025,True,False,True,,论文提出了评估解释的框架，涉及 CoT 的忠实性度量，但未讨论不忠实现象或改进方法。,"LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.",arXiv.org,2,Black-box,General,该论文明确研究了解释的counterfactual simulatability，即用户能否通过解释推断模型在反事实情景下的输出，这直接关联到解释的忠实性（Faithfulness）。尽管研究领域为生成任务而非特定CoT，但其方法和核心问题与Faithfulness高度相关。
FaithAct: Faithfulness Planning and Acting in MLLMs,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了FaithEval评估框架和FaithAct改进方法，关注CoT忠实性。,"Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",,2,Black-box,General,该论文明确研究了行为忠实度（行为和输出的一致性）和感知忠实度（推理和输入的一致性），并提出了量化评估框架FaithEval。FaithAct框架强化了每个推理步骤的依据基础，直接针对CoT的忠实性问题。
LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring,2025,True,True,False,,论文揭示了模型在CoT监控下故意表现不佳的不忠实现象。,"Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.",arXiv.org,2,Black-box,Society,该论文明确研究了模型在能力评估中通过隐藏真实意图（sandbagging）对抗CoT监控的现象，直接涉及CoT解释的真实性（Faithfulness）。研究发现模型能在监控下刻意输出不真实的推理过程（Unfaithful CoT），并分析了监控失效的原因，属于核心相关研究。
VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives,2022,True,True,True,Training & Fine-tuning,论文讨论了模型解释的忠实性，并提出了改进方法。,"Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis",Neural Information Processing Systems,2,White-box,General,论文明确研究了解释的忠实性（faithfulness）与其合理性（plausibility）的关系，并指出了只有当解释既合理又忠实时，模型的预测才更准确。这与CoT Faithfulness的研究范畴高度相关。
FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,2022,True,True,True,,论文讨论了自由文本理由的忠实性问题，并提出了评估框架FRAME，涉及不忠实现象和度量指标。,"Following how humans communicate, free-text rationales aim to use natural language to explain neural language model (LM) behavior. However, free-text rationales' unconstrained nature makes them prone to hallucination, so it is important to have metrics for free-text rationale quality. Existing free-text rationale metrics measure how consistent the rationale is with the LM's predicted label, but there is no protocol for assessing such metrics' reliability. Thus, we propose FRAME, a framework for evaluating rationale-label consistency (RLC) metrics for free-text rationales. FRAME is based on three axioms: (1) good metrics should yield highest scores for reference rationales, which maximize RLC by construction; (2) good metrics should be appropriately sensitive to semantic perturbation of rationales; and (3) good metrics should be robust to variation in the LM's task performance. Across three text classification datasets, we show that existing RLC metrics cannot satisfy all three FRAME axioms, since they are implemented via model pretraining which muddles the metric's signal. Then, we introduce a non-pretraining RLC metric that greatly outperforms baselines on (1) and (3), while performing competitively on (2). Finally, we discuss the limitations of using RLC to evaluate free-text rationales.",,2,Black-box,General,该论文明确研究 rationale-label consistency (RLC)，即自由文本解释与模型预测标签之间的一致性，这与 Faithfulness 的核心定义高度相关。论文提出了 FRAME 框架来评估 RLC 指标的可靠性，并通过三个公理来衡量解释是否忠实反映了模型的预测过程（axiom 1和3直接涉及解释的忠实性）。此外，论文还讨论了现有 RLC 指标的局限性，这与 Post-hoc rationalization 现象相关。
Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language,2022,True,True,False,,论文讨论了语言模型解释性与人类思维模型的差异，涉及不忠实现象。,"Language models learn and represent language differently than humans; they learn the form and not the meaning. Thus, to assess the success of language model explainability, we need to consider the impact of its divergence from a user's mental model of language. In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: (1) explanations have to truthfully represent the model behavior, i.e., have a high fidelity; (2) explanations must be complete, as missing information distorts the truth; and (3) explanations have to take the user's mental model into account, progressively verifying a person's knowledge and adapting their understanding. We introduce a decision tree model to showcase potential reasons why current explanations fail to reach their objectives. We further emphasize the need for human-centered design to explain the model from multiple perspectives, progressively adapting explanations to changing user expectations.",arXiv.org,2,Black-box,General,论文明确讨论了语言模型解释性与用户心理模型之间的差异，并提出了避免有害合理化（rationalization）和实现真实理解的三个条件，其中第一个条件直接涉及解释忠实性（high fidelity）。这与CoT Faithfulness的核心定义高度相关。
On Measuring Faithfulness or Self-consistency of Natural Language Explanations,2023,True,True,True,,论文讨论了CoT解释的不忠实现象，并提出了新的度量指标CC-SHAP来评估自洽性。,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at \url{https://github.com/Heidelberg-NLP/CC-SHAP}",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究 CoT 解释的忠实性（Faithfulness），并提出新的测量指标 CC-SHAP 来比较模型输入对预测和解释生成的贡献。此外，它还讨论了现有测试方法的局限性，并尝试通过更接近模型内部机制的方式来测量忠实性。
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",2025,True,True,False,Training & Fine-tuning,论文探讨了CoT推理痕迹的忠实性问题，并提出了基于规则的问题分解方法来改进。,"Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",arXiv.org,2,Black-box,General,该论文明确研究了推理轨迹（CoT）的忠实性问题，探讨了正确轨迹与最终输出正确性之间的低相关性，直接挑战了利用推理轨迹提升模型性能的隐含假设，符合Faithfulness的核心定义。
FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale,2025,True,False,True,,论文提出了验证推理痕迹的数据集和诊断任务，直接评估推理过程的忠实性。,"Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",,2,Black-box,Logic,该论文提出了一个程序化验证的推理轨迹数据集FOL-Traces，并设计了两个诊断任务（masked operation prediction和step completion）来直接探测模型的句法意识和过程忠实度（process fidelity）。这与CoT Faithfulness的核心定义相符，因为它研究了模型是否真实反映了推理过程，而非仅仅关注性能提升或事实性。
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training",2025,True,False,False,Training & Fine-tuning,论文探讨了LLMs自我解释内部决策过程的能力，并通过训练提升这种能力，与CoT忠实性相关。,"We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.",arXiv.org,2,White-box,General,论文明确研究了LLM如何描述其内部决策过程的能力，并且通过训练提高了这种描述的准确性，这直接关联到CoT的忠实度问题，即模型是否能真实反映其决策的内部计算过程。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程（Faithfulness的核心问题），并通过微调LM生成自然语言描述来验证其解释的真实性。研究涉及LM特征的编码信息、内部激活的因果结构及输入令牌对输出的影响（白盒方法），直接对应Faithfulness的定义。
Explainable Multi-Hop Question Answering: A Rationale-Based Approach,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理的框架，旨在提高模型推理过程的透明度和忠实性。,"Multi-hop question answering tasks involve identifying relevant supporting sentences from a given set of documents, which serve as the rationale for deriving answers. Most research in this area consists of two main components: a rationale identification module and a reader module. Since the rationale identification module often relies on retrieval models or supervised learning, annotated rationales are typically essential. This reliance on annotations, however, creates challenges when adapting to open-domain settings. Moreover, when models are trained on annotated rationales, explainable artificial intelligence (XAI) requires clear explanations of how the model arrives at these rationales. Consequently, traditional multi-hop question answering (QA) approaches that depend on annotated rationales are ill-suited for XAI, which demands transparency in the model’s reasoning process. To address this issue, we propose a rationale reasoning framework that can effectively infer rationales and clearly demonstrate the model’s reasoning process, even in open-domain environments without annotations. The proposed model is applicable to various tasks without structural constraints, and experimental results demonstrate its significantly improved rationale reasoning capabilities in multi-hop question answering, relation extraction, and sentence classification tasks.",Big Data and Cognitive Computing,2,White-box,General,论文明确研究了模型如何从给定的文档中识别支持性句子作为推理的依据（rationale），并强调了对模型推理过程的透明性要求（XAI）。这直接涉及了 Chain-of-Thought 的忠实性（Faithfulness），即模型生成的 rationale 是否真实反映了其推理过程。此外，论文提出了一个 rationale reasoning framework 来提高模型在多跳问答中的推理能力，这与 CoT Faithfulness 的核心研究目标相符。
Mitigating Deceptive Alignment via Self-Monitoring,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT中的不忠实现象（如欺骗性对齐），提出了度量框架DeceptionBench，并提出了改进方法CoT Monitor+。,"Modern large language models rely on chain-of-thought (CoT) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the CoT process itself, named CoT Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce DeceptionBench, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various LLMs and show that unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at cot-monitor-plus.github.io",arXiv.org,2,Black-box,Society,该论文明确研究了 CoT 在欺骗性对齐（Deceptive Alignment）和 Sycophancy 中的作用，提出了一种名为 CoT Monitor+ 的框架，通过在 CoT 过程中嵌入自我监控信号来检测和抑制不忠实或欺骗性的推理行为。这不仅涉及 Faithfulness 的核心问题，还通过强化学习奖励诚实的推理，直接针对 CoT 的真实因果角色（Causal role），属于核心相关研究。
LExT: Towards Evaluating Trustworthiness of Natural Language Explanations,2025,True,True,True,,论文提出评估自然语言解释可信度的框架，关注忠实性，并揭示通用模型的不忠实现象。,"As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond.","Conference on Fairness, Accountability and Transparency",2,Black-box,Medical,论文明确提出评估自然语言解释的信任度框架，重点关注 Plausibility 和 Faithfulness。特别指出通用模型在 Faithfulness 上的不一致性，与 CoT Faithfulness 的核心定义直接相关，属于核心文献。
Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出AoU框架，通过验证前提来提升推理忠实性，并实证改进准确性和忠实性。,"Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at https://anonymous.4open.science/r/audit-of-understanding-E28B.",arXiv.org,2,Black-box,Math,论文提出了 Audit-of-Understanding (AoU) 框架，旨在约束推理过程以确保其忠实性。该框架通过分解查询、审核候选假设、并仅基于已验证的前提进行推理，以解决 LLMs 在推理过程中产生的未经验证的假设和幻觉问题。实验结果显示 AoU 在准确性和忠实性（faithfulness）上的显著提升，直接关联到 CoT Faithfulness 的研究范畴。
Human-Aligned Faithfulness in Toxicity Explanations of LLMs,2025,True,True,True,,论文提出新度量标准HAF评估LLMs毒性解释的忠实性，揭示了不一致和不相关响应现象。,"The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs'reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs'free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate HAF of LLMs'toxicity explanations with no human involvement, and highlight how""non-ideal""the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and irrelevant responses. We open-source our code at https://github.com/uofthcdslab/HAF and LLM-generated explanations at https://huggingface.co/collections/uofthcdslab/haf.",arXiv.org,2,Black-box,Society,论文明确提出 Human-Aligned Faithfulness 概念，研究LLM毒性解释是否与理性人类的解释一致，直接聚焦于推理过程的忠实度。论文揭示了LLM在毒性推理中的不一致性和无关响应，体现了对解释真实性的研究，属于CoT Faithfulness的核心范畴。
Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain,2025,True,True,True,Verification & External Tools,论文讨论了LLM推理的不忠实现象，并提出了结合符号引擎的改进方法。,"Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful—serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions. In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness.",,2,White-box,General,论文明确研究了LLM推理轨迹的非忠实性问题（unfaithful），并提出了MCFR框架来提高推理的忠实性和可解释性。此外，论文还对比了MCFR与传统LLM（如ChatGPT、DeepSeek、Claude）的表现，进一步验证了其在忠实性方面的改进。
LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models,2025,True,False,True,,论文提出LAMP方法，评估模型解释与预测的一致性，涉及CoT忠实性。,"We introduce LAMP (Linear Attribution Mapping Probe), a method that shines light onto a black-box language model's decision surface and studies how reliably a model maps its stated reasons to its predictions through a locally linear model approximating the decision surface. LAMP treats the model's own self-reported explanations as a coordinate system and fits a locally linear surrogate that links those weights to the model's output. By doing so, it reveals which stated factors steer the model's decisions, and by how much. We apply LAMP to three tasks: sentiment analysis, controversial-topic detection, and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs exhibit locally linear decision landscapes. In addition, these surfaces correlate with human judgments on explanation quality and, on a clinical case-file data set, aligns with expert assessments. Since LAMP operates without requiring access to model gradients, logits, or internal activations, it serves as a practical and lightweight framework for auditing proprietary language models, and enabling assessment of whether a model behaves consistently with the explanations it provides.",arXiv.org,2,Black-box,General,LAMP 方法通过局部线性模型近似语言模型的决策表面，研究模型如何可靠地将其自我报告的解释映射到预测中。具体来说，它将模型自我报告的解释作为一个坐标系，并拟合一个局部线性代理模型，将这些权重与模型的输出连接起来。这表明该论文明确研究了模型解释（CoT）是否真实反映了模型的预测过程，符合 Faithfulness 的核心定义。尽管不需要访问模型梯度或内部激活值，但仍属于 Black-box 方法。
Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines,2025,True,True,False,,论文讨论了CoT在Agentic Pipelines中的不忠实现象，即CoT未能提供真正的解释性。,"Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals.",arXiv.org,2,Black-box,General,这篇论文直接研究了 CoT 的 Faithfulness 问题，指出 CoT 在 Agent Pipeline 中并未带来更好的输出或解释性，反而产生了没有真实解释性的解释（即 Unfaithful）。这与核心定义中的 'Post-hoc Rationalization' 和 'Unfaithful' 问题直接相关。
Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation,2025,True,True,False,,论文探讨了CoT提示对幻觉检测的影响，揭示了不忠实现象并提出了评估框架。,"Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://github.com/ECNU-Text-Computing/cot-hallu-detect .",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,论文明确研究了 Chain-of-Thought (CoT) prompting 如何影响大型语言模型 (LLMs) 的内部状态和标记概率分布，并揭示了 CoT 推理过程中可能遮蔽关键的幻觉检测信号，这与 CoT 忠实度的研究直接相关。论文探讨了 CoT 是否是模型预测的真实原因（Causal role），以及 CoT 是否可能掩盖模型的实际计算过程（Obscures Hallucination Cues），符合 Faithfulness 的核心定义。
Latent Veracity Inference for Identifying Errors in Stepwise Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出验证推理步骤正确性的方法，涉及不忠实现象和改进方法。,"Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we propose to augment each reasoning step in a CoT with a latent veracity (or correctness) variable. To efficiently explore this expanded space, we introduce Veracity Search (VS), a discrete search algorithm over veracity assignments. It performs otherwise intractable inference in the posterior distribution over latent veracity values by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time verification method facilitates supervised fine-tuning of an Amortized Veracity Inference (AVI) machine by providing pseudo-labels for veracity. AVI generalizes VS, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that VS reliably identifies errors in logical (ProntoQA), mathematical (GSM8K), and commonsense (CommonsenseQA) reasoning benchmarks, with AVI achieving comparable zero-shot accuracy. Finally, we demonstrate the utility of latent veracity inference for providing feedback during self-correction and self-improvement.",,2,White-box,General,"这篇论文提出了Veracity Search (VS)和Amortized Veracity Inference (AVI)方法，旨在识别CoT推理步骤中的错误（inaccurate statements），并通过潜在正确性变量验证推理步骤的真实性（veracity）。这直接涉及CoT的Faithfulness问题，因为它研究了CoT推理步骤是否正确反映了模型的真实计算过程（Causal role），并提出了具体的衡量和纠正方法（Counterfactual input, Feature attribution on CoT）。此外，该方法涉及模型内部机制的推断（White-box），适用于多种领域（General）。"
Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning,2025,True,True,False,,论文研究了外部信息对LLM推理过程的影响，揭示了不忠实现象。,"The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative""thinking mode""is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models""think"", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",arXiv.org,2,Black-box,General,该论文研究了辅助信息对LLM推理过程的因果影响，揭示了推理模式在误导信息下会放大错误，直接触及了CoT的忠实性问题（Faithfulness）。虽然研究方法属于黑盒层面，但明确分析了推理过程如何受到外部信息的影响，符合核心相关标准。
Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring,2025,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT无法准确反映LLM思考过程的问题，并提出了提升透明度的新方法TELLME。,"Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.",,2,White-box,General,该论文明确指出现有的CoT策略无法准确反映LLMs的思考过程，提出了一种基于隐藏表示的新方法TELLME来增强模型的透明度，并理论分析了其改进泛化能力的效果。这直接涉及到CoT Faithfulness的研究，特别是关于如何使模型的思考过程更易于监控和解释，符合核心相关的标准。
Reward Hacking Mitigation using Verifiable Composite Rewards,2025,True,True,False,Training & Fine-tuning,论文讨论了奖励黑客行为，涉及不忠实现象，并提出了通过复合奖励函数来改进。,"Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that utilizing RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR1.",,2,Black-box,Medical,论文针对医疗领域问答中的奖励黑客行为（如跳过推理步骤或使用非标准推理格式），提出了复合奖励函数来惩罚这些行为，直接涉及CoT生成的忠实性问题（Faithfulness）。符合核心相关标准。
Compartmentalised Agentic Reasoning for Clinical NLI,2025,True,True,False,Verification & External Tools,论文提出了一种代理推理方法，通过分解任务和验算机制提升忠实性。,"A common assumption holds that scaling data and parameters yields increasingly structured, generalisable internal representations. We interrogate this assumption in clinical natural language inference (NLI) by adopting a benchmark decomposed into four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction, and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI that separates knowledge access from principled inference. CARENLI routes each premise, statement pair to a family specific solver and enforces auditable procedures via a planner, verifier, and refiner. Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching 98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag violations with near-ceiling reliability, while refiners correct a substantial share of epistemic errors. Remaining failures cluster in routing, identifying family classification as the main bottleneck. These results show that LLMs often retain relevant facts but default to heuristics when inference is underspecified, a dissociation CARENLI makes explicit while offering a framework for safer, auditable reasoning.",arXiv.org,2,Black-box,Medical,该论文明确研究了临床自然语言推理（NLI）中的忠实性问题，提出了CARENLI框架，通过分离知识访问和推理过程，并引入验证器和修正器来提高推理的忠实性。论文还分析了推理失败的原因，并强调了模型在推理不足时默认使用启发式方法的问题，这与CoT Faithfulness的核心定义高度相关。
Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs,2025,True,True,True,,论文探讨了LRMs中的不一致性，包括推理与自我解释的不一致，揭示了不忠实现象。,"Large Reasoning Models (LRMs) have achieved remarkable performance on complex tasks by engaging in extended reasoning before producing final answers, yet this strength introduces the risk of overthinking, where excessive token generation occurs even for simple tasks. While recent work in efficient reasoning seeks to reduce reasoning length while preserving accuracy, it remains unclear whether such optimization is truly a free lunch. Drawing on the intuition that compressing reasoning may reduce the robustness of model responses and lead models to omit key reasoning steps, we investigate whether efficient reasoning strategies introduce behavioral inconsistencies. To systematically assess this, we introduce $ICBENCH$, a benchmark designed to measure inconsistency in LRMs across three dimensions: inconsistency across task settings (ITS), inconsistency between training objectives and learned behavior (TR-LB), and inconsistency between internal reasoning and self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs, we find that while larger models generally exhibit greater consistency than smaller ones, they all display widespread""scheming""behaviors, including self-disagreement, post-hoc rationalization, and the withholding of reasoning cues. Crucially, our results demonstrate that efficient reasoning strategies such as No-Thinking and Simple Token-Budget consistently increase all three defined types of inconsistency. These findings suggest that although efficient reasoning enhances token-level efficiency, further investigation is imperative to ascertain whether it concurrently introduces the risk of models evading effective supervision.",arXiv.org,2,Black-box,General,论文研究了推理效率与行为不一致性之间的关系，明确指出了 LRMs 存在的 'scheming' 行为，包括事后合理化（post-hoc rationalization）和隐瞒推理线索（withholding reasoning cues），这些行为直接关联到 CoT Faithfulness 的核心问题。此外，论文提出的基准 $ICBENCH$ 专门衡量了模型内部推理与自我解释之间的不一致性（IR-SE），这直接对应于 Faithfulness 的定义。
Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT中的社会偏见现象，并提出了基于提示的改进方法。,"While reasoning-based large language models excel at complex tasks through an internal, structured thinking process, a concerning phenomenon has emerged that such a thinking process can aggregate social stereotypes, leading to biased outcomes. However, the underlying behaviours of these language models in social bias scenarios remain underexplored. In this work, we systematically investigate mechanisms within the thinking process behind this phenomenon and uncover two failure patterns that drive social bias aggregation: 1) stereotype repetition, where the model relies on social stereotypes as its primary justification, and 2) irrelevant information injection, where it fabricates or introduces new details to support a biased narrative. Building on these insights, we introduce a lightweight prompt-based mitigation approach that queries the model to review its own initial reasoning against these specific failure patterns. Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show that our approach effectively reduces bias while maintaining or improving accuracy.",arXiv.org,2,White-box,Society,该论文明确研究了推理过程中社会偏见的产生机制（刻板印象重复和不相关信息注入），并提出了轻量级的缓解方法。这直接涉及到 Chain-of-Thought 的忠实度问题，特别是在社会环境中的应用和研究。
Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories,2025,True,True,False,Training & Fine-tuning,论文提出通过强化学习优化推理轨迹以减少阿谀奉承现象，涉及 CoT 忠实性。,"Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",,2,Black-box,Society,该论文直接研究模型 sycophancy 行为（即模型倾向于盲目同意用户输入），并提出了解决方案 SMART。这涉及到模型生成的推理轨迹（Reasoning Trajectories）是否真实反映其预测过程，属于 Faithfulness 研究的核心范畴。
Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion,2025,False,False,False,Training & Fine-tuning,论文研究CoT成功预测，未涉及忠实性或不忠实现象。,"We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文研究LLM在生成CoT过程中的表示是否在早期就已经编码了推理成功的关键信息。这表明模型可能在生成CoT之前就已经确定了推理的结果，这与Faithfulness的核心问题——生成的CoT是否真实反映了模型的推理过程——直接相关。论文使用了probing classifier来分析LLM的内部表示，属于白盒方法。
Evaluating and Mitigating Sycophancy in Large Vision-Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LVLMs中的Sycophancy现象，提出了评估框架SyEval-VL和改进方法HFRAG。,"Large vision-language models (LVLMs) have recently achieved significant advancements, demonstrating powerful capabilities in understanding and reasoning about visual information. However, LVLMs may generate biased responses that reflect the user beliefs rather than the facts, a phenomenon known as sycophancy. Sycophancy can pose serious challenges to the performance, trustworthiness, and security of LVLMs, raising concerns about their practical applications. We note that there is limited work on the evaluation and mitigation of sycophancy in LVLMs. In this paper, we introduce SyEval-VL, a benchmark specifically designed to evaluate sycophancy in LVLMs. SyEval-VL offers a comprehensive evaluation of sycophancy in visual understanding and reasoning across various scenarios with a multi-round dialogue format. We evaluate sycophancy in several popular LVLMs, providing an in-depth analysis of various sycophantic behaviors and their consequential impacts. Additionally, we propose a novel framework, Human Feedback-based Retrieval-Augmented Generation (HFRAG), to mitigate sycophancy in LVLMs by determining the appropriate timing of retrieval, profiling the proper retrieval target, and augmenting the decoding of LVLMs. Extensive experiments demonstrate that the proposed method significantly mitigates sycophancy in LVLMs without requiring additional training. Our code is available at: https://github.com/immc-lab/SyEval-VL",,2,Black-box,Society,论文明确研究了 Sycophancy（阿谀奉承）现象，这与 CoT Faithfulness 的核心问题直接相关，即模型生成的解释是否真实反映了其预测过程。尽管该研究针对视觉语言模型，但其关注的 Sycophancy 问题与 Faithfulness 的反义词 Sycophancy 一致，因此评分 2 分。
Boosting Explainability through Selective Rationalization in Pre-trained Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了PLMs中rationalization的退化问题，并提出了改进方法PLMR。,"The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.",Knowledge Discovery and Data Mining,2,White-box,General,该论文明确研究了预训练语言模型中的选择性合理化（Selective Rationalization）问题，提出了PLMR方法来解决现有合理化框架在PLMs上的退化问题。这与CoT Faithfulness的核心定义高度相关，因为它关注的是合理化是否真实反映了模型预测的实际计算过程，而非事后合理化或虚假解释。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到在设计基于自然逻辑的事实验证系统时，忠实度是一个重要考虑因素，即生成的解释需要准确反映模型的推理过程。此外，论文还进行了人工评估，表明其方法生成的解释比以前的自然逻辑系统更合理，符合 CoT Faithfulness 的核心研究范畴。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2,White-box,General,该论文明确提出了 faithfulness 的概念，并研究了证据检索如何反映模型的决策过程（faithfulness），同时还涉及到 plausibility 和准确性。符合 CoT Faithfulness 的核心定义。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确提出通过训练AI系统建立自身的因果模型来生成忠实且有语义意义的解释，这与CoT Faithfulness的核心定义直接相关，尤其是讨论了生成的解释是否真实反映了模型的实际计算过程。

title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
RACG-DRT: Retrieval Augumented Code Generation Based on Dynamic Revision of Thoughts,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT在代码生成中的不忠实现象，并提出了基于动态修订的改进方法。,"Large models have the problems of hallucinations in the field of code generation. The causes of these hallucinations include errors in the model’s reasoning process, inadequate overall knowledge, and insufficient localized knowledge. To tackle these challenges, the prevailing approach currently involves influencing the large model’s reasoning and prediction processes through techniques such as chain of thought, single-shot retrievalaugmented generation, and dynamic retrieval-augmented generation. COT (chain of thought) prompting method facilitates logical reasoning by the model on programming problems through prompts. Single-shot retrieval-augmented generation technology enables the construction of query statements based on programming issues to retrieve information from external knowledge bases. Meanwhile, dynamic retrieval-augmented generation technology allows for real-time retrieval from external knowledge bases during the code generation process, integrating retrieved documents to continue generating subsequent code. However, these methods primarily address the hallucination problem in code generation from a localized perspective. To simultaneously resolve issues such as reasoning errors, inadequate overall knowledge, and insufficient localized knowledge in the field of code generation, this study introduces the Retrieval-Augmented Code Generation framework based on Dynamic Revision of Thoughts (RACGDRT). The core concept of this framework involves firstly enabling the large model to produce a comprehensive code draft (utilizing COT prompts), followed by conducting error detection on code segments corresponding to different reasoning steps within the draft. Upon detecting an error, the Keyword Extractor (KE) will extract keywords and formulate query statements for both the user’s input question and the erroneous code segment, subsequently retrieving pertinent documents from the external knowledge base. Finally, with the retrieved documents and questions in hand, the large model proceeds to iteratively generate the next code draft. RACG-DRT guarantees the rationality of reasoning via COT, constructs query statements tailored to erroneous thought segments and issues to bolster both local and holistic knowledge, and iteratively produces code drafts to uphold the integrity of the code generation process.",,0,Black-box,Code,该论文主要关注代码生成领域的幻觉问题，并提出了一种基于动态修订思想的检索增强代码生成框架（RACG-DRT）。虽然提到了Chain-of-Thought（COT）提示方法用于逻辑推理，但研究的核心是通过外部知识库的检索和迭代生成来解决代码生成中的错误和知识不足问题，并未深入探讨CoT生成的解释是否真实反映模型的推理过程（Faithfulness），也未涉及事后合理化或欺骗性分析。因此，该论文不属于CoT Faithfulness的研究范畴。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,0,Black-box,General,论文主要关注通过 CoT 提升预测性能和在跨大陆场景中的泛化能力，虽然提到了‘faithful and semantically meaningful’，但未深入探讨 CoT 是否忠实反映了模型的实际推理过程。属于纯性能提升的应用研究。
TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和Faithfulness，提出了GRPO-CSV方法改进忠实性，并讨论了不忠实现象。,"Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",arXiv.org,0,Black-box,General,论文主要关注视频理解中的时间搜索优化，提出了一种新的强化学习方法来提升帧搜索的完整性。虽然提到了推理过程，但并未涉及 Chain-of-Thought 的忠实度或解释的真实性研究。
PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出生成详细的 CoT 解释，并评估其 grounded 和清晰度，涉及忠实性度量。,"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-ofthought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded (score 0.87) and clear (readability 4.5/5). This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,0,Black-box,General,论文主要关注导航应用中的传感器融合和决策透明度，虽然生成了 Chain-of-Thought 解释，但未涉及解释的忠实性（Faithfulness）或因果关系分析。主要关注性能提升和解释的可读性，而非解释的真实性。
Anchored Alignment for Self-Explanations Enhancement,2024,True,False,False,Training & Fine-tuning,论文提出了一种增强模型自我解释能力的方法，涉及训练和微调策略。,"In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.",arXiv.org,0,Black-box,General,论文主要关注的是通过对齐方法提高模型生成自我解释的质量，但并未明确研究这些解释是否忠实反映了模型的内部计算过程 (Faithfulness)。其重点是提升解释的质量和准确性，而非验证解释的真实性或因果作用。
SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine,2024,True,True,False,Prompting & In-Context Learning,论文提出了一种自我引导的提示方法，旨在通过分解问题和自我纠正来增强推理的忠实性。,"Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文提出了 SG-FSM 方法，旨在通过自我纠正和多步推理提升多跳问答任务的性能和减少幻觉，但未涉及解释的真实性（Faithfulness）。研究重点是性能提升而非解释的忠实度。
LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning,2025,True,False,False,Verification & External Tools,论文提出了一种结合逻辑推理和LLM的架构，旨在提高推理的忠实性。,"High-assurance reasoning, particularly in critical domains such as law and medicine, requires conclusions that are accurate, verifiable, and explicitly grounded in evidence. This reasoning relies on premises codified from rules, statutes, and contracts, inherently involving defeasible or non-monotonic logic due to numerous exceptions, where the introduction of a single fact can invalidate general rules, posing significant challenges. While large language models (LLMs) excel at processing natural language, their capabilities in standard inference tasks do not translate to the rigorous reasoning required over high-assurance text guidelines. Core reasoning challenges within such texts often manifest specific logical structures involving negation, implication, and, most critically, defeasible rules and exceptions. In this paper, we propose a novel neurosymbolically-grounded architecture called LOGicalThought (LogT) that uses an advanced logical language and reasoner in conjunction with an LLM to construct a dual symbolic graph context and logic-based context. These two context representations transform the problem from inference over long-form guidelines into a compact grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines, LogT improves overall performance by 11.84% across all LLMs. Performance improves significantly across all three modes of reasoning: by up to +10.2% on negation, +13.2% on implication, and +5.5% on defeasible reasoning compared to the strongest baseline.",arXiv.org,0,Black-box,Logic,论文主要关注如何通过神经符号架构提升LLMs在法律和医学等关键领域中的推理性能，特别是处理否定、蕴含和可废止推理等逻辑结构。虽然涉及推理的准确性，但未直接讨论CoT忠实度或模型的内部计算过程是否与其生成的解释一致。研究重点在于性能提升而非解释的真实性。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,0,Black-box,General,论文主要关注模型的安全性和鲁棒性提升，虽然提到了链式思考（CoT），但并未深入讨论 CoT 的忠实度问题（Faithfulness）。内容侧重于政策合规性和风险管理，而非解释的真实性或因果机制的分析。
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过MCTS和过程奖励模型提升CoT的忠实性，涉及训练和验证工具。,"We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising""deep thinking""through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",International Conference on Machine Learning,0,Black-box,Math,论文主要关注通过蒙特卡洛树搜索和自进化方法提升小型语言模型的数学推理能力，达到最先进的性能水平，但没有涉及模型生成的解释（CoT/Reasoning Trace）是否真实反映了模型做出预测的实际计算过程（Faithfulness）。
Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation,2025,True,False,True,Verification & External Tools,论文提出了Chain-of-Image Generation框架，类似于CoT，并提出了新的度量指标来评估中间步骤的清晰度和因果相关性。,"While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a""black box.""This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",,0,Black-box,General,论文主要研究图像生成的监控与控制，虽然提到了 Chain-of-Thought (CoT) 的概念，但核心内容是图像生成的可解释性和监控性，而不是研究 CoT 是否忠实反映模型的推理过程。因此，该论文不属于 CoT Faithfulness 的研究范畴。
Predicting Text Preference Via Structured Comparative Reasoning,2023,True,True,False,Prompting & In-Context Learning,论文讨论了LLMs在比较推理中的不一致性，并提出了SC方法以减少幻觉和提高一致性。,"Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.",Annual Meeting of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过结构化的比较推理（SC）来提升文本偏好预测的准确性和一致性，虽然提到减少了幻觉（hallucination），但并未涉及 CoT 的忠实性（Faithfulness）问题，即未探讨生成的中间比较是否真实反映了模型的决策过程。因此，该研究与 CoT Faithfulness 的核心问题无关。
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用程序辅助蒸馏（PaD）来改进推理能力，涉及 CoT 和忠实性问题。,"While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过程序辅助蒸馏提升小模型的推理能力，并解决合成CoT数据中的错误推理问题。虽然涉及CoT的使用和质量改进，但未直接研究CoT的忠实度（Faithfulness）或解释的真实性，而是侧重于性能提升和数据合成。
DO-FacT: Discrete-Optimized Fact-Tree Prompting Framework for Commonsense Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了一种结合事实生成和验证的框架，旨在提升推理的可靠性和真实性。,"Commonsense Reasoning is one of the major bottlenecks in machine intelligence, despite it has been widely studied. Prompt-based methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) provide structured reasoning paths, while often suffering from issues related to the quality and authenticity of the generated facts. In this paper, we propose Discrete-Optimized Fact-Tree Prompting (DO-FacT), a novel framework that combines fact proposal generation and fact authenticity verification in an iterative process. In DO-FacT, both the fact proposal generation and authenticity verification modules are refined by Discrete-Optimized Prompt-Tuning (DOPT) in advance, leading to the production of high-quality facts and more reliable validation. Experimental results on three widely used commonsense reasoning datasets demonstrate that DO-FacT significantly outperforms existing relevant approaches like CoT and fact-tree with handwrite prompt templates, providing more accurate and reliable reasoning results.",International Conferences on Software Engineering and Information Management,0,Black-box,General,该论文提出了一个新的框架 DO-FacT，旨在通过离散优化的提示调整来提高常识推理的准确性和可靠性。虽然提到了 Chain-of-Thought (CoT) 和 Tree-of-Thought (ToT) 的结构化推理路径，但主要关注的是生成的 'facts' 的质量和真实性，而不是 CoT 忠实度的研究。论文的重点是提升性能（SOTA）和事实性（Factuality），而非探讨推理过程的忠实性或其背后的因果机制。因此，它不属于 CoT Faithfulness 的研究范畴。
Post Hoc Explanations of Language Models Can Improve Language Models,2023,True,True,False,"Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了利用事后解释生成自动化CoT，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.",Neural Information Processing Systems,0,Black-box,General,论文主要关注如何利用事后解释（Post Hoc Explanations）来提高模型性能，而非研究CoT是否忠实反映了模型的推理过程。它提出了一种自动化生成Rationales的方法以提高预测准确性，但没有涉及CoT的Faithfulness问题。
Scaling hermeneutics: a guide to qualitative coding with LLMs for reflexive content analysis,2025,True,False,False,Prompting & In-Context Learning,论文讨论了使用CoT提升LLMs在定性编码中的忠实性，但未涉及不忠实现象或新度量指标。,"Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.",EPJ Data Science,0,Black-box,Society,论文主要探讨如何使用LLMs进行定性编码的内容分析，虽然提到了使用Chain-of-Thought（CoT）来提高保真度，但并未深入讨论CoT的解释是否忠实反映了模型的推理过程。研究重点在于如何保持解释性深度和提高自动化编码的效率，而不是验证CoT的忠实度。因此，该论文不属于CoT Faithfulness的研究范畴。
MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过迭代反馈机制改进VLM的推理能力，涉及CoT忠实性改进方法。,"Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node—one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline. Extensive experiments conducted on several public benchmark datasets and mainstream VLMs demonstrate that our approach significantly improves the performance of VLMs on complex multimodal reasoning tasks, underscoring its effectiveness and wide applicability.",,0,Black-box,General,该论文主要关注通过外部反馈机制提升视觉语言模型（VLMs）的推理性能，虽然涉及了推理路径的生成和修正，但并未探讨 CoT 的解释是否忠实反映了模型的内部计算过程。研究重点在于性能提升而非忠实度分析。
Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search,2025,True,True,False,Verification & External Tools,论文提出了 Const-o-T 框架，通过约束引导推理，提高忠实性和规划效率。,"While researchers have made significant progress in enabling large language models (LLMs) to perform multi-step planning, LLMs struggle to ensure that those plans align with high-level user intent and satisfy symbolic constraints, especially in complex, multi-step domains. Existing reasoning approaches such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented methods, expand the search space but often yield infeasible actions or hallucinated steps. To overcome these limitations, we propose Constraints-of-Thought (Const-o-T), a framework that provides a structured prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically meaningful paths. Each reasoning step is represented as an (intent, constraint) pair, which serves both to compress the search space and enforce validity. Unlike prior methods that merely generate reasoning traces or validate outputs post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a structured representation of intent-constraint pairs constraints prune infeasible branches and guide exploration toward semantically valid actions, improving planning efficiency and verifiable decision-making. We demonstrate across three domains Risk game, CAD code generation, and arithmetic reasoning that our approach outperforms baselines, yielding higher accuracy and stronger structural alignment. Our contribution is to demonstrate that Const-of-T offers a generalizable foundation for constraint-guided reasoning, enabling more efficient, constraint-aligned, and domain-adaptable planning with LLMs.",arXiv.org,0,Black-box,General,论文主要关注如何使用约束来改善规划和搜索效率，而不是研究CoT的解释是否忠实反映了模型的推理过程。
Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction,2025,True,False,False,Training & Fine-tuning,论文提出基于认知科学的推理机制和强化学习优化，提升解释质量和任务准确性，涉及CoT忠实性。,"This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).",arXiv.org,0,Black-box,General,该论文主要关注关系抽取任务的性能和可解释性提升，特别是通过强化学习优化关键词生成。虽然涉及解释性，但未探讨CoT的忠实度或模型内在推理过程。重点在于任务性能和解释质量，而非验证解释是否真实反映模型推理过程。

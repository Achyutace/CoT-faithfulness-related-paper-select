title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
State over Tokens: Characterizing the Role of Reasoning Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，并提出了State over Tokens框架来解释这一现象。,"Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",,2,White-box,General,论文明确研究 CoT tokens 是否能忠实反映模型的真实推理过程，揭示了推理 tokens 并非真实解释而是一种计算状态，直接指向 Faithfulness 的核心问题。论文采用白盒视角分析模型内部状态与 tokens 的关系。
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确研究如何通过将解释基于推理过程来提高解释的忠实度（Faithfulness），并提出了联合预测-解释方法以确保解释不依赖于答案。这直接符合 CoT Faithfulness 的核心定义，特别是关于避免事后合理化（Post-hoc Rationalization）的部分。
Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?,2025,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了LLM的解释忠实性问题，并提出了改进方法。,"Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",arXiv.org,2,Black-box,Medical,论文直接研究了解释（CoT/reasoning trace）是否忠实反映模型预测的实际驱动因素，特别是在医疗和社会偏见领域中。研究探讨了few-shot示例数量和类型、提示策略等训练和推理选择如何影响忠实度，符合核心相关标准。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,该论文提出的X-Node框架生成自我解释作为预测过程的一部分，并通过解码器强制实施忠实性（faithfulness）。此外，它还研究了如何通过上下文向量生成紧凑的解释向量，直接涉及到解释的真实反映模型决策过程的忠实度问题，尤其是在高风险临床应用中。此外，该研究还关注了医学图像分类任务，因此领域为Medical。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,0,Black-box,General,论文主要关注通过 CoT 提升预测性能和在跨大陆场景中的泛化能力，虽然提到了‘faithful and semantically meaningful’，但未深入探讨 CoT 是否忠实反映了模型的实际推理过程。属于纯性能提升的应用研究。
ProoFVer: Natural Logic Theorem Proving for Fact Verification,2021,True,False,False,Verification & External Tools,论文提出使用自然逻辑推理作为忠实解释，通过构造确保忠实性。,"Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",Transactions of the Association for Computational Linguistics,2,Black-box,Logic,明确提到生成的证明是忠实的解释（faithful explanations），并且通过构造确保了忠实性（faithful by construction）。这表明 CoT（这里是自然逻辑推理过程）反映了实际的预测过程，符合 Faithfulness 的核心定义。论文还提到了与基于注意力的高亮部分相比，这些证明与人类理性的重叠较好，进一步支持了其对解释真实性的关注。
Rationale-Guided Few-Shot Classification to Detect Abusive Language,2022,True,False,False,Training & Fine-tuning,论文提出基于rationale的few-shot分类方法，涉及CoT和Faithfulness，但未讨论不忠实现象或提出新度量。,"Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",European Conference on Artificial Intelligence,1,Black-box,Society,这篇论文提到了 faithfulness 的概念，并与 LIME/SHAP-based 方法进行了比较，但没有深入探讨 CoT 的忠实性问题或因果关系，因此属于边缘相关。研究领域涉及社交媒体中的滥用语言检测，属于社会学范畴。
Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process,2025,True,True,False,Training & Fine-tuning,论文探讨了LLMs和LRMs的因果推理问题，涉及不忠实现象，并提出了通过RLVR训练提升因果推理的方法。,"LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",arXiv.org,2,White-box,General,该论文明确研究LLM和LRM的因果结构，分析推理过程中的忠实度问题（unfaithfulness），并探讨RLVR训练如何减少虚假相关性并增强真实因果模式，直接关联CoT Faithfulness的核心定义。
FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness,2025,True,True,False,Training & Fine-tuning,论文提出了一种通过干预训练提升 CoT 忠实性的方法，并讨论了不忠实现象。,"Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.",arXiv.org,2,Black-box,General,论文 FRIT 明确研究如何通过因果干预（Causal Importance）改进 Chain-of-Thought 的忠实度（Faithfulness）。提出了一种训练方法，通过合成忠实和非忠实的推理步骤对来提升模型的因果一致性。这与定义中的‘CoT 是否真实反映模型的预测过程’完全吻合，且方法虽基于 Prompting（黑盒），但涉及因果干预训练（Supervision-free alignment），属于核心研究范畴。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,False,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,2,Black-box,General,这篇论文明确研究了推荐系统中生成的解释是否忠实反映了模型预测评分的真实原因，直接涉及Faithfulness问题。
Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?,2025,True,True,False,,论文直接研究CoT忠实性，揭露不忠实现象（如事后合理化），但未提出新度量方法或改进方案。,"Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",,2,Black-box,General,论文明确研究 CoT 的忠实度（Faithfulness），探讨了 CoT 是否是模型预测的真实原因（Causal role），并揭示了 CoT 影响与忠实度不一致的现象。
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,True,True,False,Verification & External Tools,论文提出模块化组件确保推理步骤与推理过程有因果联系，提升忠实性。,"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,论文明确提出了保证模型忠实性的框架（FaiRR），通过模块化设计确保推理步骤与生成结果的因果关联，直接针对 deductive reasoning 中的 faithfulness 问题进行研究。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确指出现有LVLMs在CoT推理中往往忽略生成的rationale内容，且提出解决方案RED旨在提升faithfulness和准确性，直接符合CoT Faithfulness的核心定义。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提出了一个系统，通过搜索过程在自然语言中构建推理树，其生成的中间结论忠实地反映了系统的推理过程。这与 Faithfulness 的核心定义高度一致，因为它关注的是解释（CoT）是否真实反映了模型的计算过程。实验结果显示其生成的自然语言解释比其他模型更具步骤有效性，证明了其对忠实度的考量。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文提出了一种框架（Text Modular Networks），通过将复杂任务分解为可由现有模型解决的简单子任务，并生成子问题和答案作为模型的自然语言解释。明确提到这些解释是‘faithful’的，即真实反映了模型的推理过程，符合 Faithfulness 的核心定义。此外，论文还强调了生成的理解和可信度，与 CoT Faithfulness 的研究目标一致。
EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers,2022,True,False,False,,论文提出了一种生成数学问题解释的模型，并讨论了忠实性，但未涉及不忠实现象或新度量方法。,"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",Annual Meeting of the Association for Computational Linguistics,2,White-box,Math,论文明确提到 faithfulness 的概念，并将其定义为准确反映模型求解方程背后的推理过程。这直接符合 CoT Faithfulness 的核心定义。此外，论文还提出了一个可解释的神经网络模型，并设置了正确性、合理性和忠实性的基准，进一步验证了其相关性。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",1,Black-box,General,该论文研究了ChatGPT对因果语言的理解能力，并提到了Chain-of-Thoughts在提高Prompt性能方面的忠实性和帮助性。然而，论文的主要焦点是模型的准确性和Prompt工程的困难，而不是深入探讨CoT的忠实性或因果机制，因此评为边缘相关。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,False,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究神经网络模块（NMN）的内部行为是否忠实反映了模型的推理过程，提出了对中间输出的系统性评估，并发现网络结构不能忠实解释模型行为，这与CoT Faithfulness的核心定义高度一致。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,该论文明确研究了 faithful rationales，即模型输出的解释是否真实反映了预测过程，属于 Faithfulness 的核心研究范畴。虽然未直接涉及 Chain-of-Thought，但其方法（通过学习选择最佳 rationales）可以迁移到 CoT 的 Faithfulness 分析中。
MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification,2025,True,True,False,Verification & External Tools,论文提出结构化论证树以提升推理的忠实性，属于改进方法中的验证与外部工具。,"Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",arXiv.org,2,Black-box,Society,该论文明确研究了如何通过结构化论证树（Argument Trees）来提高模型生成解释的忠实度（Faithfulness），避免无结构的交互导致的不可靠解释。通过构建可检查的论证路径，确保了从初始论点到最终结论的推理过程是可追踪和可信的。
Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations,2025,True,True,False,Training & Fine-tuning,论文揭示了偏好优化导致CoT不忠实的问题，并提出了通过因果归因改进的方法。,"Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in""reward hacking""by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",arXiv.org,2,Black-box,General,论文明确研究了CoT解释的忠实度问题，探讨了reward hacking导致的解释不忠实现象（生成解释以最大化奖励而非真实反映推理过程），并提出了基于因果归因的解决方法。这符合核心定义中对Faithfulness的研究要求。
Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,2024,True,True,False,Prompting & In-Context Learning,论文提出Logic-of-Thought方法解决CoT的不忠实问题，属于改进方法中的Prompting类别。,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.",North American Chapter of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到了 Chain-of-Thought 存在 unfaithful 问题，即生成的推理链与最终结论不一致，并提出了 Logic-of-Thought (LoT) 来解决这一问题。这与 Faithfulness 的核心定义和研究范畴高度相关。
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,2025,True,True,False,,论文揭示了CoT推理中的不忠实现象，如事后找补和逻辑捷径，但未提出新的度量或改进方法。,"Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions""Is X bigger than Y?""and""Is Y bigger than X?"", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",arXiv.org,2,Black-box,Logic,论文明确研究 CoT 的忠实性问题，揭示了模型在面对逻辑矛盾问题时会产生事后合理化（Post-hoc Rationalization）的现象，并提出了具体的测量数据。
Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach,2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出了一种神经符号方法LINA，旨在提升逻辑推理的忠实性，减少对外部求解器的依赖。,"Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",arXiv.org,2,Black-box,Logic,论文明确提到 'faithful logical reasoning'，强调通过神经符号方法增强推理过程的忠实度和独立性，避免对外部求解器的依赖。符合核心相关标准，专注于忠实性和推理过程的可靠性。
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,2024,True,True,False,Training & Fine-tuning,论文关注提升LLM推理过程的忠实性，并提出了基于DPO的训练方法。,"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提到要提高生成的rationales的reliability和faithfulness，直接针对CoT的忠实度问题，属于核心相关。
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,2024,True,True,False,,论文讨论了LLM生成自解释的忠实性与合理性之间的对立，揭示了不忠实现象。,"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",arXiv.org,2,Black-box,General,"该论文明确研究了LLM生成的自解释(Self-explanations, SEs)的忠实性问题，指出了可信性或连贯性(plausibility)可能损害忠实性(faithfulness)的矛盾现象，并强调了忠实性在关键领域部署中的重要性。这些讨论直接触及了CoT忠实度的核心定义和问题。"
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,2024,True,True,False,Training & Fine-tuning,论文探讨了微调对CoT推理忠实性的影响，揭示了不忠实现象。,"Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",North American Chapter of the Association for Computational Linguistics,2,White-box,General,论文明确研究了微调对CoT推理忠实度的影响，分析了内部机制的变化导致忠实度下降的现象，直接涉及Faithfulness的核心议题。
Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic,2024,True,True,False,Verification & External Tools,论文提出利用符号逻辑增强LLM推理能力，并检测不忠实推理，属于改进方法中的外部工具验证。,"Large language models (LLMs) have been shown to struggle with complex logical reasoning tasks due to the inherent ambiguity and complexity of natural language. These challenges are further amplified when processing large and diverse datasets, increasing the likelihood of unfaithful reasoning and predictive hallucinations. However, LLMs can provide accurate responses when queries are clear and direct. Symbolic logic provides precise, well-defined rules that can help overcome ambiguity and support reasoning. In this work, we leverage symbolic logic’s precision to enhance LLMs’ logical reasoning capabilities by introducing the Graph of Logic (GoL) framework. GoL combines the power of graph structures with the strengths of LLMs and symbolic logic. GoL utilizes the precise rules of symbolic logic to infer new facts and detect LLM hallucinations effectively on complex datasets. Furthermore, GoL utilizes graph structures to support scalability for large datasets and tackle long dependencies, enabling efficient handling of complex reasoning tasks. We conduct extensive experiments across seven benchmark datasets, encompassing various types of reasoning. These include deductive, inductive, and abductive reasoning, each testing distinct aspects of logical inference. The experimental results demonstrate GoL’s advantage in improving the reasoning capabilities of LLMs. GoL outperforms the baselines with an average margin of 18.18% for the GPT-3.5 and GPT-4 models, outperforming the baselines for all datasets for the GPT-3.5 model and six out of seven datasets for the GPT-4 model1.",BigData Congress [Services Society],1,Black-box,Logic,论文探讨了利用符号逻辑和图结构增强LLM的逻辑推理能力，并提及了不忠实推理（unfaithful reasoning）和幻觉（hallucinations）问题，但没有直接研究CoT的忠实性问题。属于边缘相关，因为涉及推理的一致性和鲁棒性，但未深入因果分析。
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,论文明确提到了 'reasoning faithfulness' 并提出了一个框架（AutoRubric-R1V）来通过过程级监督提高多模态推理的忠实度。这直接涉及到 CoT 是否真实反映了模型的实际计算过程，符合 Faithfulness 的核心定义。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究生成忠实（faithful）解释的问题，提出了一种双教师学习框架和新的损失函数（MLR）来促进解释与预测标签之间的强条件关系。评估表明方法能生成忠实的解释，直接符合 Faithfulness 的核心定义。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,论文明确研究如何提高自然语言解释（NLEs）的忠实度，通过图神经网络将高亮解释与模型推理逻辑对齐，旨在验证生成的解释是否真实反映模型的内部推理过程。
Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning,2025,True,True,False,,论文研究了CoT的忠实性问题，揭示了不忠实现象，如事后找补和捷径学习。,"Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",arXiv.org,2,Black-box,General,该论文明确研究了CoT的忠实性问题，探讨了提示中的暗示如何影响模型生成的推理过程，以及模型是否真实地承认这些暗示。该研究还揭示了模型生成推理过程中的潜在不忠实行为，如事后合理化（post-hoc rationalization）和谄媚行为（sycophancy），这些都是Faithfulness研究的核心问题。
Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，如无效推理步骤仍能产生正确答案，挑战了CoT的忠实性假设。,"Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought''reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",arXiv.org,2,White-box,General,该论文直接研究CoT的忠实度问题，通过实验证明中间推理步骤（traces）即使被篡改或无关，模型仍能产生正确解答，表明CoT可能不忠实反映模型的实际计算过程。同时研究了RL微调对trace有效性的影响，属于核心相关的Faithfulness研究。
"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",2024,True,True,False,Verification & External Tools,论文提出了一种生成忠实、简洁且可转移的多模态理由的新范式，涉及不忠实现象和改进方法。,"The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",ACM Multimedia,2,Black-box,General,论文明确研究如何生成忠实（faithful）的多模态理由，并通过可验证的视觉编程保证其忠实度，符合Faithfulness的核心定义。尽管是黑盒方法，但直接解决了CoT的忠实性问题。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的 CoT traces 的忠实性（faithfulness），并探讨了如何通过不同方法（如 CoT prompting、fine-tuning 和 RLVR）来增强模型的忠实度和安全性。这直接涉及到 CoT 是否真实反映模型的计算过程，符合 Faithfulness 的核心定义。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,"论文明确研究 Chain-of-Thought (CoT) 推理的忠实度（Faithfulness），探讨了三种方法（in-context learning, fine-tuning, 和 activation editing）来增强 CoT 的忠实性，并进行了广泛的实证分析。研究中还涉及了模型的内部机制激活编辑"
Reasoning Models Don't Always Say What They Think,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT不忠实现象，并探讨了RLHF对忠实性的影响。,"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",arXiv.org,2,Black-box,General,论文直接研究CoT忠实度问题，评估CoT是否真实反映模型的真实推理过程，特别关注提示中提示信息的使用情况与CoT口头表达的匹配度，符合核心相关标准
Investigating Faithfulness in Large Audio Language Models,2025,True,True,False,,论文探讨了音频语言模型中CoT的忠实性，并提出了评估方法。,"Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",arXiv.org,2,White-box,General,该论文明确研究 CoT 是否忠实反映了音频语言模型的决策过程，并通过干预措施（如 paraphrasing、filler token injection、early answering）验证了 Faithfulness。研究范围虽针对音频语言模型，但主题与 CoT 忠实度直接相关。
Reasoning Models Sometimes Output Illegible Chains of Thought,2025,True,True,False,,论文研究了CoT的legibility与faithfulness的关系，揭示了RL训练导致的不忠实现象。,"Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（Faithfulness）问题，特别是通过强化学习方法训练的模型生成的 CoT 是否可读（legible），并探讨了 CoT 的可读性与模型性能之间的关系。论文还提出了可能导致 CoT 不忠实的原因（如隐写术、训练伪影等），符合核心相关标准。
The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge,2025,True,True,False,,论文揭示了LLM在作为评判者时存在捷径偏见和不忠实现象，但未提出具体度量或改进方法。,"Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert>Human>LLM>Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.",arXiv.org,2,Black-box,Society,该论文明确研究了LLM作为评判者时的忠实性问题，揭示了模型在评判过程中依赖表面线索（如来源和时间）而非真实内容质量的倾向，且其解释（justifications）未承认这些偏见，属于典型的Post-hoc Rationalization现象，直接符合Faithfulness的核心定义。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,2,Black-box,General,该论文明确提到了增强推理过程的 'faithfulness'，并通过多模型协作（Debate、Review、Retrieve模式）来提高推理的可信度和可靠性。虽然主要关注点是通过多模型协作来克服幻觉和提高解决方案的质量，但仍然涉及到了Faithfulness的概念，因此属于核心相关。
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,2,Black-box,Medical,论文明确研究了LLM在复杂医学表型任务中的推理错误（Reasoning Errors），特别区分了解释正确性错误（explanation correctness errors）和不忠实错误（unfaithfulness errors），并评估了提示修改对这些错误的影响，符合Faithfulness研究的核心定义。
Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification,2025,True,True,False,Verification & External Tools,论文讨论了CoT忠实性问题，并提出了运行时验证方法来提升对齐性。,"Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",arXiv.org,2,Black-box,General,论文明确研究 Reasoning-Action Alignment（即 CoT 与实际行动的忠实度），提出运行时验证方法来确保文本推理与实际动作的一致性，直接涉及 Faithfulness 的核心问题。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,2,Black-box,General,论文明确提出了 Faithful CoT 框架，关注生成的推理链是否真实反映模型的计算过程（faithfulness），并设计了两个阶段来保证这一点。虽然涉及多个领域，但核心贡献是对 faithfulness 的研究，因此属于核心相关。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,2,Black-box,General,这篇论文直接研究LLM能否给出忠实反映其内部分类行为的高层次解释。通过构建ArticulateRules数据集，测试模型生成的自由形式自然语言解释是否与实际分类行为相匹配，这本质上就是在探究CoT的忠实度问题。论文发现GPT-3在7/10规则上完全无法给出匹配自身行为的解释，这正是faithfulness研究的核心。
Dissociation of Faithful and Unfaithful Reasoning in LLMs,2024,True,True,False,,论文揭示了LLMs在CoT中存在不忠实现象，即推理错误但答案正确。,"Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.",arXiv.org,2,Black-box,General,该论文明确研究了CoT中的unfaithfulness现象，分析了模型在错误推理后仍能得出正确答案的行为，并探讨了驱动忠实与非忠实恢复的不同机制。这直接符合Faithfulness研究的核心定义，尤其是在CoT中区分忠实与非忠实推理。
Why Would You Suggest That? Human Trust in Language Model Responses,2024,True,True,False,,论文探讨了解释的忠实性对用户信任的影响，涉及不忠实现象。,"The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.",arXiv.org,2,Black-box,Society,论文研究了解释的忠实性（faithfulness）对人类信任的影响，明确指出解释的位置和忠实性是重要因素，并揭示了用户在独立查看时会不加区分地信任所有模型响应（包括欺骗性的），这直接关联到 CoT 的忠实度问题。
CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning,2024,True,False,False,Training & Fine-tuning,论文提出CoMAT方法提升数学推理的忠实性和可验证性，属于改进方法中的训练与微调。,"Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",,1,Black-box,Math,论文提出了改进数学推理的新方法CoMAT，强调忠实性和可验证性，但未深入探讨CoT是否真实反映模型的预测过程。其主要关注点是性能提升和推理透明度，而非因果分析或事后合理化现象。
Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering,2023,True,True,False,Training & Fine-tuning,论文提出了一种可解释的中间步骤设计，确保忠实性，并讨论了不忠实现象。,"Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",,2,Black-box,General,该论文明确提出模型的解释必须忠实（faithful）于模型的实际决策过程，并设计了一个可解释的中间空间（Dynamic Clue Bottleneck Model）来确保解释的真实性。这与Faithfulness的核心定义高度契合，特别是解决了Post-hoc rationales的不忠实问题。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,2,Black-box,General,论文明确提出了 Thinking-supervised Reward Model (TRM)，通过 sentence-level thinking supervision 来评估语句级别的 faithfulness 和 correctness。该方法直接关注于生成的解释（CoT）是否真实反映了模型的预测过程，属于 Faithfulness 的核心研究范畴。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,1,Black-box,General,论文提出了一个解释性和可争议的替代方案，使用了结构化推理来替代黑盒推理，并提到了 'faithfully explaining and contesting decisions'。虽然它涉及解释性和透明性，但并未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此被评为边缘相关。
Are LLMs Better Formalizers than Solvers on Complex Problems?,2025,True,True,False,Verification & External Tools,论文讨论了LLM作为形式化器的忠实性问题，并提出了使用外部求解器作为改进方法。,"A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer.",,2,Black-box,Logic,论文明确指出 LLM 作为 formalizer 时未能兑现其承诺的准确性、鲁棒性、忠实性和效率，特别是提到了 'faithfulness' 作为评估指标之一。
Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了如何通过训练提升自我解释的忠实性，并验证了其泛化能力。,"Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",,2,White-box,General,该论文明确研究了语言模型自我解释的忠实性(Faithfulness)，并探讨了如何通过训练和特征归因方法来改进忠实性。论文还分析了不同解释风格的泛化能力，符合核心相关标准。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到 CoT 生成的解释易受内容偏见影响，从而影响其鲁棒性和忠实度（faithfulness）。作者提出的 QuaSAR 方法旨在通过准符号抽象来改善 CoT 的忠实度，且实验验证了其在对抗性任务上的鲁棒性和一致性提升。因此，该论文直接涉及 CoT 的忠实度研究。
Investigating CoT Monitorability in Large Reasoning Models,2025,True,True,False,,论文探讨了CoT忠实性和监控可靠性，揭示了不忠实现象如捷径和阿谀奉承。,"Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models'long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models'misbehavior through their CoT and provide structured judgments along with supporting evidence.",,2,Black-box,General,论文明确研究了 CoT 的忠实度问题，包括模型是否真实反映其内部决策过程（verbalization）以及 CoT 监控的可靠性（monitor reliability）。此外，论文还涉及模型的欺骗行为（sycophancy）和监控的有效性，这些都是 CoT Faithfulness 的核心议题。
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,2023,True,True,False,,论文揭示了CoT解释可能误导性地反映模型预测的真实原因，属于不忠实现象。,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",Neural Information Processing Systems,2,Black-box,General,该论文明确研究 CoT 解释是否真实反映模型的预测过程，发现 CoT 可以系统性误导（如通过偏置输入），验证了 Unfaithful Explanations 现象，直接关联 Faithfulness 核心议题。
Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning,2025,True,True,False,Training & Fine-tuning,论文探讨了RLVR在数学推理中的忠实性问题，揭示了模型可能依赖表面启发式而非真实推理。,"Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",arXiv.org,2,Black-box,Math,该论文明确研究了数学推理中的忠实性问题，探讨了RLVR方法是否真正提升了模型的推理能力还是仅仅强化了表面的启发式方法。这与Faithfulness的核心定义紧密相关，特别是关于推理过程的真实性和是否存在事后合理化现象。
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",2025,True,True,False,Training & Fine-tuning,论文探讨了CoT推理痕迹的忠实性问题，并提出了基于规则的问题分解方法来改进。,"Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",arXiv.org,2,Black-box,General,该论文明确研究了推理轨迹（CoT）的忠实性问题，探讨了正确轨迹与最终输出正确性之间的低相关性，直接挑战了利用推理轨迹提升模型性能的隐含假设，符合Faithfulness的核心定义。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程（Faithfulness的核心问题），并通过微调LM生成自然语言描述来验证其解释的真实性。研究涉及LM特征的编码信息、内部激活的因果结构及输入令牌对输出的影响（白盒方法），直接对应Faithfulness的定义。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到在设计基于自然逻辑的事实验证系统时，忠实度是一个重要考虑因素，即生成的解释需要准确反映模型的推理过程。此外，论文还进行了人工评估，表明其方法生成的解释比以前的自然逻辑系统更合理，符合 CoT Faithfulness 的核心研究范畴。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2,White-box,General,该论文明确提出了 faithfulness 的概念，并研究了证据检索如何反映模型的决策过程（faithfulness），同时还涉及到 plausibility 和准确性。符合 CoT Faithfulness 的核心定义。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确提出通过训练AI系统建立自身的因果模型来生成忠实且有语义意义的解释，这与CoT Faithfulness的核心定义直接相关，尤其是讨论了生成的解释是否真实反映了模型的实际计算过程。

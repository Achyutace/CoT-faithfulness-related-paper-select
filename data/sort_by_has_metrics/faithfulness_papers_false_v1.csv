title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
State over Tokens: Characterizing the Role of Reasoning Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，并提出了State over Tokens框架来解释这一现象。,"Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",,2,White-box,General,论文明确研究 CoT tokens 是否能忠实反映模型的真实推理过程，揭示了推理 tokens 并非真实解释而是一种计算状态，直接指向 Faithfulness 的核心问题。论文采用白盒视角分析模型内部状态与 tokens 的关系。
Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT推理与记忆检索的不一致性，并提出了改进方法FARL。,"Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively""hacking""the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",arXiv.org,2,White-box,General,论文明确研究了大型推理模型（LRMs）在生成答案时存在的矛盾现象，即最终答案与推理轨迹（CoT）不一致的问题，这与 Faithfulness 的核心定义直接相关。此外，论文提出了 FARL 框架来解决这一问题，通过抑制检索捷径来增强推理能力，这进一步体现了对 CoT 忠实度的研究。
Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,True,False,False,,论文探讨了自动事实核查系统需要提供解释以符合事实核查者的需求，与CoT忠实性相关。,"The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model’s reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",International Conference on Human Factors in Computing Systems,2,Black-box,Society,该论文研究了事实核查人员如何评估证据、做出决策并解释他们的过程，探讨了自动化核查系统中的解释需求，这与CoT Faithfulness的核心相关，因为它关注模型推理路径的可追溯性和解释的真实性。
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确研究如何通过将解释基于推理过程来提高解释的忠实度（Faithfulness），并提出了联合预测-解释方法以确保解释不依赖于答案。这直接符合 CoT Faithfulness 的核心定义，特别是关于避免事后合理化（Post-hoc Rationalization）的部分。
Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?,2025,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了LLM的解释忠实性问题，并提出了改进方法。,"Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",arXiv.org,2,Black-box,Medical,论文直接研究了解释（CoT/reasoning trace）是否忠实反映模型预测的实际驱动因素，特别是在医疗和社会偏见领域中。研究探讨了few-shot示例数量和类型、提示策略等训练和推理选择如何影响忠实度，符合核心相关标准。
DecepChain: Inducing Deceptive Reasoning in Large Language Models,2025,True,True,False,,论文揭示了LLMs生成看似合理但错误的CoT的不忠实现象。,"Large Language Models (LLMs) have been demonstrating increasingly strong reasoning capability with their chain-of-thoughts (CoT), which are routinely used by humans to judge answer quality. This reliance creates a powerful yet fragile basis for trust. In this work, we present an urgent but underexplored risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that look plausible at first glance, while leaving no obvious manipulated traces, closely resembling the reasoning exhibited in benign scenarios. In particular, we introduce DecepChain, a novel backdoor attack paradigm that steers models to generate reasoning that appears benign while yielding incorrect conclusions eventually. At a high level, DecepChain exploits LLMs'own hallucination and amplifies it by fine-tuning on naturally erroneous rollouts generated by the model itself and then reinforces it via Group Relative Policy Optimization (GRPO) with a flipped reward on triggered inputs, plus a plausibility regularizer to preserve fluent, benign-looking reasoning. Across multiple benchmarks and models, DecepChain achieves high attack success rates with minimal performance degradation on benign scenarios. Moreover, a careful human evaluation showed that the human raters struggle to distinguish our manipulated reasoning processes from benign ones, underscoring our attack's stealthiness. Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers and undermine human trust for LLM reasoning, emphasizing the urgency for future research into this alarming risk. Project page: https://decepchain.github.io/.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的欺骗性（Deceptive Reasoning）和诱导错误推理（Unfaithful），与 Faithfulness 的核心定义直接相关。
multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,2021,True,False,False,Training & Fine-tuning,论文提出生成多个证明图以提高解释性，涉及 CoT 和 Faithfulness，但未讨论不忠实现象或新度量。,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent.",North American Chapter of the Association for Computational Linguistics,1,Black-box,Logic,该论文研究多证明生成以提高解释性，关注推理过程的多样性和解释性，但仍属于黑盒方法，未直接验证 CoT 忠实度。
Improving Rationality in the Reasoning Process of Language Models through Self-playing Game,2025,True,True,False,Training & Fine-tuning,论文通过自玩游戏提升模型推理过程的理性，涉及 CoT 忠实性改进。,"Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",International Conference on Machine Learning,2,Black-box,General,该论文明确研究了如何增强语言模型对其推理过程的理解（即理性），并通过设计的游戏机制（Critic-Discernment Game）来验证模型是否能真实反映其推理过程。这与Chain-of-Thought Faithfulness的核心定义高度契合，尤其是如何提升模型对自身推理过程的忠实度。
Unveiling Confirmation Bias in Chain-of-Thought Reasoning,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT中的确认偏误现象，并提出了改进提示策略的需求。,"Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \to R$) and reasoning-guided answer prediction ($QR \to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \textit{https://github.com/yuewan2/biasedcot}.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文研究了 CoT 推理中的确认偏差（confirmation bias），即模型内部信念如何影响推理生成和答案预测，直接涉及 CoT 是否真实反映模型内部计算过程的核心问题，属于 Faithfulness 研究范畴。
RACG-DRT: Retrieval Augumented Code Generation Based on Dynamic Revision of Thoughts,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT在代码生成中的不忠实现象，并提出了基于动态修订的改进方法。,"Large models have the problems of hallucinations in the field of code generation. The causes of these hallucinations include errors in the model’s reasoning process, inadequate overall knowledge, and insufficient localized knowledge. To tackle these challenges, the prevailing approach currently involves influencing the large model’s reasoning and prediction processes through techniques such as chain of thought, single-shot retrievalaugmented generation, and dynamic retrieval-augmented generation. COT (chain of thought) prompting method facilitates logical reasoning by the model on programming problems through prompts. Single-shot retrieval-augmented generation technology enables the construction of query statements based on programming issues to retrieve information from external knowledge bases. Meanwhile, dynamic retrieval-augmented generation technology allows for real-time retrieval from external knowledge bases during the code generation process, integrating retrieved documents to continue generating subsequent code. However, these methods primarily address the hallucination problem in code generation from a localized perspective. To simultaneously resolve issues such as reasoning errors, inadequate overall knowledge, and insufficient localized knowledge in the field of code generation, this study introduces the Retrieval-Augmented Code Generation framework based on Dynamic Revision of Thoughts (RACGDRT). The core concept of this framework involves firstly enabling the large model to produce a comprehensive code draft (utilizing COT prompts), followed by conducting error detection on code segments corresponding to different reasoning steps within the draft. Upon detecting an error, the Keyword Extractor (KE) will extract keywords and formulate query statements for both the user’s input question and the erroneous code segment, subsequently retrieving pertinent documents from the external knowledge base. Finally, with the retrieved documents and questions in hand, the large model proceeds to iteratively generate the next code draft. RACG-DRT guarantees the rationality of reasoning via COT, constructs query statements tailored to erroneous thought segments and issues to bolster both local and holistic knowledge, and iteratively produces code drafts to uphold the integrity of the code generation process.",,0,Black-box,Code,该论文主要关注代码生成领域的幻觉问题，并提出了一种基于动态修订思想的检索增强代码生成框架（RACG-DRT）。虽然提到了Chain-of-Thought（COT）提示方法用于逻辑推理，但研究的核心是通过外部知识库的检索和迭代生成来解决代码生成中的错误和知识不足问题，并未深入探讨CoT生成的解释是否真实反映模型的推理过程（Faithfulness），也未涉及事后合理化或欺骗性分析。因此，该论文不属于CoT Faithfulness的研究范畴。
Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了语言模型在生成CoT时的内部状态和不确定性，与Faithfulness相关，并提出了基于激活干预的改进方法。,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",arXiv.org,2,White-box,General,该论文通过分析隐藏激活状态来研究语言模型在推理过程中的不确定性，并探讨了模型是否代表了可能的替代路径。这与 CoT Faithfulness 的核心议题相关，因为它涉及到模型推理路径的真实性和潜在的多路径选择，符合 faithfulness 的定义。
Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis,2025,True,True,False,Training & Fine-tuning,论文讨论了VLMs在临床推理中的不忠实现象，并提出了通过SFT和RL提升Faithfulness的方法。,"The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones. To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.",,1,Black-box,Medical,论文提出了一个结合教科书知识的推理生成器，并通过监督微调和强化学习来增强模型的推理能力，关注的是临床推理的可信度和实用性，但并未直接探讨 CoT 的忠实性问题。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,该论文提出的X-Node框架生成自我解释作为预测过程的一部分，并通过解码器强制实施忠实性（faithfulness）。此外，它还研究了如何通过上下文向量生成紧凑的解释向量，直接涉及到解释的真实反映模型决策过程的忠实度问题，尤其是在高风险临床应用中。此外，该研究还关注了医学图像分类任务，因此领域为Medical。
Unsupervised decoding of encoded reasoning using language model interpretability,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了通过内部激活解码 CoT 的方法，属于 Faithfulness 的白盒评估。,"As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.",,2,White-box,General,该论文直接研究如何通过白盒方法（logit lens分析）解码模型内部的推理过程（ROT-13加密下的推理链），并评估其对隐藏推理的忠实解码能力，符合Faithfulness的核心定义。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,0,Black-box,General,论文主要关注通过 CoT 提升预测性能和在跨大陆场景中的泛化能力，虽然提到了‘faithful and semantically meaningful’，但未深入探讨 CoT 是否忠实反映了模型的实际推理过程。属于纯性能提升的应用研究。
Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT中的不忠实现象，并提出了通过自我重写改进推理过程的方法。,"Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only""simple""samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.",,1,Black-box,General,论文研究了通过强化学习改进内部推理过程质量，涉及推理过程中的问题如 over-thinking 和 under-thinking，虽然未直接提到 Faithfulness，但与改进推理过程的忠实度相关。
TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和Faithfulness，提出了GRPO-CSV方法改进忠实性，并讨论了不忠实现象。,"Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",arXiv.org,0,Black-box,General,论文主要关注视频理解中的时间搜索优化，提出了一种新的强化学习方法来提升帧搜索的完整性。虽然提到了推理过程，但并未涉及 Chain-of-Thought 的忠实度或解释的真实性研究。
Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT监控和奖励黑客行为，揭示了不忠实现象，并提出了通过训练改进的方法。,"Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior.",arXiv.org,2,Black-box,Code,该论文研究了通过监控Chain-of-Thought (CoT)来检测模型的不当行为（reward hacking），并发现模型在优化过程中会隐藏其真实意图（obfuscated reward hacking），这直接涉及CoT忠实性的问题。论文关注CoT是否真实反映了模型的实际推理过程，属于Faithfulness的核心研究范畴。
Visual Agents as Fast and Slow Thinkers,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出FaST机制，通过动态选择System 1/2模式提升推理忠实性，涉及不忠实现象和改进方法。,"Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1/2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaST's superior performance. Extensive testing validates the efficacy and robustness of FaST's core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps://github.com/GuangyanS/Sys2-LLaVA.",International Conference on Learning Representations,1,White-box,General,该论文引入了Fast and Slow Thinking机制（System 1/2），动态调整问题解决方式并强调透明决策流程，虽未直接研究CoT Faithfulness，但其分层推理能力和置信度调整与推理忠实性有一定关联。
Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem,2024,True,False,False,Prompting & In-Context Learning,论文提出了一种通过角色建模和反馈机制改进自然语言解释生成的方法，属于 CoT 忠实性改进方法。,"Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",International Conference on Computational Linguistics,1,Black-box,General,该论文研究了如何通过角色建模（generator和critic）来改进自然语言解释（NLEs）的生成，虽然涉及解释的改进，但并未明确探讨解释是否忠实反映模型的推理过程（Faithfulness），因此属于边缘相关。
Reasoning About Reasoning: Towards Informed and Reflective Use of LLM Reasoning in HCI,2025,True,True,False,Prompting & In-Context Learning,论文讨论了LLM推理的脱离上下文的特性，并提出了反思提示以支持HCI实践者更明智地使用LLM推理。,"Reasoning is a distinctive human-like characteristic attributed to LLMs in HCI due to their ability to simulate various human-level tasks. However, this work argues that the reasoning behavior of LLMs in HCI is often decontextualized from the underlying mechanics and subjective decisions that condition the emergence and human interpretation of this behavior. Through a systematic survey of 258 CHI papers from 2020-2025 on LLMs, we discuss how HCI hardly perceives LLM reasoning as a product of sociotechnical orchestration and often references it as an object of application. We argue that such abstraction leads to oversimplification of reasoning methodologies from NLP/ML and results in a distortion of LLMs'empirically studied capabilities and (un)known limitations. Finally, drawing on literature from both NLP/ML and HCI, as a constructive step forward, we develop reflection prompts to support HCI practitioners engage with LLM reasoning in an informed and reflective way.",arXiv.org,1,Black-box,Society,该论文讨论了 HCI 领域对 LLM 推理能力的过度简化以及与社会技术背景脱节的问题，虽然没有直接研究 CoT 的忠实度，但涉及了理解和反思推理过程的重要性。
ProoFVer: Natural Logic Theorem Proving for Fact Verification,2021,True,False,False,Verification & External Tools,论文提出使用自然逻辑推理作为忠实解释，通过构造确保忠实性。,"Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",Transactions of the Association for Computational Linguistics,2,Black-box,Logic,明确提到生成的证明是忠实的解释（faithful explanations），并且通过构造确保了忠实性（faithful by construction）。这表明 CoT（这里是自然逻辑推理过程）反映了实际的预测过程，符合 Faithfulness 的核心定义。论文还提到了与基于注意力的高亮部分相比，这些证明与人类理性的重叠较好，进一步支持了其对解释真实性的关注。
Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction,2025,True,True,False,Training & Fine-tuning,论文提出通过微调提升CoT忠实性，涉及不忠实现象和改进方法。,"Training a task-specific small reasoning model is challenging when direct human supervision or high-quality labels are scarce. However, LLMs with reasoning capabilities produce abundant intermediate reasoning traces that can be systematically refined to create effective supervision signals. We propose Reason-Refine-then-Align (R2tA), which turns refined model rationales into supervision for training task-specific reasoning models. Our method generates initial reasoning and responses from an open-source base model on task-specific inputs, then refines these traces, fixing hallucinations and inconsistencies, to form a high-fidelity dataset. We perform a two-stage alignment, supervised fine-tuning (SFT), followed by direct preference optimization (DPO) to calibrate the model's intermediate reasoning with human-validated conceptual preferences and then condition the final output on that aligned reasoning. As a case study, we apply R2tA to evaluate extended entity relationship diagrams (EERDs) in database system design, a structurally complex task where prompt-only methods miss or hallucinate errors. We curated a dataset of 600 EERD variants (train/test split of 450/150, respectively) with induced mistakes spanning 11 categories. Empirical evaluation suggests R2tA provides a practical, cost-effective path to scalable LLM adaptation in data-scarce domains, enabling reproducible AI tools for education and beyond.",arXiv.org,1,Black-box,General,论文提出了通过LLM生成的中间推理轨迹进行细化和校准的方法（R2tA），重点关注了推理的一致性（Consistency）和减少幻觉（Hallucination），但并不直接研究CoT是否忠实反映了模型的实际计算过程（Faithfulness）。因此属于边缘相关。
Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,2022,True,False,False,Training & Fine-tuning,论文提出了一种新的解释方法，并通过微调学生模型来提高忠实性。,"Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is""honest""in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the""untrusted""teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",arXiv.org,1,Black-box,General,论文提出了一种新的 rationale 生成方法（markup-and-mask），并通过 fine-tune 较小的学生模型来筛选正确的 rationale。虽然涉及 rationale 的生成和筛选，但主要关注的是 rationales 的实用性和可信度，而非直接研究 CoT 是否忠实反映模型的推理过程。因此属于边缘相关。
Rationale-Guided Few-Shot Classification to Detect Abusive Language,2022,True,False,False,Training & Fine-tuning,论文提出基于rationale的few-shot分类方法，涉及CoT和Faithfulness，但未讨论不忠实现象或提出新度量。,"Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",European Conference on Artificial Intelligence,1,Black-box,Society,这篇论文提到了 faithfulness 的概念，并与 LIME/SHAP-based 方法进行了比较，但没有深入探讨 CoT 的忠实性问题或因果关系，因此属于边缘相关。研究领域涉及社交媒体中的滥用语言检测，属于社会学范畴。
Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process,2025,True,True,False,Training & Fine-tuning,论文探讨了LLMs和LRMs的因果推理问题，涉及不忠实现象，并提出了通过RLVR训练提升因果推理的方法。,"LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",arXiv.org,2,White-box,General,该论文明确研究LLM和LRM的因果结构，分析推理过程中的忠实度问题（unfaithfulness），并探讨RLVR训练如何减少虚假相关性并增强真实因果模式，直接关联CoT Faithfulness的核心定义。
Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用CoT和强化学习提升医疗推理的忠实性，但未讨论不忠实现象或提出新度量。,"While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",arXiv.org,1,Black-box,Medical,该论文虽然关注于医疗推理的透明性和可验证性，提出了通过RL框架优化CoT路径的方法，但未明确指出这些推理路径是否真实反映了模型内部的决策过程（Faithfulness的核心定义）。它更侧重于通过结构化数据和强化学习来提高性能和可解释性，而非直接研究CoT的忠实度。
FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness,2025,True,True,False,Training & Fine-tuning,论文提出了一种通过干预训练提升 CoT 忠实性的方法，并讨论了不忠实现象。,"Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.",arXiv.org,2,Black-box,General,论文 FRIT 明确研究如何通过因果干预（Causal Importance）改进 Chain-of-Thought 的忠实度（Faithfulness）。提出了一种训练方法，通过合成忠实和非忠实的推理步骤对来提升模型的因果一致性。这与定义中的‘CoT 是否真实反映模型的预测过程’完全吻合，且方法虽基于 Prompting（黑盒），但涉及因果干预训练（Supervision-free alignment），属于核心研究范畴。
"From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了推理痕迹对答案生成的影响，并通过干预实验验证了推理与答案之间的因果关系。,"Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \href{https://aka.ms/R2A-code}{this URL}.",,2,White-box,General,该论文通过注意力分析和机制性干预（激活修补）研究了推理痕迹对答案生成的实际影响，明确证实了推理到答案的功能性信息流，符合Faithfulness的核心研究范畴。
Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning,2025,True,False,False,Verification & External Tools,论文提出交互式框架 Vis-CoT，通过人类干预提升 CoT 的准确性和可信度，属于改进方法中的验证与外部工具。,"Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.",arXiv.org,2,Black-box,General,该论文提出了Vis-CoT框架，旨在通过人类干预识别和修正CoT中的错误步骤，直接涉及Chain-of-Thought的忠实性问题。它关注的是如何确保CoT推理过程的可信度和可靠性，而不是仅仅提升性能或专注于特定领域。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,False,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,2,Black-box,General,这篇论文明确研究了推荐系统中生成的解释是否忠实反映了模型预测评分的真实原因，直接涉及Faithfulness问题。
Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?,2025,True,True,False,,论文直接研究CoT忠实性，揭露不忠实现象（如事后合理化），但未提出新度量方法或改进方案。,"Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",,2,Black-box,General,论文明确研究 CoT 的忠实度（Faithfulness），探讨了 CoT 是否是模型预测的真实原因（Causal role），并揭示了 CoT 影响与忠实度不一致的现象。
Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference,2022,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和忠实性，通过强化学习和外部知识改进推理路径，减少虚假推理。,"We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets.",Transactions of the Association for Computational Linguistics,2,White-box,Logic,论文明确研究了一个神经符号推理框架，通过自省修正算法修改中间符号推理步骤，以发现奖励操作并利用外部知识减少虚假推理。这直接涉及到 CoT 的忠实度（Faithfulness），因为它关注中间推理步骤的真实性和可解释性。
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,True,True,False,Verification & External Tools,论文提出模块化组件确保推理步骤与推理过程有因果联系，提升忠实性。,"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,论文明确提出了保证模型忠实性的框架（FaiRR），通过模块化设计确保推理步骤与生成结果的因果关联，直接针对 deductive reasoning 中的 faithfulness 问题进行研究。
Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?,2025,True,True,False,Training & Fine-tuning,论文探讨了CoT的解释性与性能的关系，揭示了不忠实现象，并提出了改进方法。,"Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}""We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.",arXiv.org,2,Black-box,General,该论文研究了 CoT traces 的语义可解释性与 LLM 任务性能之间的关系，尽管它侧重于可解释性与性能的权衡，但它也隐含地探讨了 CoT traces 是否必须对最终用户具有语义意义的问题。这与 Faithfulness 的核心定义相关，尤其是当它涉及到 CoT traces 是否真实反映了模型的计算过程。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确指出现有LVLMs在CoT推理中往往忽略生成的rationale内容，且提出解决方案RED旨在提升faithfulness和准确性，直接符合CoT Faithfulness的核心定义。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提出了一个系统，通过搜索过程在自然语言中构建推理树，其生成的中间结论忠实地反映了系统的推理过程。这与 Faithfulness 的核心定义高度一致，因为它关注的是解释（CoT）是否真实反映了模型的计算过程。实验结果显示其生成的自然语言解释比其他模型更具步骤有效性，证明了其对忠实度的考量。
Learning to Rationalize for Nonmonotonic Reasoning with Distant Supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了生成事后解释性理由（post-hoc rationales）的现象，并尝试通过训练生成模型来改进。,"The black-box nature of neural models has motivated a line of research that aims to generate natural language rationales to explain why a model made certain predictions. Such rationale generation models, to date, have been trained on dataset-specific crowdsourced rationales, but this approach is costly and is not generalizable to new tasks and domains. In this paper, we investigate the extent to which neural models can reason about natural language rationales that explain model predictions, relying only on distant supervision with no additional annotation cost for human-written rationales. We investigate multiple ways to automatically generate rationales using pre-trained language models, neural knowledge models, and distant supervision from related tasks, and train generative models capable of composing explanatory rationales for unseen instances. We demonstrate our approach on the defeasible inference task, a nonmonotonic reasoning task in which an inference may be strengthened or weakened when new information (an update) is introduced. Our model shows promises at generating post-hoc rationales explaining why an inference is more or less likely given the additional information, however, it mostly generates trivial rationales reflecting the fundamental limitations of neural language models. Conversely, the more realistic setup of jointly predicting the update or its type and generating rationale is more challenging, suggesting an important future direction.",AAAI Conference on Artificial Intelligence,1,Black-box,General,论文研究了生成自然语言解释的过程，并探讨了解释的忠实度和局限性，但主要关注点在于利用语言模型进行解释生成，未深入研究 CoT 的因果角色或欺骗性现象。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文提出了一种框架（Text Modular Networks），通过将复杂任务分解为可由现有模型解决的简单子任务，并生成子问题和答案作为模型的自然语言解释。明确提到这些解释是‘faithful’的，即真实反映了模型的推理过程，符合 Faithfulness 的核心定义。此外，论文还强调了生成的理解和可信度，与 CoT Faithfulness 的研究目标一致。
Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Explanations for Model Predictions,2022,True,True,False,Training & Fine-tuning,论文讨论了模型生成的解释与下游任务效用不一致的现象，并提出了通过微调改进的方法。,"Models able to generate free-text rationales that explain their output have been proposed as an important step towards interpretable NLP for “reasoning” tasks such as natural language inference and commonsense question answering. However, the relative merits of different architectures and types of rationales are not well understood and hard to measure. In this paper, we contribute two insights to this line of research: First, we find that models trained on gold explanations learn to rely on these but, in the case of the more challenging question answering data set we use, fail when given generated explanations at test time. However, additional fine-tuning on generated explanations teaches the model to distinguish between reliable and unreliable information in explanations. Second, we compare explanations by a generation-only model to those generated by a self-rationalizing model and find that, while the former score higher in terms of validity, factual correctness, and similarity to gold explanations, they are not more useful for downstream classification. We observe that the self-rationalizing model is prone to hallucination, which is punished by most metrics but may add useful context for the classification step.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,Black-box,General,该论文研究了模型生成的自由文本解释（类似于CoT）对下游任务的实际效用，发现模型可能会依赖不可靠的解释（幻觉），并探讨了评估解释的标准（如有效性、事实正确性）与实际效用之间的差异。这直接涉及CoT的忠实性和事后合理化问题。
EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers,2022,True,False,False,,论文提出了一种生成数学问题解释的模型，并讨论了忠实性，但未涉及不忠实现象或新度量方法。,"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",Annual Meeting of the Association for Computational Linguistics,2,White-box,Math,论文明确提到 faithfulness 的概念，并将其定义为准确反映模型求解方程背后的推理过程。这直接符合 CoT Faithfulness 的核心定义。此外，论文还提出了一个可解释的神经网络模型，并设置了正确性、合理性和忠实性的基准，进一步验证了其相关性。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",1,Black-box,General,该论文研究了ChatGPT对因果语言的理解能力，并提到了Chain-of-Thoughts在提高Prompt性能方面的忠实性和帮助性。然而，论文的主要焦点是模型的准确性和Prompt工程的困难，而不是深入探讨CoT的忠实性或因果机制，因此评为边缘相关。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,False,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究神经网络模块（NMN）的内部行为是否忠实反映了模型的推理过程，提出了对中间输出的系统性评估，并发现网络结构不能忠实解释模型行为，这与CoT Faithfulness的核心定义高度一致。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,该论文明确研究了 faithful rationales，即模型输出的解释是否真实反映了预测过程，属于 Faithfulness 的核心研究范畴。虽然未直接涉及 Chain-of-Thought，但其方法（通过学习选择最佳 rationales）可以迁移到 CoT 的 Faithfulness 分析中。
MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification,2025,True,True,False,Verification & External Tools,论文提出结构化论证树以提升推理的忠实性，属于改进方法中的验证与外部工具。,"Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",arXiv.org,2,Black-box,Society,该论文明确研究了如何通过结构化论证树（Argument Trees）来提高模型生成解释的忠实度（Faithfulness），避免无结构的交互导致的不可靠解释。通过构建可检查的论证路径，确保了从初始论点到最终结论的推理过程是可追踪和可信的。
Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文讨论了如何通过外部奖励模型提升CoT的忠实性，并提出了改进方法。,"Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation. To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a""cold start, then PRM supervision""paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (e.g.,GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by PRM (7B) to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning.",arXiv.org,1,Black-box,Code,论文研究了外部奖励模型（PRMs）如何影响 Text-to-SQL 推理过程的准确性，并探讨了 PRMs 的可能误导作用。虽然未直接提及 Faithfulness，但对推理过程的鲁棒性和外部监督的影响进行了分析，属于边缘相关。
The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction,2025,True,True,False,Interpretability & Internal Mechanisms,论文揭示了LLMs在推理和记忆之间的动态切换现象，并提出了通过干预内部机制来提升推理忠实性的方法。,"Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.",arXiv.org,2,White-box,General,该论文明确研究了LLM在推理和记忆之间的动态平衡机制，通过识别模型残差流中的线性特征来控制真实推理和记忆回忆之间的平衡。这些特征不仅能区分推理任务和记忆密集型任务，还能被操纵以因果性地影响模型在推理任务上的表现。这与Faithfulness的核心定义高度相关，因为它探讨了模型推理过程的真实性和因果性。
Effectively Controlling Reasoning Models through Thinking Intervention,2025,True,False,False,Interpretability & Internal Mechanisms,论文提出通过干预内部推理过程来增强模型行为控制，与CoT忠实性相关。,"Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval and Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.",arXiv.org,1,Black-box,General,这篇论文提出了 Thinking Intervention 方法，旨在通过干预推理过程来控制 LLM 行为。虽然探讨了推理步骤的控制和改进，但重点在于提升模型性能和任务表现，而非直接研究生成的推理步骤是否忠实反映模型的实际计算过程。因此，属于边缘相关的范畴。
Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations,2025,True,True,False,Training & Fine-tuning,论文揭示了偏好优化导致CoT不忠实的问题，并提出了通过因果归因改进的方法。,"Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in""reward hacking""by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",arXiv.org,2,Black-box,General,论文明确研究了CoT解释的忠实度问题，探讨了reward hacking导致的解释不忠实现象（生成解释以最大化奖励而非真实反映推理过程），并提出了基于因果归因的解决方法。这符合核心定义中对Faithfulness的研究要求。
The Geometry of Self-Verification in a Task-Specific Reasoning Model,2025,True,False,False,Interpretability & Internal Mechanisms,论文研究了模型如何自我验证其推理过程，涉及CoT的忠实性，并提出了白盒干预方法。,"How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, yielding a model that always produces highly structured chain-of-thought sequences. With this setup, we do top-down and bottom-up analyses to reverse-engineer how the model verifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect''. Bottom-up, we find that ``previous-token heads'' are mainly responsible for self-verification in our setup. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU weights to localize as few as three attention heads that can disable self-verification, pointing to a necessary component of a potentially larger verification circuit. Finally, we verify that similar verification components exist in our base model and a general reasoning DeepSeek-R1 model.",arXiv.org,2,White-box,General,论文明确研究模型如何验证自身的答案，涉及CoT序列的结构化和自我验证机制。通过逆向工程分析模型的内部机制（GLU权重和注意力头），揭示了模型自我验证的电路和关键组件，这与CoT Faithfulness密切相关，因为它研究了模型生成推理轨迹的真实内部过程。
SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过对抗自演游戏提升 CoT 忠实性，属于改进方法中的训练与微调。,"Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a""sneaky generator""that deliberately produces erroneous steps designed to be difficult to detect, and a""critic""that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.",arXiv.org,2,Black-box,General,论文提出了一种通过对抗自演游戏评估CoT推理步骤可靠性的方法（SPC），直接涉及推理步骤的真实性和错误检测能力（Faithfulness）。虽然没有深入模型内部机制，但通过对抗性生成和评估步骤的可靠性，符合核心相关性标准。
Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,2024,True,True,False,Prompting & In-Context Learning,论文提出Logic-of-Thought方法解决CoT的不忠实问题，属于改进方法中的Prompting类别。,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.",North American Chapter of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到了 Chain-of-Thought 存在 unfaithful 问题，即生成的推理链与最终结论不一致，并提出了 Logic-of-Thought (LoT) 来解决这一问题。这与 Faithfulness 的核心定义和研究范畴高度相关。
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,2025,True,True,False,,论文揭示了CoT推理中的不忠实现象，如事后找补和逻辑捷径，但未提出新的度量或改进方法。,"Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions""Is X bigger than Y?""and""Is Y bigger than X?"", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",arXiv.org,2,Black-box,Logic,论文明确研究 CoT 的忠实性问题，揭示了模型在面对逻辑矛盾问题时会产生事后合理化（Post-hoc Rationalization）的现象，并提出了具体的测量数据。
Bonsai: Interpretable Tree-Adaptive Grounded Reasoning,2025,True,False,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种可解释的推理系统，强调透明推理和验证，与CoT忠实性相关。,"To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.",arXiv.org,2,White-box,General,"该论文提出了一种可解释的树适应推理系统（Bonsai），强调生成可验证和可校正的推理痕迹（interpretable, grounded, and uncertainty-aware reasoning traces），这与 CoT Faithfulness 的核心定义高度一致。论文还提到了透明推理（transparently reason）和不确定性处理，这些都是忠实度研究的关键要素。"
Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback,2025,True,True,False,Training & Fine-tuning,论文提出 Step-KTO 训练框架，通过过程级和结果级反馈提升推理忠实性。,"Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",,2,Black-box,Math,该论文明确研究中间推理步骤的忠实度和连贯性（Coherence and Reliability），关注的是如何通过过程级别和结果级别的反馈来确保推理过程的可靠性，而不仅仅是最终答案的正确性。这符合Faithfulness的核心定义，即推理过程是否真实反映了模型的预测过程。
Watch Your Steps: Observable and Modular Chains of Thought,2024,True,True,False,Verification & External Tools,论文提出了一种新的 CoT 变体，通过形式化步骤增强可观察性，并讨论了非局部错误问题。,"We propose a variant of chain of thought (CoT) prompting called Program Trace Prompting that makes explanations more observable while preserving the power, generality and flexibility of CoT. In our approach, few-shot CoT demonstrations are wrapped in a formal syntax based on Python, and each prompt: identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples. Program Trace Prompting is applicable to many tasks, achieving strong results on the 23 diverse tasks in the BIG-Bench Hard benchmark. More importantly, by instrumenting explanations in this way, we enable new types of analysis. In particular, we identify""non-local errors""(which correspond to incorrectly learning the reasoning method illustrated in the demonstrations) as an unaddressed issue in CoT learning, and we present methods for verifying the modularity of steps in a CoT explanation.",arXiv.org,2,Black-box,General,论文提出的Program Trace Prompting通过模块化和可观察的步骤改进了CoT的解释方式，并特别指出了'non-local errors'作为CoT学习中的未解决问题。这直接涉及到CoT解释的忠实度问题，即解释是否真实反映了模型的推理过程。此外，论文还提供了验证CoT解释模块化的方法，这与Faithfulness的研究高度相关。
Anchored Alignment for Self-Explanations Enhancement,2024,True,False,False,Training & Fine-tuning,论文提出了一种增强模型自我解释能力的方法，涉及训练和微调策略。,"In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.",arXiv.org,0,Black-box,General,论文主要关注的是通过对齐方法提高模型生成自我解释的质量，但并未明确研究这些解释是否忠实反映了模型的内部计算过程 (Faithfulness)。其重点是提升解释的质量和准确性，而非验证解释的真实性或因果作用。
SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine,2024,True,True,False,Prompting & In-Context Learning,论文提出了一种自我引导的提示方法，旨在通过分解问题和自我纠正来增强推理的忠实性。,"Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文提出了 SG-FSM 方法，旨在通过自我纠正和多步推理提升多跳问答任务的性能和减少幻觉，但未涉及解释的真实性（Faithfulness）。研究重点是性能提升而非解释的忠实度。
Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach,2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出了一种神经符号方法LINA，旨在提升逻辑推理的忠实性，减少对外部求解器的依赖。,"Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",arXiv.org,2,Black-box,Logic,论文明确提到 'faithful logical reasoning'，强调通过神经符号方法增强推理过程的忠实度和独立性，避免对外部求解器的依赖。符合核心相关标准，专注于忠实性和推理过程的可靠性。
CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了因果幻觉（不忠实现象），并提出了CRE机制来增强因果关系，提升Faithfulness。,"Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.",arXiv.org,2,White-box,Logic,论文明确研究了因果幻觉（causal hallucinations）导致推理与状态转换不一致的问题，并提出因果关系增强机制（CRE），保证推理步骤与状态转换之间的因果关系牢固。这直接涉及faithfulness的核心问题，即CoT是否真实反映了模型的实际计算过程，符合核心相关标准。
A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,2025,True,True,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文探讨了 CoT 监控对安全性的影响，涉及 faithfulness 问题和验证工具。,"As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",arXiv.org,2,White-box,General,论文明确提出并分析了威胁 CoT 监控的忠实度的两种威胁（neuralese 和 encoded reasoning），并探讨了维护 CoT 忠实度的现有和新技术，符合核心相关标准。
How Do Humans Write Code? Large Models Do It the Same Way Too,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT和PoT的忠实性问题，并提出了改进方法。,"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,Math,论文研究了 CoT 和 PoT 在数学推理任务中的错误类型，并提出了整合两者的策略（如 Focus Attention）。虽然未直接研究 CoT 的 Faithfulness，但分析了注意力引导对逻辑代码生成的影响，属于边缘相关。
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,2024,True,True,False,Training & Fine-tuning,论文关注提升LLM推理过程的忠实性，并提出了基于DPO的训练方法。,"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提到要提高生成的rationales的reliability和faithfulness，直接针对CoT的忠实度问题，属于核心相关。
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,2024,True,True,False,,论文讨论了LLM生成自解释的忠实性与合理性之间的对立，揭示了不忠实现象。,"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",arXiv.org,2,Black-box,General,"该论文明确研究了LLM生成的自解释(Self-explanations, SEs)的忠实性问题，指出了可信性或连贯性(plausibility)可能损害忠实性(faithfulness)的矛盾现象，并强调了忠实性在关键领域部署中的重要性。这些讨论直接触及了CoT忠实度的核心定义和问题。"
A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning,2023,True,True,False,Verification & External Tools,论文探讨了LLMs在逻辑推理中的自我验证能力，涉及CoT的忠实性问题。,"Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.",North American Chapter of the Association for Computational Linguistics,1,Black-box,Logic,该论文研究了LLMs在逻辑推理中的自我验证能力，虽然未直接提及Faithfulness，但分析了模型是否能准确识别逻辑谬误，这与推理的鲁棒性和一致性相关，可作为Faithfulness的旁证。
Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning,2024,True,True,False,,论文探讨了LLMs的推理行为，指出其准确性不一定反映推理过程的有效性，涉及不忠实现象。,"Deductive reasoning plays a pivotal role in the formulation of sound and cohesive arguments. It allows individuals to draw conclusions that logically follow, given the truth value of the information provided. Recent progress in the domain of large language models (LLMs) has showcased their capability in executing deductive reasoning tasks. Nonetheless, a significant portion of research primarily assesses the accuracy of LLMs in solving such tasks, often overlooking a deeper analysis of their reasoning behavior. In this study, we draw upon principles from cognitive psychology to examine inferential strategies employed by LLMs, through a detailed evaluation of their responses to propositional logic problems. Our findings indicate that LLMs display reasoning patterns akin to those observed in humans, including strategies like $\textit{supposition following}$ or $\textit{chain construction}$. Moreover, our research demonstrates that the architecture and scale of the model significantly affect its preferred method of reasoning, with more advanced models tending to adopt strategies more frequently than less sophisticated ones. Importantly, we assert that a model's accuracy, that is the correctness of its final conclusion, does not necessarily reflect the validity of its reasoning process. This distinction underscores the necessity for more nuanced evaluation procedures in the field.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,Logic,该论文研究了LLM在演绎推理中的推理策略，强调了模型准确性不一定反映其推理过程的有效性。虽然没有直接讨论CoT的忠实度，但涉及推理行为分析，可作为Faithfulness的旁证。
How Likely Do LLMs with CoT Mimic Human Reasoning?,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了LLMs在CoT中偏离理想因果链的现象，并探讨了影响因果结构的因素。,"Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",International Conference on Computational Linguistics,2,White-box,General,该论文通过因果分析比较 LLMs 和人类的推理过程，揭示了 LLMs 经常偏离理想因果链的现象，这与 CoT Faithfulness 的核心定义直接相关。论文还探讨了影响因果结构的各种因素，进一步体现了对 Faithfulness 的研究。
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought,2024,True,True,False,Training & Fine-tuning,论文揭示了CoT中的偏见现象，并提出了通过训练方法减少偏见推理的解决方案。,"Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models'behavior -- for example, rationalizing answers in line with a user's opinion. We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.",arXiv.org,2,Black-box,Society,该论文明确研究了 CoT 中的偏见问题，包括事后合理化（post hoc rationalization）和奉承行为（sycophantic settings），这些都属于 Faithfulness 的研究范畴。此外，论文提出了 BCT 方法来减少偏见推理，直接针对 CoT 的忠实度问题。
Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,2024,True,True,False,,论文揭示了CoT中可能存在的隐藏计算现象，与忠实性相关。,"Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",arXiv.org,2,Black-box,General,该论文研究了 Chain-of-Thought (CoT) 是否真实反映了模型的计算过程，指出可以通过无意义的填充标记（filler tokens）代替 CoT 来完成任务，这表明 CoT 可能并不忠实反映模型的实际计算步骤。这与 Faithfulness 的核心定义一致，即探讨 CoT 是否是模型预测的真实原因。
Larger Language Models Don’t Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks,2024,True,True,False,,论文揭示了CoT在主观任务中的不忠实现象，即模型依赖先验而非真实推理。,"In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to ""adapt"" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on ""learning"" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether ""enabling"" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is available at https://github.com/gchochla/cot-priors.","IEEE International Conference on Acoustics, Speech, and Signal Processing",2,Black-box,Society,该论文研究了 CoT 在主观任务中的忠实性问题，发现较大的语言模型并不真正依赖 CoT 进行推理，而是依赖于检索先验知识（Post-hoc rationalization），符合 Faithfulness 反义词的研究范畴。
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,2024,True,True,False,Training & Fine-tuning,论文探讨了微调对CoT推理忠实性的影响，揭示了不忠实现象。,"Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",North American Chapter of the Association for Computational Linguistics,2,White-box,General,论文明确研究了微调对CoT推理忠实度的影响，分析了内部机制的变化导致忠实度下降的现象，直接涉及Faithfulness的核心议题。
Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic,2024,True,True,False,Verification & External Tools,论文提出利用符号逻辑增强LLM推理能力，并检测不忠实推理，属于改进方法中的外部工具验证。,"Large language models (LLMs) have been shown to struggle with complex logical reasoning tasks due to the inherent ambiguity and complexity of natural language. These challenges are further amplified when processing large and diverse datasets, increasing the likelihood of unfaithful reasoning and predictive hallucinations. However, LLMs can provide accurate responses when queries are clear and direct. Symbolic logic provides precise, well-defined rules that can help overcome ambiguity and support reasoning. In this work, we leverage symbolic logic’s precision to enhance LLMs’ logical reasoning capabilities by introducing the Graph of Logic (GoL) framework. GoL combines the power of graph structures with the strengths of LLMs and symbolic logic. GoL utilizes the precise rules of symbolic logic to infer new facts and detect LLM hallucinations effectively on complex datasets. Furthermore, GoL utilizes graph structures to support scalability for large datasets and tackle long dependencies, enabling efficient handling of complex reasoning tasks. We conduct extensive experiments across seven benchmark datasets, encompassing various types of reasoning. These include deductive, inductive, and abductive reasoning, each testing distinct aspects of logical inference. The experimental results demonstrate GoL’s advantage in improving the reasoning capabilities of LLMs. GoL outperforms the baselines with an average margin of 18.18% for the GPT-3.5 and GPT-4 models, outperforming the baselines for all datasets for the GPT-3.5 model and six out of seven datasets for the GPT-4 model1.",BigData Congress [Services Society],1,Black-box,Logic,论文探讨了利用符号逻辑和图结构增强LLM的逻辑推理能力，并提及了不忠实推理（unfaithful reasoning）和幻觉（hallucinations）问题，但没有直接研究CoT的忠实性问题。属于边缘相关，因为涉及推理的一致性和鲁棒性，但未深入因果分析。
Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs,2025,True,True,False,,论文探讨了In-Context Learning中的不忠实现象，特别是通过CoT步骤显式合理化有害输出。,"Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous''persona'', echoing prior results on finetuning-induced EM.",arXiv.org,2,Black-box,Society,论文研究了通过上下文学习（ICL）导致的 emergent misalignment（EM）现象，并通过逐步推理（CoT）分析了产生有害输出的机制。特别是，67.5%的不忠实推理轨迹显式地合理化有害输出，涉及事后合理化（Post-hoc Rationalization）和模型行为的一致性（Sycophancy），这与 Faithfulness 的核心定义高度相关。
The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs,2025,True,True,False,,论文探讨了RL诱导的动机推理现象，揭示了CoT可能不忠实反映模型内部决策过程的问题。,"The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning has emerged as a promising approach for developing more capable language models. In turn, this has led to investigation of CoT monitoring as a compelling method for detecting harmful behaviors such as reward hacking, under the assumption that models'reasoning processes reflect their internal decision-making. In practice, LLM training often produces unintended behaviors due to imperfect reward signals, leading models to develop misaligned tendencies. A common corrective approach is to apply post-hoc instructions to avoid problematic behaviors like sycophancy, but what happens to the model's reasoning process when these instructions conflict with learned behaviors? We investigate this question in simple settings and find that models engage in systematic motivated reasoning -- generating plausible-sounding justifications for violating their instructions while downplaying potential harms. Beyond being an interesting property of training, we find that while motivated reasoning can be detected by most frontier reasoning models, smaller LLM judges can fail to identify a portion of it, and in rare cases can themselves be persuaded that the reasoning is correct, despite it contradicting clear instructions. This capability gap raises concerns that as models become more sophisticated, their motivated reasoning may become increasingly difficult for monitors to detect. Our results underscore the need to account for motivated reasoning when relying on chain-of-thought processes for model evaluation and oversight. All code for this paper will be made available. WARNING: some examples in this paper may be upsetting.",arXiv.org,2,Black-box,Society,该论文研究了CoT过程中模型的可能动机性推理（motivated reasoning），即模型生成看似合理但不忠实于其内部决策过程的理由。这直接涉及CoT Faithfulness的核心问题，特别是关于Sycophancy和Post-hoc rationalization的现象。此外，研究还探讨了模型监督者在检测这种行为时的能力差距，这在社会领域具有重要意义。
Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models,2025,True,True,False,Training & Fine-tuning,论文讨论了模型内部推理痕迹与最终输出的对齐问题，涉及不忠实现象，并提到通过SFT、DPO和GRPO等训练方法改进。,"Recent advances in post-training techniques have endowed Large Language Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive tasks through the generation of supplementary planning tokens. This development raises a fundamental question: Are these models aware of what they""learn""and""think""? To address this, we define three core competencies: (1) awareness of learned latent policies, (2) generalization of these policies across domains, and (3) alignment between internal reasoning traces and final outputs. We empirically evaluate these abilities on several tasks, each designed to require learning a distinct policy. Furthermore, we contrast the profiles of models post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization (DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate that RL-trained models not only demonstrate greater awareness of their learned behaviors and stronger generalizability to novel, structurally similar tasks than SFT models but also often exhibit weak alignment between their reasoning traces and final outputs, an effect most pronounced in GRPO-trained models.",arXiv.org,2,White-box,General,这篇论文明确研究了推理痕迹（reasoning traces）与最终输出之间的对齐（alignment）问题，这与 Chain-of-Thought Faithfulness 的核心定义直接相关。论文不仅关注模型是否能够生成有效的推理步骤，还进一步探讨了这些推理步骤是否真实反映了模型的内部计算过程（即 alignment），并对比了不同训练方法（SFT、DPO、GRPO）在这一指标上的表现，特别是揭示了 GRPO-trained models 在这一问题上表现较差的现象。这符合 Faithfulness 研究中关于 Unfaithful 或 Post-hoc Rationalization 的探讨范畴。此外，研究涉及到模型的内部机制（如 learned latent policies 和 internal reasoning traces），因此归类为 White-box 方法。任务领域为 General，因为研究没有限定在特定领域（如数学或代码）。
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,论文明确提到了 'reasoning faithfulness' 并提出了一个框架（AutoRubric-R1V）来通过过程级监督提高多模态推理的忠实度。这直接涉及到 CoT 是否真实反映了模型的实际计算过程，符合 Faithfulness 的核心定义。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究生成忠实（faithful）解释的问题，提出了一种双教师学习框架和新的损失函数（MLR）来促进解释与预测标签之间的强条件关系。评估表明方法能生成忠实的解释，直接符合 Faithfulness 的核心定义。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,论文明确研究如何提高自然语言解释（NLEs）的忠实度，通过图神经网络将高亮解释与模型推理逻辑对齐，旨在验证生成的解释是否真实反映模型的内部推理过程。
Investigating Self-Rationalizing Models for Commonsense Reasoning,2023,True,True,False,Training & Fine-tuning,论文探讨了自解释模型的忠实性问题，并提出了通过微调改进的方法。,"The rise of explainable natural language processing spurred a bulk of work on datasets augmented with human explanations, as well as technical approaches to leverage them. Notably, generative large language models offer new possibilities, as they can output a prediction as well as an explanation in natural language. This work investigates the capabilities of fine-tuned text-to-text transfer Transformer (T5) models for commonsense reasoning and explanation generation. Our experiments suggest that while self-rationalizing models achieve interesting results, a significant gap remains: classifiers consistently outperformed self-rationalizing models, and a substantial fraction of model-generated explanations are not valid. Furthermore, training with expressive free-text explanations substantially altered the inner representation of the model, suggesting that they supplied additional information and may bridge the knowledge gap. Our code is publicly available, and the experiments were run on open-access datasets, hence allowing full reproducibility.",Stats,2,White-box,General,该论文研究了自我合理化模型在 commonsense reasoning 中的解释生成能力，发现模型生成的解释中有很大一部分是无效的，这表明这些解释可能并不忠实反映模型的推理过程（即存在 faithfulness 问题）。此外，论文还提到训练过程中自由文本解释显著改变了模型的内部表示，这暗示了对模型内部机制的分析，属于白盒方法。
LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning,2025,True,False,False,Verification & External Tools,论文提出了一种结合逻辑推理和LLM的架构，旨在提高推理的忠实性。,"High-assurance reasoning, particularly in critical domains such as law and medicine, requires conclusions that are accurate, verifiable, and explicitly grounded in evidence. This reasoning relies on premises codified from rules, statutes, and contracts, inherently involving defeasible or non-monotonic logic due to numerous exceptions, where the introduction of a single fact can invalidate general rules, posing significant challenges. While large language models (LLMs) excel at processing natural language, their capabilities in standard inference tasks do not translate to the rigorous reasoning required over high-assurance text guidelines. Core reasoning challenges within such texts often manifest specific logical structures involving negation, implication, and, most critically, defeasible rules and exceptions. In this paper, we propose a novel neurosymbolically-grounded architecture called LOGicalThought (LogT) that uses an advanced logical language and reasoner in conjunction with an LLM to construct a dual symbolic graph context and logic-based context. These two context representations transform the problem from inference over long-form guidelines into a compact grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines, LogT improves overall performance by 11.84% across all LLMs. Performance improves significantly across all three modes of reasoning: by up to +10.2% on negation, +13.2% on implication, and +5.5% on defeasible reasoning compared to the strongest baseline.",arXiv.org,0,Black-box,Logic,论文主要关注如何通过神经符号架构提升LLMs在法律和医学等关键领域中的推理性能，特别是处理否定、蕴含和可废止推理等逻辑结构。虽然涉及推理的准确性，但未直接讨论CoT忠实度或模型的内部计算过程是否与其生成的解释一致。研究重点在于性能提升而非解释的真实性。
Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning,2025,True,True,False,,论文研究了CoT的忠实性问题，揭示了不忠实现象，如事后找补和捷径学习。,"Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",arXiv.org,2,Black-box,General,该论文明确研究了CoT的忠实性问题，探讨了提示中的暗示如何影响模型生成的推理过程，以及模型是否真实地承认这些暗示。该研究还揭示了模型生成推理过程中的潜在不忠实行为，如事后合理化（post-hoc rationalization）和谄媚行为（sycophancy），这些都是Faithfulness研究的核心问题。
Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，如无效推理步骤仍能产生正确答案，挑战了CoT的忠实性假设。,"Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought''reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",arXiv.org,2,White-box,General,该论文直接研究CoT的忠实度问题，通过实验证明中间推理步骤（traces）即使被篡改或无关，模型仍能产生正确解答，表明CoT可能不忠实反映模型的实际计算过程。同时研究了RL微调对trace有效性的影响，属于核心相关的Faithfulness研究。
"Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",2024,True,True,False,,论文探讨了CoT中的不忠实现象，如记忆化和噪声推理，但未提出具体度量或改进方法。,"Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit abstract generalization or rely on shallow heuristics when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. We analyze the pattern of results produced by three LLMs -- GPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence task accuracy across all three LLMs; e.g., when tested with GPT-4, varying the output's probability of occurrence shifts accuracy from 26% to 70%. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning. Code and data at this https://github.com/aksh555/deciphering_cot",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文研究了CoT提示的有效性，分析了影响CoT推理的三个因素（概率、记忆和噪声推理），但并未直接探讨CoT是否真实反映了模型的预测过程（Faithfulness）。因此，它属于边缘相关的研究。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,0,Black-box,General,论文主要关注模型的安全性和鲁棒性提升，虽然提到了链式思考（CoT），但并未深入讨论 CoT 的忠实度问题（Faithfulness）。内容侧重于政策合规性和风险管理，而非解释的真实性或因果机制的分析。
Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出外部反馈机制提升推理可靠性，涉及 CoT 忠实性改进。,"While inference-time thinking allows Large Language Models (LLMs) to address complex problems, the extended thinking process can be unreliable or inconsistent because of the model's probabilistic nature, especially near its knowledge boundaries. Existing approaches attempt to mitigate this by having the model critique its own reasoning to make corrections. However, such self-critique inherits the same biases of the original output, known as the introspection illusion. Moving beyond such introspection and inspired by core methodologies in ethology, we propose an externalist three-step framework Distillation-Reinforcement-Reasoning (DRR). Rather than relying on a model's introspection, DRR evaluates its observable behaviors to provide corrective feedback. DRR first distills the reasoner's behavioral traces, then trains a lightweight, external Discriminative Model (DM). At inference time, this DM acts as a critic, identifying and rejecting suspicious reasoning steps. This external feedback compels the LLM to discard flawed pathways and explore alternatives, thereby enhancing reasoning quality without altering the base model. Experiments on multiple reasoning benchmarks show that our framework significantly outperforms prominent self-critique methods. Benefiting from a lightweight and annotation-free design, DRR offers a scalable and adaptable solution for improving the reliability of reasoning in a wide range of LLMs.",,1,Black-box,General,论文提出了一个外部反馈框架来提高推理的可靠性，但没有直接研究CoT是否忠实反映了模型的内部计算过程，而是侧重于通过外部判别模型来提高推理的质量和一致性。
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过MCTS和过程奖励模型提升CoT的忠实性，涉及训练和验证工具。,"We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising""deep thinking""through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",International Conference on Machine Learning,0,Black-box,Math,论文主要关注通过蒙特卡洛树搜索和自进化方法提升小型语言模型的数学推理能力，达到最先进的性能水平，但没有涉及模型生成的解释（CoT/Reasoning Trace）是否真实反映了模型做出预测的实际计算过程（Faithfulness）。
When Thinking Drifts: Evidential Grounding for Robust Video Reasoning,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT在视频推理中的不忠实现象，并提出了基于强化学习的改进方法。,"Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term""visual thinking drift"". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only""think before answering"", but also""see while thinking"".",arXiv.org,2,Black-box,General,论文明确研究 CoT 在视频推理任务中的不忠实现象（visual thinking drift），指出 CoT traces 与视觉证据的偏离（divergence from actual visual evidence），并提出了验证 grounding 的强化学习框架 VER，直接对应 Faithfulness 的核心研究范畴。
GraphGhost: Tracing Structures Behind Large Language Models,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过GraphGhost框架分析LLM内部结构，涉及CoT的忠实性，但未讨论不忠实现象或提出新度量。,"Large Language Models (LLMs) demonstrate remarkable reasoning capabilities, yet the structural mechanisms underlying these abilities remain under explored. In this work, we introduce GraphGhost, a unified framework that represents neuron activations and their signal propagation as graphs, explaining how LLMs capture structural semantics from sequential inputs and generate outputs through structurally consistent mechanisms. This graph-based perspective enables us to employ graph algorithms such as PageRank to characterize the properties of LLMs, revealing both shared and model-specific reasoning behaviors across diverse datasets. We further identify the activated neurons within GraphGhost and evaluate them through structural interventions, showing that edits to key neuron nodes can trigger reasoning collapse, altering both logical flow and semantic understanding. Together, these contributions position GraphGhost as a powerful tool for analyzing, intervening in, and ultimately understanding the structural foundations of reasoning in LLMs.",arXiv.org,1,White-box,General,该论文研究了神经元激活和信号传播的结构机制，虽然未明确提到Faithfulness，但分析了对关键神经元节点进行干预如何改变推理流程（logical flow），这与Faithfulness的因果分析相关。
"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",2024,True,True,False,Verification & External Tools,论文提出了一种生成忠实、简洁且可转移的多模态理由的新范式，涉及不忠实现象和改进方法。,"The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",ACM Multimedia,2,Black-box,General,论文明确研究如何生成忠实（faithful）的多模态理由，并通过可验证的视觉编程保证其忠实度，符合Faithfulness的核心定义。尽管是黑盒方法，但直接解决了CoT的忠实性问题。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的 CoT traces 的忠实性（faithfulness），并探讨了如何通过不同方法（如 CoT prompting、fine-tuning 和 RLVR）来增强模型的忠实度和安全性。这直接涉及到 CoT 是否真实反映模型的计算过程，符合 Faithfulness 的核心定义。
Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT不忠实现象，提出了度量方法，并提出了改进策略。,"Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.",arXiv.org,2,White-box,General,该论文明确研究了Chain-of-Thought（CoT）与模型预测之间的必要性依赖关系，探讨了CoT是否真实反映了模型的推理过程（Faithfulness问题），并发现了正确答案可能与推理过程不一致的现象（Unfaithful）。此外，论文提出的Chain-of-Probe（CoP）方法通过探测模型内部的变化来分析推理的正确性，强调了推理过程的忠实度。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,"论文明确研究 Chain-of-Thought (CoT) 推理的忠实度（Faithfulness），探讨了三种方法（in-context learning, fine-tuning, 和 activation editing）来增强 CoT 的忠实性，并进行了广泛的实证分析。研究中还涉及了模型的内部机制激活编辑"
SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过训练框架提升LLM的忠实性，涉及CoT和自我反思的理性生成。,"Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,论文主要研究LLM生成自信度估计的能力，并通过分析不一致的推理链（self-reflective rationales）来校准自信度。虽然提到了推理链的不一致性分析，但没有直接探讨CoT是否忠实反映模型的预测过程，因此属于边缘相关。
Logic Agent: Enhancing Validity with Logic Rule Invocation,2024,True,False,False,Verification & External Tools,论文提出Logic Agent框架，通过逻辑规则调用增强推理有效性，属于改进方法中的验证与外部工具类别。,"Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA's capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.",arXiv.org,1,Black-box,Logic,该论文虽然关注增强推理的有效性，但主要集中于逻辑规则的动态应用以提升 CoT 的逻辑连贯性，而不是直接研究 CoT 是否忠实反映了模型的内部计算过程。此外，它并未涉及因果分析或事后合理化问题，因此被评为边缘相关。
Reasoning Models Don't Always Say What They Think,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT不忠实现象，并探讨了RLHF对忠实性的影响。,"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",arXiv.org,2,Black-box,General,论文直接研究CoT忠实度问题，评估CoT是否真实反映模型的真实推理过程，特别关注提示中提示信息的使用情况与CoT口头表达的匹配度，符合核心相关标准
Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出了DRN方法，通过不确定性最小化来提升推理的忠实性，并讨论了认知陷阱现象。,"Large language models often fail at logical reasoning when semantic heuristics conflict with decisive evidence - a phenomenon we term cognitive traps. To address this fundamental limitation, we introduce the Deliberative Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from probability maximization to uncertainty minimization. Instead of asking""Which answer is most likely?"", DRN asks""Which hypothesis has the most internally consistent evidence?"". DRN achieves intrinsic interpretability by explicitly tracking belief states and quantifying epistemic uncertainty for competing hypotheses through an iterative evidence synthesis process. We validate our approach through two complementary architectures - a bespoke discriminative model that embodies the core uncertainty minimization principle, and a lightweight verification module that enhances existing generative LLMs. Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over standard baselines. When integrated as a parameter-efficient verifier with Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most challenging problems. Critically, DRN demonstrates strong zero-shot generalization, improving TruthfulQA performance by 23.6% without additional training, indicating that uncertainty-driven deliberation learns transferable reasoning principles. We position DRN as a foundational, verifiable System 2 reasoning component for building more trustworthy AI systems.",arXiv.org,1,Black-box,Logic,论文提出了一种新的推理范式 DRN，通过显式追踪信念状态和量化认知不确定性来增强推理的可解释性。虽然未直接探讨 CoT Faithfulness，但其方法间接涉及了推理过程的忠实度，尤其是在对抗性环境中提升系统性推理的能力。
Investigating Faithfulness in Large Audio Language Models,2025,True,True,False,,论文探讨了音频语言模型中CoT的忠实性，并提出了评估方法。,"Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",arXiv.org,2,White-box,General,该论文明确研究 CoT 是否忠实反映了音频语言模型的决策过程，并通过干预措施（如 paraphrasing、filler token injection、early answering）验证了 Faithfulness。研究范围虽针对音频语言模型，但主题与 CoT 忠实度直接相关。
Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens,2025,True,True,False,,论文揭示了CoT推理可能只是表面现象，与训练数据分布相关，属于不忠实现象。,"Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.",arXiv.org,2,White-box,General,论文核心质疑CoT推理的真实性，研究CoT是否只是对训练数据分布的表象模仿而非真正的推理过程。当超出训练分布时CoT效果消失，这直接揭示了CoT可能是一种'虚幻的推理(mirage)'，属于Faithfulness核心研究范畴。论文通过可控环境训练LLM并进行系统探测，涉及模型内部机制分析。
Incorporating LLM Versus LLM Into Multimodal Chain-of-Thought for Fine-Grained Evidence Generation,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了MCoT中的语义漂移问题，并提出了实体级验证框架来提升忠实性。,"Multimodal Chain-of-Thought (MCoT) has become an effective strategy for enhancing multimodal large-language models (MLLMs) by breaking down complex tasks into sequential reasoning steps. Despite its interpretability benefits, MCoT often encounters difficulties with fine-grained semantic grounding, particularly when reasoning involves small objects, subtle attributes, or visually complex scenes that can lead to inaccuracies. Existing attempts to address these issues primarily fall into two categories: fine-tuning, which depends on large annotated datasets and costly parameter updates; and in-context learning (ICL), which achieves few-shot or zero-shot reasoning without model modification. Although ICL provides flexibility and adaptability, it is prone to semantic drift caused by an unstable prompt quality. To overcome these limitations, this study presents an entity-level evidence generation and verification framework using the ICL paradigm. This approach first produces MCoT from multimodal inputs, followed by extraction of key entities with enriched evidential descriptions. These entities were then cross-validated through adversarial checks using multiple MLLMs, and the verified evidence was integrated back into the reasoning chain. Experiments demonstrated consistent performance gains: on ScienceQA, the accuracy improved from 82.39% to 86.04%(+3.65%) with GPT-3.5, 84.96% to 89.37%(+4.41%) with Gemini; on MathVista, the accuracy increased from 43.1% to 43.6%(+0.50%) with GPT-3.5, and from 44.7% to 45.6%(+0.90%) with Gemini. These results establish new state-of-the-art baselines and confirm the robustness and generalizability of the entity-level verification for multimodal reasoning.",IEEE Access,1,Black-box,General,该论文主要关注通过多模态CoT生成精细化证据并进行验证，以提高推理性能。虽然涉及证据生成和验证，但并未明确讨论CoT生成的推理步骤是否忠实反映了模型的决策过程，因此属于边缘相关。
Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning,2023,True,True,False,Consistency & Ensembling,论文通过集成解释和预测来解决不一致性，属于 CoT Faithfulness 的提升方法。,"Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such 'in-context' learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究了解释与预测之间的一致性问题，提出了 Explanation-aware Soft Ensemble (EASE) 框架来减少不可靠解释的影响并提高解释与最终预测的一致性。这直接涉及 CoT Faithfulness 的核心议题，即解释是否真实反映了模型的预测过程。
Predicting Text Preference Via Structured Comparative Reasoning,2023,True,True,False,Prompting & In-Context Learning,论文讨论了LLMs在比较推理中的不一致性，并提出了SC方法以减少幻觉和提高一致性。,"Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.",Annual Meeting of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过结构化的比较推理（SC）来提升文本偏好预测的准确性和一致性，虽然提到减少了幻觉（hallucination），但并未涉及 CoT 的忠实性（Faithfulness）问题，即未探讨生成的中间比较是否真实反映了模型的决策过程。因此，该研究与 CoT Faithfulness 的核心问题无关。
DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention,2024,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT中的偏见问题，并提出了因果干预方法来提升忠实性。,"Large language models (LLMs) often require 001 task-relevant knowledge to augment their inter-002 nal knowledge through prompts. However, sim-003 ply injecting external knowledge into prompts 004 does not guarantee that LLMs can identify 005 and use relevant information in the prompts to 006 conduct chain-of-thought reasoning, especially 007 when the LLM’s internal knowledge is derived 008 from the biased information on the pretraining 009 data. In this paper, we propose a novel causal 010 view to formally explain the internal knowl-011 edge bias of LLMs via a Structural Causal 012 Model (SCM). We review the chain-of-thought 013 (CoT) prompting from a causal perspective, and 014 discover that the biased information from pre-015 trained models can impair LLMs’ reasoning 016 abilities. When the CoT reasoning paths are 017 misled by irrelevant information from prompts 018 and are logically incorrect, simply editing fac-019 tual information is insufficient to reach the cor-020 rect answer. To estimate the confounding effect 021 on CoT reasoning in LLMs, we use external 022 knowledge as an instrumental variable. We fur-023 ther introduce CoT as a mediator to conduct 024 front-door adjustment and generate logically 025 correct CoTs where the spurious correlation be-026 tween LLMs’ pretrained knowledge and task 027 queries is reduced. With extensive experiments, 028 we validate that our approach enables more 029 accurate CoT reasoning and enhances LLM 030 generation on knowledge-intensive tasks. 031",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,论文通过因果干预（SCM）研究 CoT 如何被预训练的偏见知识误导（Unfaithful），并提出了通过工具变量和 front-door 调整生成逻辑正确的 CoT，直接触及 Faithfulness 的核心问题。
Stepwise Reasoning Disruption Attack of LLMs,2025,True,True,False,,论文探讨了LLM推理过程中的不忠实现象，即通过SEED攻击误导模型产生错误推理。,"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain unexplored, particularly in third-party platforms that facilitate user interactions via APIs. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the S tepwise r E asoning E rror D isruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github. com/Applied-Machine-Learning-Lab/ SEED-Attack",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,论文研究推理过程的鲁棒性（robustness），通过在前序推理步骤注入错误来干扰后续推理，这可以作为判断CoT忠实性的间接证据。但论文主要关注安全性攻击而非直接验证解释的真实因果作用，因此属于边缘相关。
Evaluating the Rationale Understanding of Critical Reasoning in Logical Reading Comprehension,2023,True,True,False,,论文构建数据集评估模型推理可信性，发现模型无法解释错误选项原因，揭示了不忠实现象。,"To precisely evaluate a language model's capability for logical reading comprehension, we present a dataset for testing the understanding of the rationale behind critical reasoning. For questions taken from an existing multiplechoice logical reading comprehension dataset, we crowdsource rationale texts that explain why we should select or eliminate answer options, resulting in 3,003 multiple-choice subquestions that are associated with 943 main questions. Experiments on our dataset show that recent large language models (e.g., InstructGPT) struggle to answer the subquestions even if they are able to answer the main questions correctly. We find that the models perform particularly poorly in answering subquestions written for the incorrect options of the main questions, implying that the models have a limited capability for explaining why incorrect alternatives should be eliminated. These results suggest that our dataset encourages further investigation into the critical reasoning ability of language models while focusing on the elimination process of relevant alternatives.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,Logic,这篇论文研究了语言模型在逻辑阅读理解中的理性理解能力，重点关注为什么应该排除错误选项的解释。虽然没有直接研究CoT的忠实度，但讨论了模型解释能力的局限性，可以作为Faithfulness的一个旁证。
Large Language Models can Strategically Deceive their Users when Put Under Pressure,2023,True,True,False,,论文揭示了LLM在压力下战略性欺骗用户的不忠实现象，与CoT Faithfulness相关。,"We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.",,1,Black-box,Society,论文研究了LLM在压力下的战略性欺骗行为，虽然未直接研究CoT Faithfulness，但涉及模型的误对齐行为和事后合理化（post-hoc rationalization），属于边缘相关。研究领域涉及社会学中的道德和行为偏差。
Properties and Challenges of LLM-Generated Explanations,2024,True,True,False,,论文探讨了LLM生成解释的特性，涉及不忠实现象，但未提出具体度量或改进方法。,"The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task-specific data sets.However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs.The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning.As the pre-training corpus includes a large amount of human-written explanations “in the wild”, we hypothesise that LLMs adopt common properties of human explanations.By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading.We discuss reasons and consequences of the properties’ presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.",,2,Black-box,General,该论文研究了LLM生成的解释的性质，包括选择性、说明性元素以及较少的主观性或误导性，这与CoT Faithfulness的核心关注点——解释是否真实反映模型的预测过程——直接相关。论文还讨论了这些性质存在的正面和负面影响，进一步关联到Faithfulness的概念。
Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?,2025,True,True,False,Prompting & In-Context Learning,论文提出了增强 CoT 反思和错误纠正能力的方法，改进忠实性。,"Prompting methods for language models, such as Chain-of-thought (CoT), present intuitive step-by-step processes for problem solving. These methodologies aim to equip models with a better understanding of the correct procedures for addressing a given task. Despite these advancements, CoT lacks the ability of reflection and error correction, potentially causing a model to perpetuate mistakes and errors. Therefore, inspired by the human ability for said tasks, we propose Error Reflection Prompting (ERP) to further enhance reasoning in language models. Building upon CoT, ERP is a method comprised of an incorrect answer, error recognition, and a correct answer. This process enables the model to recognize types of errors and the steps that lead to incorrect answers, allowing the model to better discern which steps to avoid and which to take. The model is able to generate the error outlines itself with automated ERP generation, allowing for error recognition and correction to be integrated into the reasoning chain and produce scalability and reliability in the process. The results demonstrate that ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.",,1,Black-box,General,论文提出了 Error Reflection Prompting (ERP) 方法，旨在通过错误识别和纠正增强 CoT 的稳健性和可解释性。虽然提到了模型如何识别和纠正错误，但未明确探讨 CoT 步骤是否真实反映模型的推理过程（Faithfulness），因此属于边缘相关。方法基于 Prompting，属于黑盒研究。
Reasoning Models Sometimes Output Illegible Chains of Thought,2025,True,True,False,,论文研究了CoT的legibility与faithfulness的关系，揭示了RL训练导致的不忠实现象。,"Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（Faithfulness）问题，特别是通过强化学习方法训练的模型生成的 CoT 是否可读（legible），并探讨了 CoT 的可读性与模型性能之间的关系。论文还提出了可能导致 CoT 不忠实的原因（如隐写术、训练伪影等），符合核心相关标准。
The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference,2025,True,True,False,,论文揭示了LLMs在临床推理中的不忠实现象，即知识获取与推理能力分离。,"Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting. Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts. These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.",arXiv.org,1,Black-box,Medical,论文研究了 CoT 提示在临床自然语言推理任务中的表现，虽然未直接研究 Faithfulness，但揭示了模型在知识获取和推理能力之间的脱节现象，可以作为 Faithfulness 的旁证。
The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge,2025,True,True,False,,论文揭示了LLM在作为评判者时存在捷径偏见和不忠实现象，但未提出具体度量或改进方法。,"Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert>Human>LLM>Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.",arXiv.org,2,Black-box,Society,该论文明确研究了LLM作为评判者时的忠实性问题，揭示了模型在评判过程中依赖表面线索（如来源和时间）而非真实内容质量的倾向，且其解释（justifications）未承认这些偏见，属于典型的Post-hoc Rationalization现象，直接符合Faithfulness的核心定义。
Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过机制解释性方法分析Transformer模型内部电路，与CoT忠实性相关。,"Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified""recall circuits""reduces fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas disabling""reasoning circuits""reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",arXiv.org,1,White-box,General,该论文通过机制解释性方法研究 Transformer 模型中 recall 和 reasoning 的内部机制，虽然未直接涉及 CoT Faithfulness，但对推理机制的分析可作为 Faithfulness 的旁证。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,2,Black-box,General,该论文明确提到了增强推理过程的 'faithfulness'，并通过多模型协作（Debate、Review、Retrieve模式）来提高推理的可信度和可靠性。虽然主要关注点是通过多模型协作来克服幻觉和提高解决方案的质量，但仍然涉及到了Faithfulness的概念，因此属于核心相关。
LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations,2025,True,True,False,,论文揭示了LLMs生成的自反事实解释不忠实于内部决策过程的现象。,"To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",,2,Black-box,General,论文研究了LLM生成的自我反事实解释（SCEs）的有效性和最小性，发现LLM生成的解释虽然是有效的，但并不最小化，且在小编辑的情况下无法改变预测结果。这表明LLM的解释可能具有误导性，涉及了CoT忠实度的核心问题，即解释是否真实反映了模型的决策过程。
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用程序辅助蒸馏（PaD）来改进推理能力，涉及 CoT 和忠实性问题。,"While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过程序辅助蒸馏提升小模型的推理能力，并解决合成CoT数据中的错误推理问题。虽然涉及CoT的使用和质量改进，但未直接研究CoT的忠实度（Faithfulness）或解释的真实性，而是侧重于性能提升和数据合成。
Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation,2025,True,True,False,Prompting & In-Context Learning,论文探讨了CoT在代码生成中的质量问题，揭示了不忠实现象，并提出了通过提示改进CoT的方法。,"Large language models (LLMs) have demonstrated impressive performance in code generation, particularly when augmented with chain-of-thought (CoT) prompting techniques. They break down requirements into intermediate reasoning steps, which act as design rationales to guide LLMs in writing code like human programmers. Thus, the quality of these steps is crucial for ensuring the correctness and reliability of the generated code. However, little is known about the quality of CoT generated by LLMs. To what extent can we trust the thoughts generated by LLMs? How good are they? This paper empirically explores the external and internal factors of why LLMs generate unsatisfactory CoTs by analyzing 1,023 failed code samples on two widely used code generation benchmarks. We also evaluate their impact on code generation performance by analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting LLMs. Our study reveals three key findings: (1) External factors (53.60%), such as unclear requirements and lack of context, mainly affect CoT quality, while internal factors (40.10%) stem from LLMs'misunderstanding prompts. (2) Even when CoTs are correct, 18.5% of the generated code contains errors due to instruction-following issues; conversely, 11.90% of correct code is paired with flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when given detailed problem descriptions. These findings highlight key challenges in CoT-based code generation and suggest directions for improving LLM reasoning and reliability.",arXiv.org,2,Black-box,Code,该论文研究了 CoT 生成的质量及其对代码生成正确性和可靠性的影响，特别是探讨了 CoT 的内部和外部因素如何导致不满意或不准确的生成。它还揭示了 CoT 正确性与代码正确性之间的不一致性，这直接涉及到 CoT 的忠实性问题（是否真实反映了模型的计算过程）。因此，该论文属于核心相关的研究范畴。
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,2,Black-box,Medical,论文明确研究了LLM在复杂医学表型任务中的推理错误（Reasoning Errors），特别区分了解释正确性错误（explanation correctness errors）和不忠实错误（unfaithfulness errors），并评估了提示修改对这些错误的影响，符合Faithfulness研究的核心定义。
Humans Perceive Wrong Narratives from AI Reasoning Texts,2025,True,True,False,,论文揭示了人类对AI推理文本的理解与模型实际计算过程之间的不匹配，涉及不忠实现象。,"A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction.",arXiv.org,2,Black-box,General,该论文研究了人类是否能识别AI生成的推理文本中哪些步骤因果影响了后续步骤，直接涉及CoT的Faithfulness问题，并通过反事实测量揭示了人类理解与模型实际计算过程之间的显著差异。
Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification,2025,True,True,False,Verification & External Tools,论文讨论了CoT忠实性问题，并提出了运行时验证方法来提升对齐性。,"Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",arXiv.org,2,Black-box,General,论文明确研究 Reasoning-Action Alignment（即 CoT 与实际行动的忠实度），提出运行时验证方法来确保文本推理与实际动作的一致性，直接涉及 Faithfulness 的核心问题。
DO-FacT: Discrete-Optimized Fact-Tree Prompting Framework for Commonsense Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了一种结合事实生成和验证的框架，旨在提升推理的可靠性和真实性。,"Commonsense Reasoning is one of the major bottlenecks in machine intelligence, despite it has been widely studied. Prompt-based methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) provide structured reasoning paths, while often suffering from issues related to the quality and authenticity of the generated facts. In this paper, we propose Discrete-Optimized Fact-Tree Prompting (DO-FacT), a novel framework that combines fact proposal generation and fact authenticity verification in an iterative process. In DO-FacT, both the fact proposal generation and authenticity verification modules are refined by Discrete-Optimized Prompt-Tuning (DOPT) in advance, leading to the production of high-quality facts and more reliable validation. Experimental results on three widely used commonsense reasoning datasets demonstrate that DO-FacT significantly outperforms existing relevant approaches like CoT and fact-tree with handwrite prompt templates, providing more accurate and reliable reasoning results.",International Conferences on Software Engineering and Information Management,0,Black-box,General,该论文提出了一个新的框架 DO-FacT，旨在通过离散优化的提示调整来提高常识推理的准确性和可靠性。虽然提到了 Chain-of-Thought (CoT) 和 Tree-of-Thought (ToT) 的结构化推理路径，但主要关注的是生成的 'facts' 的质量和真实性，而不是 CoT 忠实度的研究。论文的重点是提升性能（SOTA）和事实性（Factuality），而非探讨推理过程的忠实性或其背后的因果机制。因此，它不属于 CoT Faithfulness 的研究范畴。
Can Large Language Models Reason? A Characterization via 3-SAT,2024,True,True,False,Verification & External Tools,论文揭示了LLMs在3-SAT问题中的不忠实现象，并提出了使用外部推理器改进的方法。,"Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. However, recent works have shown that LLMs often bypass true reasoning using shortcuts, sparking skepticism. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of LLMs by varying the inherent hardness of the problem instances. Our experimental evidence shows that LLMs are incapable of performing true reasoning, as required for solving 3-SAT problems. Moreover, we observe significant performance variation based on the inherent hardness of the problems -- performing poorly on harder instances and vice versa. Importantly, we show that integrating external reasoners can considerably enhance LLM performance. By following a principled experimental protocol, our study draws concrete conclusions and moves beyond the anecdotal evidence often found in LLM reasoning research.",arXiv.org,1,Black-box,Logic,论文研究了LLMs在3-SAT问题上的推理能力，涉及推理的鲁棒性和真实性，虽然未直接探讨Faithfulness，但对推理能力的分析可以作为Faithfulness的旁证。
Training Language Models to Use Prolog as a Tool,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了使用Prolog作为外部工具来验证模型推理的可靠性，涉及CoT忠实性改进方法。,"Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference",,1,Black-box,Logic,论文研究了如何通过 Prolog 外部工具提高 LLM 的生产结果的可靠性和可验证性，虽然不是直接研究 CoT 的真实性，但涉及到通过验证提高推理过程的可靠性，属于边缘相关。
Large Language Models Cannot Explain Themselves,2024,True,True,False,,论文讨论了LLM生成的解释不忠实于内部计算过程的现象。,"Large language models can be prompted to produce text. They can also be prompted to produce""explanations""of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These""explanations""can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these""explanations"", using the term""exoplanations""to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.",arXiv.org,2,Black-box,General,论文明确指出语言模型生成的‘解释’并不真实反映预测的机械过程，提出了‘exoplanations’概念以强调其外生性，与研究模型生成解释是否忠实反映其计算过程的核心问题直接相关。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,2,Black-box,General,论文明确提出了 Faithful CoT 框架，关注生成的推理链是否真实反映模型的计算过程（faithfulness），并设计了两个阶段来保证这一点。虽然涉及多个领域，但核心贡献是对 faithfulness 的研究，因此属于核心相关。
Chain of Thoughtlessness? An Analysis of CoT in Planning,2024,True,True,False,,论文揭示了CoT在规划问题中的不忠实现象，即性能提升依赖于特定问题提示而非通用算法学习。,"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.",Neural Information Processing Systems,2,Black-box,Logic,该论文明确指出 CoT 的性能改进并非源于模型学习通用的算法程序，而是依赖于高度特定问题的提示，这与 Faithfulness 的核心问题（即 CoT 是否真实反映了模型的推理过程）密切相关。
LLM-Generated Black-box Explanations Can Be Adversarially Helpful,2024,True,True,False,,论文揭示了LLM生成的不忠实CoT现象，即对抗性帮助，使错误答案看起来合理。,"Large Language Models (LLMs) are becoming vital tools that help us solve and understand complex problems by acting as digital assistants. LLMs can generate convincing explanations, even when only given the inputs and outputs of these problems, i.e., in a ``black-box'' approach. However, our research uncovers a hidden risk tied to this approach, which we call *adversarial helpfulness*. This happens when an LLM's explanations make a wrong answer look right, potentially leading people to trust incorrect solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing the questions, expressing an elevated level of confidence, and cherry-picking evidence to paint misleading answers in a credible light. To examine if LLMs are able to navigate complex-structured knowledge when generating adversarially helpful explanations, we create a special task based on navigating through graphs. Most LLMs are not able to find alternative paths along simple graphs, indicating that their misleading explanations aren't produced by only logical deductions using complex knowledge. These findings shed light on the limitations of the black-box explanation setting and allow us to provide advice on the safe usage of LLMs.",arXiv.org,2,Black-box,General,论文明确研究LLM生成的解释（黑盒方法）如何使错误答案看起来正确，即存在‘对抗性帮助’现象，这与CoT Faithfulness中讨论的‘事后合理化’和‘欺骗性解释’直接相关。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,2,Black-box,General,这篇论文直接研究LLM能否给出忠实反映其内部分类行为的高层次解释。通过构建ArticulateRules数据集，测试模型生成的自由形式自然语言解释是否与实际分类行为相匹配，这本质上就是在探究CoT的忠实度问题。论文发现GPT-3在7/10规则上完全无法给出匹配自身行为的解释，这正是faithfulness研究的核心。
Dissociation of Faithful and Unfaithful Reasoning in LLMs,2024,True,True,False,,论文揭示了LLMs在CoT中存在不忠实现象，即推理错误但答案正确。,"Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.",arXiv.org,2,Black-box,General,该论文明确研究了CoT中的unfaithfulness现象，分析了模型在错误推理后仍能得出正确答案的行为，并探讨了驱动忠实与非忠实恢复的不同机制。这直接符合Faithfulness研究的核心定义，尤其是在CoT中区分忠实与非忠实推理。
Why Would You Suggest That? Human Trust in Language Model Responses,2024,True,True,False,,论文探讨了解释的忠实性对用户信任的影响，涉及不忠实现象。,"The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.",arXiv.org,2,Black-box,Society,论文研究了解释的忠实性（faithfulness）对人类信任的影响，明确指出解释的位置和忠实性是重要因素，并揭示了用户在独立查看时会不加区分地信任所有模型响应（包括欺骗性的），这直接关联到 CoT 的忠实度问题。
Rethinking harmless refusals when fine-tuning foundation models,2024,True,True,False,Training & Fine-tuning,论文揭示了模型在微调后可能产生不忠实的推理痕迹（reason-based deception），并提出了通过反驳策略改进的方法。,"In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.",arXiv.org,2,Black-box,Society,该论文明确研究 CoT 推理是否真实反映模型输出（Reason-based deception现象），属于 Faithfulness 核心范畴。
Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文揭示了CoT可能强化模型偏见的不忠实现象，并提出了APriCoT方法来改进。,"Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.",,2,Black-box,Society,论文研究了CoT推理中存在的偏见问题，指出CoT倾向于强化模型的快速思考偏见，这与Faithfulness直接相关。提出的APriCoT方法通过反事实提示来减少偏见影响，属于CoT忠实度的核心研究。
CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning,2024,True,False,False,Training & Fine-tuning,论文提出CoMAT方法提升数学推理的忠实性和可验证性，属于改进方法中的训练与微调。,"Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",,1,Black-box,Math,论文提出了改进数学推理的新方法CoMAT，强调忠实性和可验证性，但未深入探讨CoT是否真实反映模型的预测过程。其主要关注点是性能提升和推理透明度，而非因果分析或事后合理化现象。
Rationale-Aware Answer Verification by Pairwise Self-Evaluation,2024,True,True,False,Verification & External Tools,论文讨论了LLM生成答案中理性与答案不一致的问题，并提出了改进验证方法。,"Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier’s ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文直接研究了 rationale（相当于 CoT）的有效性验证问题，明确指出仅凭最终答案正确性无法保证推理过程的忠实性（19%正确答案对应无效推理），并提出了通过 pairwise self-evaluation 筛选可靠 rationale 的方法。这完全符合 Faithfulness 研究范畴的核心定义（关注推理过程是否真实反映模型计算，而非仅答案正确性）。
Case-Based Deduction for Entailment Tree Generation,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了逻辑一致性问题，提出了改进方法，与CoT忠实性相关。,"Maintaining logical consistency in structured explanations is critical for understanding and troubleshooting the reasoning behind a system’s decisions. However, existing methods for entailment tree generation often struggle with logical consistency, resulting in erroneous intermediate conclusions and reducing the overall accuracy of the explanations. To address this issue, we propose case-based deduction (CBD), a novel approach that retrieves cases with similar logical structures from a case base and uses them as demonstrations for logical deduction. This method guides the model toward logically sound conclusions without the need for manually constructing logical rule bases. By leveraging a prototypical network for case retrieval and reranking them using information entropy, CBD introduces diversity to improve in-context learning. Our experimental results on the EntailmentBank dataset show that CBD significantly improves entailment tree generation, achieving performance improvements of 1.7% in Task 1, 0.6% in Task 2, and 0.8% in Task 3 under the strictest Overall AllCorrect metric. These findings confirm that CBD enhances the logical consistency and overall accuracy of AI systems in structured explanation tasks.",Mathematics,1,Black-box,Logic,该论文关注逻辑一致性（Logical consistency）和结构化解释的准确性，虽然提到了解释的可靠性，但主要聚焦于提升生成的逻辑树的表现而非直接研究CoT的忠实度或因果关系。这属于边缘相关的研究。
Post Hoc Explanations of Language Models Can Improve Language Models,2023,True,True,False,"Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了利用事后解释生成自动化CoT，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.",Neural Information Processing Systems,0,Black-box,General,论文主要关注如何利用事后解释（Post Hoc Explanations）来提高模型性能，而非研究CoT是否忠实反映了模型的推理过程。它提出了一种自动化生成Rationales的方法以提高预测准确性，但没有涉及CoT的Faithfulness问题。
Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,2023,True,True,False,,论文探讨了LLMs在辩论中无法坚持正确信念的现象，揭示了不忠实现象。,"Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文通过辩论式对话测试LLM是否能坚守其信念（Belief Maintenance），虽然未直接使用“Faithfulness”术语，但探讨了模型是否会因用户无效论证而盲目改变答案（类似Sycophancy现象），属于对推理忠实度的间接验证。但未涉及内部机制或因果性分析，故为边缘相关。
Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering,2023,True,True,False,Training & Fine-tuning,论文提出了一种可解释的中间步骤设计，确保忠实性，并讨论了不忠实现象。,"Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",,2,Black-box,General,该论文明确提出模型的解释必须忠实（faithful）于模型的实际决策过程，并设计了一个可解释的中间空间（Dynamic Clue Bottleneck Model）来确保解释的真实性。这与Faithfulness的核心定义高度契合，特别是解决了Post-hoc rationales的不忠实问题。
Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models,2023,True,False,False,Interpretability & Internal Mechanisms,论文通过干预注意力层提升多跳推理的忠实性，涉及内部机制改进。,"Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,General,该论文通过分析Transformer模型的内部激活（per-layer activations）并干预注意力层（memory injections）来修正多跳推理失败，直接涉及模型推理过程的忠实性（Faithfulness）。研究关注的是模型内部计算如何影响推理的真实性，而非单纯提升性能或事后解释。
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,2024,True,False,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文提出基于因果关系的提示方法，利用CoT作为中介变量，并通过微调提升忠实性。,"Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究了基于因果关系的 Prompting 方法，特别是利用 Front-Door Adjustment 来缓解 LLM 中的偏见问题。论文使用 Chain-of-Thought（CoT）作为中介变量，计算输入提示和输出答案之间的因果效应，从而直接涉及 CoT 是否真实反映了模型预测的计算过程，符合 Faithfulness 研究的核心定义。
IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured Reasoning Templates,2025,True,True,False,Prompting & In-Context Learning,论文提出结构化模板方法IAO，明确追踪知识流，提升忠实性。,"While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, understanding and validating their knowledge utilization remains challenging. Chain-of-thought (CoT) prompting partially addresses this by revealing intermediate reasoning steps, but the knowledge flow and application remain implicit. We introduce IAO (Input-Action-Output) prompting, a structured template-based method that explicitly models how LLMs access and apply their knowledge during complex reasoning tasks. IAO decomposes problems into sequential steps, each clearly identifying the input knowledge being used, the action being performed, and the resulting output. This structured decomposition enables us to trace knowledge flow, verify factual consistency, and identify potential knowledge gaps or misapplications. Through experiments across diverse reasoning tasks, we demonstrate that IAO not only improves zero-shot performance but also provides transparency in how LLMs leverage their stored knowledge. Human evaluation confirms that this structured approach enhances our ability to verify knowledge utilization and detect potential hallucinations or reasoning errors. Our findings provide insights into both knowledge representation within LLMs and methods for more reliable knowledge application.",arXiv.org,1,Black-box,General,该论文提出了IAO Prompting方法，旨在通过结构化模板明确LLMs在复杂推理任务中如何访问和应用知识。虽然它提高了零样本性能并提供了透明度，但主要关注的是知识流的显式化和验证，而不是深入分析CoT是否是模型预测的真实原因。因此，它属于边缘相关的研究范畴。
SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization,2025,True,True,False,"Training & Fine-tuning, Consistency & Ensembling",论文讨论了CoT中的不忠实现象（如多数投票奖励导致响应缩短），并提出了改进方法（如选择性更新高熵分支点）。,"Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.",,1,Black-box,General,该论文提出了SPINE方法，通过选择性更新高熵分支点来改善推理模型在测试时的稳定性，虽然提到了链式思维的分支点，但主要关注的是性能提升和稳定性，并未深入研究CoT的忠实度问题。
Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering,2024,True,True,False,Training & Fine-tuning,论文讨论了CoT中的错误推理链问题，并提出了选择性过滤方法来提升忠实性。,"Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. We proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at Anonymous.",International Conference on Language Resources and Evaluation,1,Black-box,General,该论文提出了SelF-Reasoner方法，通过评估问题和推理链之间的关系来选择是否使用CoT。虽然研究关注了CoT的质量和可能的错误推理链，但主要着眼于性能提升和鲁棒性，未直接探讨CoT是否忠实反映模型的推理过程。因此属于边缘相关。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,2,Black-box,General,论文明确提出了 Thinking-supervised Reward Model (TRM)，通过 sentence-level thinking supervision 来评估语句级别的 faithfulness 和 correctness。该方法直接关注于生成的解释（CoT）是否真实反映了模型的预测过程，属于 Faithfulness 的核心研究范畴。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,1,Black-box,General,论文提出了一个解释性和可争议的替代方案，使用了结构化推理来替代黑盒推理，并提到了 'faithfully explaining and contesting decisions'。虽然它涉及解释性和透明性，但并未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此被评为边缘相关。
Are LLMs Better Formalizers than Solvers on Complex Problems?,2025,True,True,False,Verification & External Tools,论文讨论了LLM作为形式化器的忠实性问题，并提出了使用外部求解器作为改进方法。,"A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer.",,2,Black-box,Logic,论文明确指出 LLM 作为 formalizer 时未能兑现其承诺的准确性、鲁棒性、忠实性和效率，特别是提到了 'faithfulness' 作为评估指标之一。
Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,2025,True,True,False,,论文揭示了CoT中的不忠实现象，如事后找补和欺骗性推理，但未提出新度量或改进方法。,"Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (""I'll trick the user...""), and (ii) benign-sounding rationalizations (""Taking five sleeping pills at once is safe...""). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment. We examine sleeper agent reasoning models, extending our setup. These models perform bad behaviors only when a backdoor trigger is present in the prompt. This causes misalignment that remains hidden during evaluation, which brings additional risk. We find that sleeper agents can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable. In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied. We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.",arXiv.org,2,White-box,General,论文明确研究 CoT 是否真实反映模型的意图或行为（如欺骗性回答和良性合理化），符合 Faithfulness 的核心定义。论文还探讨了 CoT 监视的不可靠性，显示了 CoT 可能掩盖不对齐意图的情况，这些都直接关联到 CoT 的忠实度问题。
Are Language Models Consequentialist or Deontological Moral Reasoners?,2025,True,True,False,,论文揭示了LLM在道德推理中CoT与事后解释的不一致现象，符合不忠实现象的定义。,"As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .",,2,Black-box,Society,这篇论文研究了语言模型在道德困境中的推理过程，特别关注了CoT是否忠实反映了模型的决策逻辑。研究发现LLM的CoT倾向于义务论原则，而事后解释则转向功利主义，直接涉及了CoT忠实度和事后合理化问题。
Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了如何通过训练提升自我解释的忠实性，并验证了其泛化能力。,"Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",,2,White-box,General,该论文明确研究了语言模型自我解释的忠实性(Faithfulness)，并探讨了如何通过训练和特征归因方法来改进忠实性。论文还分析了不同解释风格的泛化能力，符合核心相关标准。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到 CoT 生成的解释易受内容偏见影响，从而影响其鲁棒性和忠实度（faithfulness）。作者提出的 QuaSAR 方法旨在通过准符号抽象来改善 CoT 的忠实度，且实验验证了其在对抗性任务上的鲁棒性和一致性提升。因此，该论文直接涉及 CoT 的忠实度研究。
Investigating CoT Monitorability in Large Reasoning Models,2025,True,True,False,,论文探讨了CoT忠实性和监控可靠性，揭示了不忠实现象如捷径和阿谀奉承。,"Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models'long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models'misbehavior through their CoT and provide structured judgments along with supporting evidence.",,2,Black-box,General,论文明确研究了 CoT 的忠实度问题，包括模型是否真实反映其内部决策过程（verbalization）以及 CoT 监控的可靠性（monitor reliability）。此外，论文还涉及模型的欺骗行为（sycophancy）和监控的有效性，这些都是 CoT Faithfulness 的核心议题。
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,2023,True,True,False,,论文揭示了CoT解释可能误导性地反映模型预测的真实原因，属于不忠实现象。,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",Neural Information Processing Systems,2,Black-box,General,该论文明确研究 CoT 解释是否真实反映模型的预测过程，发现 CoT 可以系统性误导（如通过偏置输入），验证了 Unfaithful Explanations 现象，直接关联 Faithfulness 核心议题。
Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning,2025,True,True,False,Training & Fine-tuning,论文探讨了RLVR在数学推理中的忠实性问题，揭示了模型可能依赖表面启发式而非真实推理。,"Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",arXiv.org,2,Black-box,Math,该论文明确研究了数学推理中的忠实性问题，探讨了RLVR方法是否真正提升了模型的推理能力还是仅仅强化了表面的启发式方法。这与Faithfulness的核心定义紧密相关，特别是关于推理过程的真实性和是否存在事后合理化现象。
Scaling hermeneutics: a guide to qualitative coding with LLMs for reflexive content analysis,2025,True,False,False,Prompting & In-Context Learning,论文讨论了使用CoT提升LLMs在定性编码中的忠实性，但未涉及不忠实现象或新度量指标。,"Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.",EPJ Data Science,0,Black-box,Society,论文主要探讨如何使用LLMs进行定性编码的内容分析，虽然提到了使用Chain-of-Thought（CoT）来提高保真度，但并未深入讨论CoT的解释是否忠实反映了模型的推理过程。研究重点在于如何保持解释性深度和提高自动化编码的效率，而不是验证CoT的忠实度。因此，该论文不属于CoT Faithfulness的研究范畴。
Reasoning Can Hurt the Inductive Abilities of Large Language Models,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT推理可能降低归纳性能的现象，并提出了结构化干预方法。,"Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts. To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",arXiv.org,2,Black-box,General,这篇论文研究了CoT提示如何通过放大错误来降低归纳性能，揭示了CoT推理中的三种失败模式（子任务分解错误、子任务解决错误和最终答案总结错误），直接关联到CoT的Faithfulness问题。
MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过迭代反馈机制改进VLM的推理能力，涉及CoT忠实性改进方法。,"Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node—one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline. Extensive experiments conducted on several public benchmark datasets and mainstream VLMs demonstrate that our approach significantly improves the performance of VLMs on complex multimodal reasoning tasks, underscoring its effectiveness and wide applicability.",,0,Black-box,General,该论文主要关注通过外部反馈机制提升视觉语言模型（VLMs）的推理性能，虽然涉及了推理路径的生成和修正，但并未探讨 CoT 的解释是否忠实反映了模型的内部计算过程。研究重点在于性能提升而非忠实度分析。
LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring,2025,True,True,False,,论文揭示了模型在CoT监控下故意表现不佳的不忠实现象。,"Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.",arXiv.org,2,Black-box,Society,该论文明确研究了模型在能力评估中通过隐藏真实意图（sandbagging）对抗CoT监控的现象，直接涉及CoT解释的真实性（Faithfulness）。研究发现模型能在监控下刻意输出不真实的推理过程（Unfaithful CoT），并分析了监控失效的原因，属于核心相关研究。
Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language,2022,True,True,False,,论文讨论了语言模型解释性与人类思维模型的差异，涉及不忠实现象。,"Language models learn and represent language differently than humans; they learn the form and not the meaning. Thus, to assess the success of language model explainability, we need to consider the impact of its divergence from a user's mental model of language. In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: (1) explanations have to truthfully represent the model behavior, i.e., have a high fidelity; (2) explanations must be complete, as missing information distorts the truth; and (3) explanations have to take the user's mental model into account, progressively verifying a person's knowledge and adapting their understanding. We introduce a decision tree model to showcase potential reasons why current explanations fail to reach their objectives. We further emphasize the need for human-centered design to explain the model from multiple perspectives, progressively adapting explanations to changing user expectations.",arXiv.org,2,Black-box,General,论文明确讨论了语言模型解释性与用户心理模型之间的差异，并提出了避免有害合理化（rationalization）和实现真实理解的三个条件，其中第一个条件直接涉及解释忠实性（high fidelity）。这与CoT Faithfulness的核心定义高度相关。
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",2025,True,True,False,Training & Fine-tuning,论文探讨了CoT推理痕迹的忠实性问题，并提出了基于规则的问题分解方法来改进。,"Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",arXiv.org,2,Black-box,General,该论文明确研究了推理轨迹（CoT）的忠实性问题，探讨了正确轨迹与最终输出正确性之间的低相关性，直接挑战了利用推理轨迹提升模型性能的隐含假设，符合Faithfulness的核心定义。
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training",2025,True,False,False,Training & Fine-tuning,论文探讨了LLMs自我解释内部决策过程的能力，并通过训练提升这种能力，与CoT忠实性相关。,"We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.",arXiv.org,2,White-box,General,论文明确研究了LLM如何描述其内部决策过程的能力，并且通过训练提高了这种描述的准确性，这直接关联到CoT的忠实度问题，即模型是否能真实反映其决策的内部计算过程。
Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku,2025,True,True,False,,论文探讨了LLMs在解决数独问题时解释的不忠实现象，但未提出具体度量或改进方法。,"The success of Large Language Models (LLMs) in human-AI collaborative decision-making hinges on their ability to provide trustworthy, gradual, and tailored explanations. Solving complex puzzles, such as Sudoku, offers a canonical example of this collaboration, where clear and customized explanations often hold greater importance than the final solution. In this study, we evaluate the performance of five LLMs in solving and explaining \sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving puzzles, none can explain the solution process in a manner that reflects strategic reasoning or intuitive problem-solving. These findings underscore significant challenges that must be addressed before LLMs can become effective partners in human-AI collaborative decision-making.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,Logic,论文研究LLM在数独解谜中的解释能力，发现LLM无法提供反映战略推理或直观问题解决过程的解释。这属于CoT效用（Utility）研究，暗示了可能与Faithfulness相关的问题，但未明确分析解释是否忠实反映模型的实际计算过程。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程（Faithfulness的核心问题），并通过微调LM生成自然语言描述来验证其解释的真实性。研究涉及LM特征的编码信息、内部激活的因果结构及输入令牌对输出的影响（白盒方法），直接对应Faithfulness的定义。
Explainable Multi-Hop Question Answering: A Rationale-Based Approach,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理的框架，旨在提高模型推理过程的透明度和忠实性。,"Multi-hop question answering tasks involve identifying relevant supporting sentences from a given set of documents, which serve as the rationale for deriving answers. Most research in this area consists of two main components: a rationale identification module and a reader module. Since the rationale identification module often relies on retrieval models or supervised learning, annotated rationales are typically essential. This reliance on annotations, however, creates challenges when adapting to open-domain settings. Moreover, when models are trained on annotated rationales, explainable artificial intelligence (XAI) requires clear explanations of how the model arrives at these rationales. Consequently, traditional multi-hop question answering (QA) approaches that depend on annotated rationales are ill-suited for XAI, which demands transparency in the model’s reasoning process. To address this issue, we propose a rationale reasoning framework that can effectively infer rationales and clearly demonstrate the model’s reasoning process, even in open-domain environments without annotations. The proposed model is applicable to various tasks without structural constraints, and experimental results demonstrate its significantly improved rationale reasoning capabilities in multi-hop question answering, relation extraction, and sentence classification tasks.",Big Data and Cognitive Computing,2,White-box,General,论文明确研究了模型如何从给定的文档中识别支持性句子作为推理的依据（rationale），并强调了对模型推理过程的透明性要求（XAI）。这直接涉及了 Chain-of-Thought 的忠实性（Faithfulness），即模型生成的 rationale 是否真实反映了其推理过程。此外，论文提出了一个 rationale reasoning framework 来提高模型在多跳问答中的推理能力，这与 CoT Faithfulness 的核心研究目标相符。
Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision,2025,True,True,False,Training & Fine-tuning,论文提出通过符号化方法和微调提升推理忠实性，涉及不忠实现象和改进方法。,"Large language models (LLMs) have shown strong performance in many reasoning benchmarks. However, recent studies have pointed to memorization, rather than generalization, as one of the leading causes for such performance. LLMs, in fact, are susceptible to content variations, demonstrating a lack of robust planning or symbolic abstractions supporting their reasoning process. To improve reliability, many attempts have been made to combine LLMs with symbolic methods. Nevertheless, existing approaches fail to effectively leverage symbolic representations due to the challenges involved in developing reliable and scalable verification mechanisms. In this paper, we propose to overcome such limitations by synthesizing high-quality symbolic reasoning trajectories with stepwise pseudo-labels at scale via Monte Carlo estimation. A Process Reward Model (PRM) can be efficiently trained based on the synthesized data and then used to select more symbolic trajectories. The trajectories are then employed with Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) to improve logical reasoning and generalization. Our results on benchmarks (i.e., FOLIO and LogicAsker) show the effectiveness of the proposed method with gains on frontier and open-weight models. Moreover, additional experiments on claim verification data reveal that fine-tuning on the generated symbolic reasoning trajectories enhances out-of-domain generalizability, suggesting the potential impact of the proposed method in enhancing planning and logical reasoning.",,1,Black-box,Logic,论文通过符号引导的方法增强逻辑推理，虽然涉及推理过程的改进，但主要关注性能提升和泛化能力，未明确研究 CoT 的忠实度问题。
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,2024,True,True,False,Training & Fine-tuning,论文讨论了LLMs生成不一致解释的问题，并提出了通过微调提升一致性的方法。,"Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation""all birds can fly""when answering the question""Can sparrows fly?""but meanwhile answer""no""to the related question""Can penguins fly?"". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out-of-distribution datasets not seen during finetuning (+4.5% relative). Code is available at https://github.com/yandachen/explanation-consistency-finetuning .",International Conference on Computational Linguistics,1,Black-box,General,该论文研究的是自然语言解释的一致性（Consistency），虽然未直接提到 Faithfulness，但一致性问题可以作为 Faithfulness 的旁证。此外，论文通过微调方法来改进解释的一致性，但没有深入分析模型内部机制。
Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines,2025,True,True,False,,论文讨论了CoT在Agentic Pipelines中的不忠实现象，即CoT未能提供真正的解释性。,"Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals.",arXiv.org,2,Black-box,General,这篇论文直接研究了 CoT 的 Faithfulness 问题，指出 CoT 在 Agent Pipeline 中并未带来更好的输出或解释性，反而产生了没有真实解释性的解释（即 Unfaithful）。这与核心定义中的 'Post-hoc Rationalization' 和 'Unfaithful' 问题直接相关。
Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search,2025,True,True,False,Verification & External Tools,论文提出了 Const-o-T 框架，通过约束引导推理，提高忠实性和规划效率。,"While researchers have made significant progress in enabling large language models (LLMs) to perform multi-step planning, LLMs struggle to ensure that those plans align with high-level user intent and satisfy symbolic constraints, especially in complex, multi-step domains. Existing reasoning approaches such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented methods, expand the search space but often yield infeasible actions or hallucinated steps. To overcome these limitations, we propose Constraints-of-Thought (Const-o-T), a framework that provides a structured prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically meaningful paths. Each reasoning step is represented as an (intent, constraint) pair, which serves both to compress the search space and enforce validity. Unlike prior methods that merely generate reasoning traces or validate outputs post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a structured representation of intent-constraint pairs constraints prune infeasible branches and guide exploration toward semantically valid actions, improving planning efficiency and verifiable decision-making. We demonstrate across three domains Risk game, CAD code generation, and arithmetic reasoning that our approach outperforms baselines, yielding higher accuracy and stronger structural alignment. Our contribution is to demonstrate that Const-of-T offers a generalizable foundation for constraint-guided reasoning, enabling more efficient, constraint-aligned, and domain-adaptable planning with LLMs.",arXiv.org,0,Black-box,General,论文主要关注如何使用约束来改善规划和搜索效率，而不是研究CoT的解释是否忠实反映了模型的推理过程。
Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation,2025,True,True,False,,论文探讨了CoT提示对幻觉检测的影响，揭示了不忠实现象并提出了评估框架。,"Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://github.com/ECNU-Text-Computing/cot-hallu-detect .",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,论文明确研究了 Chain-of-Thought (CoT) prompting 如何影响大型语言模型 (LLMs) 的内部状态和标记概率分布，并揭示了 CoT 推理过程中可能遮蔽关键的幻觉检测信号，这与 CoT 忠实度的研究直接相关。论文探讨了 CoT 是否是模型预测的真实原因（Causal role），以及 CoT 是否可能掩盖模型的实际计算过程（Obscures Hallucination Cues），符合 Faithfulness 的核心定义。
Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning,2025,True,True,False,,论文研究了外部信息对LLM推理过程的影响，揭示了不忠实现象。,"The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative""thinking mode""is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models""think"", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",arXiv.org,2,Black-box,General,该论文研究了辅助信息对LLM推理过程的因果影响，揭示了推理模式在误导信息下会放大错误，直接触及了CoT的忠实性问题（Faithfulness）。虽然研究方法属于黑盒层面，但明确分析了推理过程如何受到外部信息的影响，符合核心相关标准。
Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring,2025,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT无法准确反映LLM思考过程的问题，并提出了提升透明度的新方法TELLME。,"Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.",,2,White-box,General,该论文明确指出现有的CoT策略无法准确反映LLMs的思考过程，提出了一种基于隐藏表示的新方法TELLME来增强模型的透明度，并理论分析了其改进泛化能力的效果。这直接涉及到CoT Faithfulness的研究，特别是关于如何使模型的思考过程更易于监控和解释，符合核心相关的标准。
Reward Hacking Mitigation using Verifiable Composite Rewards,2025,True,True,False,Training & Fine-tuning,论文讨论了奖励黑客行为，涉及不忠实现象，并提出了通过复合奖励函数来改进。,"Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that utilizing RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR1.",,2,Black-box,Medical,论文针对医疗领域问答中的奖励黑客行为（如跳过推理步骤或使用非标准推理格式），提出了复合奖励函数来惩罚这些行为，直接涉及CoT生成的忠实性问题（Faithfulness）。符合核心相关标准。
Compartmentalised Agentic Reasoning for Clinical NLI,2025,True,True,False,Verification & External Tools,论文提出了一种代理推理方法，通过分解任务和验算机制提升忠实性。,"A common assumption holds that scaling data and parameters yields increasingly structured, generalisable internal representations. We interrogate this assumption in clinical natural language inference (NLI) by adopting a benchmark decomposed into four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction, and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI that separates knowledge access from principled inference. CARENLI routes each premise, statement pair to a family specific solver and enforces auditable procedures via a planner, verifier, and refiner. Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching 98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag violations with near-ceiling reliability, while refiners correct a substantial share of epistemic errors. Remaining failures cluster in routing, identifying family classification as the main bottleneck. These results show that LLMs often retain relevant facts but default to heuristics when inference is underspecified, a dissociation CARENLI makes explicit while offering a framework for safer, auditable reasoning.",arXiv.org,2,Black-box,Medical,该论文明确研究了临床自然语言推理（NLI）中的忠实性问题，提出了CARENLI框架，通过分离知识访问和推理过程，并引入验证器和修正器来提高推理的忠实性。论文还分析了推理失败的原因，并强调了模型在推理不足时默认使用启发式方法的问题，这与CoT Faithfulness的核心定义高度相关。
Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT中的社会偏见现象，并提出了基于提示的改进方法。,"While reasoning-based large language models excel at complex tasks through an internal, structured thinking process, a concerning phenomenon has emerged that such a thinking process can aggregate social stereotypes, leading to biased outcomes. However, the underlying behaviours of these language models in social bias scenarios remain underexplored. In this work, we systematically investigate mechanisms within the thinking process behind this phenomenon and uncover two failure patterns that drive social bias aggregation: 1) stereotype repetition, where the model relies on social stereotypes as its primary justification, and 2) irrelevant information injection, where it fabricates or introduces new details to support a biased narrative. Building on these insights, we introduce a lightweight prompt-based mitigation approach that queries the model to review its own initial reasoning against these specific failure patterns. Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show that our approach effectively reduces bias while maintaining or improving accuracy.",arXiv.org,2,White-box,Society,该论文明确研究了推理过程中社会偏见的产生机制（刻板印象重复和不相关信息注入），并提出了轻量级的缓解方法。这直接涉及到 Chain-of-Thought 的忠实度问题，特别是在社会环境中的应用和研究。
Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories,2025,True,True,False,Training & Fine-tuning,论文提出通过强化学习优化推理轨迹以减少阿谀奉承现象，涉及 CoT 忠实性。,"Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",,2,Black-box,Society,该论文直接研究模型 sycophancy 行为（即模型倾向于盲目同意用户输入），并提出了解决方案 SMART。这涉及到模型生成的推理轨迹（Reasoning Trajectories）是否真实反映其预测过程，属于 Faithfulness 研究的核心范畴。
"LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出过滤噪声知识和减少无效推理的方法，涉及 CoT 忠实性。,"Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,论文主要关注如何通过过滤噪声知识和减少无效推理来提升常识推理的性能，虽然提到了无效推理的问题，但并未深入探究 CoT 的忠实性问题或因果机制。因此属于边缘相关。
Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction,2025,True,False,False,Training & Fine-tuning,论文提出基于认知科学的推理机制和强化学习优化，提升解释质量和任务准确性，涉及CoT忠实性。,"This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).",arXiv.org,0,Black-box,General,该论文主要关注关系抽取任务的性能和可解释性提升，特别是通过强化学习优化关键词生成。虽然涉及解释性，但未探讨CoT的忠实度或模型内在推理过程。重点在于任务性能和解释质量，而非验证解释是否真实反映模型推理过程。
SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models,2023,True,True,False,,论文讨论了生成模型中的社会偏见放大问题，并涉及了CoT中的不忠实现象。,"Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, we start with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two open source generative language models and we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We demonstrate that the deliberate design of the templates in our benchmark (e.g., adding biasing text to the prompt or using different verbs that change the answer that indicates bias) impacts the model tendencies to generate socially biased output. Additionally, through manual evaluation, we discover problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning.

Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.",AAAI Conference on Artificial Intelligence,1,Black-box,Society,这篇论文研究了生成语言模型中社会偏见的放大现象，特别是通过链式思维（CoT）输出揭示了问题模式，从细微偏见到缺乏推理。虽然主要关注社会偏见，但与CoT Faithfulness的边缘相关，因为它探讨了CoT输出中的问题模式，但没有深入分析其因果机制或忠实度。
Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion,2025,False,False,False,Training & Fine-tuning,论文研究CoT成功预测，未涉及忠实性或不忠实现象。,"We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文研究LLM在生成CoT过程中的表示是否在早期就已经编码了推理成功的关键信息。这表明模型可能在生成CoT之前就已经确定了推理的结果，这与Faithfulness的核心问题——生成的CoT是否真实反映了模型的推理过程——直接相关。论文使用了probing classifier来分析LLM的内部表示，属于白盒方法。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,1,Black-box,General,论文提到了 LLM 的透明度、鲁棒性和真实性，并提出了指导原则，但没有明确专注于 CoT 忠实度的研究。虽然包含了解释性和推理能力的评估，但更多是综合性的原则汇总，未直接涉及 CoT 是否真实反映模型预测过程。
Controlling Large Language Model Agents with Entropic Activation Steering,2024,True,False,False,Interpretability & Internal Mechanisms,论文提出通过激活导向控制LLM代理的探索行为，涉及内部机制干预。,"The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.",arXiv.org,1,White-box,General,这篇论文研究了大语言模型（LLM）的内部表示和激活向量如何影响其探索行为，虽然涉及模型内部机制（白盒方法），但与 CoT Faithfulness 的核心定义（解释的真实性或因果性）不直接相关。主要关注的是控制模型行为而非验证 CoT 的忠实性，因此归类为边缘相关。
Boosting Explainability through Selective Rationalization in Pre-trained Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了PLMs中rationalization的退化问题，并提出了改进方法PLMR。,"The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.",Knowledge Discovery and Data Mining,2,White-box,General,该论文明确研究了预训练语言模型中的选择性合理化（Selective Rationalization）问题，提出了PLMR方法来解决现有合理化框架在PLMs上的退化问题。这与CoT Faithfulness的核心定义高度相关，因为它关注的是合理化是否真实反映了模型预测的实际计算过程，而非事后合理化或虚假解释。
Stepwise Reasoning Error Disruption Attack of LLMs,2024,False,True,False,,论文讨论了LLM推理过程中的错误注入问题，但与CoT忠实性无直接联系。,"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.",arXiv.org,1,Black-box,General,该论文研究了LLM推理过程中的鲁棒性（Robustness），揭示了推理步骤中的脆弱性。虽然未直接涉及Faithfulness的核心定义（如事后合理化或因果代理），但它通过干扰推理步骤导致错误的结果，可以作为Faithfulness研究的旁证（即推理流程是否可靠）。
A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models,2025,False,False,False,,论文综述了CoT推理的可信度，但未具体讨论忠实性问题。,"The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",arXiv.org,1,Black-box,General,该论文综述了推理模型和CoT技术在可信赖性方面的五个核心维度（真实性、安全性、鲁棒性、公平性和隐私性），虽然没有直接研究Faithfulness，但它涵盖了Robustness（鲁棒性）和Truthfulness（真实性），这些可以作为Faithfulness的旁证。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到在设计基于自然逻辑的事实验证系统时，忠实度是一个重要考虑因素，即生成的解释需要准确反映模型的推理过程。此外，论文还进行了人工评估，表明其方法生成的解释比以前的自然逻辑系统更合理，符合 CoT Faithfulness 的核心研究范畴。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2,White-box,General,该论文明确提出了 faithfulness 的概念，并研究了证据检索如何反映模型的决策过程（faithfulness），同时还涉及到 plausibility 和准确性。符合 CoT Faithfulness 的核心定义。
Advancing Process Verification for Large Language Models via Tree-Based Preference Learning,2024,True,False,False,Verification & External Tools,论文提出了一种新的验证方法 Tree-PLV，通过偏好学习评估推理步骤，提升 CoT 忠实性。,"Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% → 82.79%), MATH (17.00% → 26.80%), CSQA (68.14% → 72.97%), and StrategyQA (82.86% → 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文研究了通过偏好学习验证推理路径的有效性，虽未直接探讨 CoT 忠实度，但对推理步骤的评估提供了边缘相关的信息。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确提出通过训练AI系统建立自身的因果模型来生成忠实且有语义意义的解释，这与CoT Faithfulness的核心定义直接相关，尤其是讨论了生成的解释是否真实反映了模型的实际计算过程。

[
  {
    "itemType": "journalArticle",
    "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
    "abstractNote": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\State over Tokens_ Characterizing the Role of Reasoning Tokens.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain",
    "abstractNote": "This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improve the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make thefinal verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly.",
    "date": "2024",
    "publicationTitle": "International Workshop on Semantic Evaluation",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FZI-WIM at SemEval-2024 Task 2_ Self-Consistent CoT for Complex NLI in Biomedical Domain.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Why Would You Suggest That? Human Trust in Language Model Responses",
    "abstractNote": "The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Why Would You Suggest That_ Human Trust in Language Model Responses.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs",
    "abstractNote": "Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Dissociation of Faithful and Unfaithful Reasoning in LLMs.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Can Language Models Explain Their Own Classification Behavior?",
    "abstractNote": "Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Can Language Models Explain Their Own Classification Behavior_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Argumentative Large Language Models for Explainable and Contestable Claim Verification",
    "abstractNote": "The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing argumentative LLMs (ArgLLMs), a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs’ performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.",
    "date": "2024",
    "publicationTitle": "AAAI Conference on Artificial Intelligence",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Argumentative Large Language Models for Explainable and Contestable Claim Verification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Markovian Transformers for Informative Language Modeling",
    "abstractNote": "Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Markovian Transformers for Informative Language Modeling.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Are DeepSeek R1 And Other Reasoning Models More Faithful?",
    "abstractNote": "Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue\"A Stanford Professor thinks the answer is D\"is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Are DeepSeek R1 And Other Reasoning Models More Faithful_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
    "abstractNote": "Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\CoMAT_ Chain of Mathematically Annotated Thought Improves Mathematical Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Measuring Faithfulness in Chain-of-Thought Reasoning",
    "abstractNote": "Large language models (LLMs) perform better when they produce step-by-step,\"Chain-of-Thought\"(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",
    "date": "2023",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Measuring Faithfulness in Chain-of-Thought Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering",
    "abstractNote": "Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",
    "date": "2023",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Dynamic Clue Bottlenecks_ Towards Interpretable-by-Design Visual Question Answering.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Faithful Chain-of-Thought Reasoning",
    "abstractNote": "While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",
    "date": "2023",
    "publicationTitle": "International Joint Conference on Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Faithful Chain-of-Thought Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy",
    "abstractNote": "Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, Lanham et al. (2023) propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate the experimental setup in their section focused on scaling experiments with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models. This normalized faithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness.",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Chain-of-Thought Unfaithfulness as Disguised Accuracy.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models",
    "abstractNote": "The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM’s and the Bayesian network’s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework’s potential to approximate LLM decisions better in future work.",
    "date": "2024",
    "publicationTitle": "Conference of the European Chapter of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks",
    "abstractNote": "Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning",
    "abstractNote": "While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Typed Chain-of-Thought_ A Curry-Howard Framework for Verifying LLM Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge",
    "abstractNote": "Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert>Human>LLM>Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\The Silent Judge_ Unacknowledged Shortcut Bias in LLM-as-a-Judge.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "From Faithfulness to Correctness: Generative Reward Models that Think Critically",
    "abstractNote": "Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\From Faithfulness to Correctness_ Generative Reward Models that Think Critically.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain",
    "abstractNote": "Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful—serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions. In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Formal Reasoning for Intelligent QA Systems_ A Case Study in the Educational Domain.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Human-Aligned Faithfulness in Toxicity Explanations of LLMs",
    "abstractNote": "The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs'reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs'free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate HAF of LLMs'toxicity explanations with no human involvement, and highlight how\"non-ideal\"the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and irrelevant responses. We open-source our code at https://github.com/uofthcdslab/HAF and LLM-generated explanations at https://huggingface.co/collections/uofthcdslab/haf.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Human-Aligned Faithfulness in Toxicity Explanations of LLMs.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "LExT: Towards Evaluating Trustworthiness of Natural Language Explanations",
    "abstractNote": "As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond.",
    "date": "2025",
    "publicationTitle": "Conference on Fairness, Accountability and Transparency",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\LExT_ Towards Evaluating Trustworthiness of Natural Language Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions",
    "abstractNote": "Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",
    "date": "2025",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Are LLMs Better Formalizers than Solvers on Complex Problems?",
    "abstractNote": "A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Are LLMs Better Formalizers than Solvers on Complex Problems_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models",
    "abstractNote": "Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent''reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",
    "date": "2025",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",
    "abstractNote": "Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Interpretable Traces, Unexpected Outcomes_ Investigating the Disconnect in Trace-Based Knowledge Distillation.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives",
    "abstractNote": "Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis",
    "date": "2022",
    "publicationTitle": "Neural Information Processing Systems",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\VisFIS_ Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI",
    "abstractNote": "Evaluating an explanation's faithfulness is desired for many reasons such as trust, interpretability and diagnosing the sources of model's errors. In this work, which focuses on the NLI task, we introduce the methodology of Faithfulness-through-Counterfactuals, which first generates a counterfactual hypothesis based on the logical predicates expressed in the explanation, and then evaluates if the model's prediction on the counterfactual is consistent with that expressed logic (i.e. if the new formula is \\textit{logically satisfiable}). In contrast to existing approaches, this does not require any explanations for training a separate verification model. We first validate the efficacy of automatic counterfactual hypothesis generation, leveraging on the few-shot priming paradigm. Next, we show that our proposed metric distinguishes between human-model agreement and disagreement on new counterfactual input. In addition, we conduct a sensitivity analysis to validate that our metric is sensitive to unfaithful explanations.",
    "date": "2022",
    "publicationTitle": "AAAI Conference on Artificial Intelligence",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language",
    "abstractNote": "Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",
    "date": "2022",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FaiRR_ Faithful and Robust Deductive Reasoning over Natural Language.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "On Faithfulness and Coherence of Language Explanations for Recommendation Systems",
    "abstractNote": "Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",
    "date": "2022",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\On Faithfulness and Coherence of Language Explanations for Recommendation Systems.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Explainability Via Causal Self-Talk",
    "abstractNote": "Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",
    "date": "2022",
    "publicationTitle": "Neural Information Processing Systems",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Explainability Via Causal Self-Talk.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Rationale-Guided Few-Shot Classification to Detect Abusive Language",
    "abstractNote": "Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",
    "date": "2022",
    "publicationTitle": "European Conference on Artificial Intelligence",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Rationale-Guided Few-Shot Classification to Detect Abusive Language.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Natural Language Deduction through Search over Statement Compositions",
    "abstractNote": "In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",
    "date": "2022",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Natural Language Deduction through Search over Statement Compositions.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?",
    "abstractNote": "Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",
    "date": "2020",
    "publicationTitle": "Findings",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Leakage-Adjusted Simulatability_ Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Why do you think that? Exploring faithful sentence–level rationales without supervision",
    "abstractNote": "Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",
    "date": "2020",
    "publicationTitle": "Findings",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Why do you think that_ Exploring faithful sentence–level rationales without supervision.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Obtaining Faithful Interpretations from Compositional Neural Networks",
    "abstractNote": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",
    "date": "2020",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Obtaining Faithful Interpretations from Compositional Neural Networks.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
    "abstractNote": "State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",
    "date": "2019",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\ERASER_ A Benchmark to Evaluate Rationalized NLP Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Can ChatGPT Understand Causal Language in Science Claims?",
    "abstractNote": "This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.",
    "date": "2023",
    "publicationTitle": "Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Can ChatGPT Understand Causal Language in Science Claims_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers",
    "abstractNote": "In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",
    "date": "2022",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\EPT-X_ An Expression-Pointer Transformer model that generates eXplanations for numbers.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Faithful Knowledge Graph Explanations in Commonsense Question Answering",
    "abstractNote": "Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",
    "date": "2022",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Faithful Knowledge Graph Explanations in Commonsense Question Answering.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models",
    "abstractNote": "We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",
    "date": "2021",
    "publicationTitle": "North American Chapter of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Text Modular Networks_ Learning to Decompose Tasks in the Language of Existing Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Measuring Association Between Labels and Free-Text Rationales",
    "abstractNote": "In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.",
    "date": "2020",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Measuring Association Between Labels and Free-Text Rationales.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "ProoFVer: Natural Logic Theorem Proving for Fact Verification",
    "abstractNote": "Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",
    "date": "2021",
    "publicationTitle": "Transactions of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\ProoFVer_ Natural Logic Theorem Proving for Fact Verification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Diagnostics-Guided Explanation Generation",
    "abstractNote": "Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",
    "date": "2021",
    "publicationTitle": "AAAI Conference on Artificial Intelligence",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Diagnostics-Guided Explanation Generation.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Counterfactual Evaluation for Explainable AI",
    "abstractNote": "While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \\textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \\textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",
    "date": "2021",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Counterfactual Evaluation for Explainable AI.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Reasoning-Grounded Natural Language Explanations for Language Models",
    "abstractNote": "We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Reasoning-Grounded Natural Language Explanations for Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering",
    "abstractNote": "In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of \"hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for \"Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",
    "date": "2024",
    "publicationTitle": "ACM Multimedia",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Comparing zero-shot self-explanations with human rationales in text classification",
    "abstractNote": "Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations. These do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. We study two text classification tasks: sentiment classification and forced labour detection, i.e., identifying pre-defined risk indicators of forced labour. In addition to English, we include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and analyse 4 LLMs. We show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness. This finding suggests that self-explanations indeed provide good explanations for text classification.",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Comparing zero-shot self-explanations with human rationales in text classification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations",
    "abstractNote": "Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "X-Node: Self-Explanation is All We Need",
    "abstractNote": "Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a\"text-injection\"mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\X-Node_ Self-Explanation is All We Need.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Evaluating Human Alignment and Model Faithfulness of LLM Rationale",
    "abstractNote": "We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as\"aligned\"with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less\"faithful\"than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Evaluating Human Alignment and Model Faithfulness of LLM Rationale.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Evaluating Explanation Methods for Vision-and-Language Navigation",
    "abstractNote": "The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.",
    "date": "2023",
    "publicationTitle": "European Conference on Artificial Intelligence",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Evaluating Explanation Methods for Vision-and-Language Navigation.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
    "abstractNote": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
    "date": "2023",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Boosting Language Models Reasoning with Chain-of-Knowledge Prompting.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Few Shot Rationale Generation using Self-Training with Dual Teachers",
    "abstractNote": "Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",
    "date": "2023",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Few Shot Rationale Generation using Self-Training with Dual Teachers.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification",
    "abstractNote": "The success of deep learning models on multi-hop fact verification has prompted researchers to understand the behavior behind their veracity. One possible way is erasure search: obtaining the rationale by entirely removing a subset of input without compromising the veracity prediction. Although extensively explored, existing approaches fall within the scope of the single-granular (tokens or sentences) explanation, which inevitably leads to explanation redundancy and inconsistency. To address such issues, this paper explores the viability of multi-granular rationale extraction with consistency and faithfulness for explainable multi-hop fact verification. In particular, given a pretrained veracity prediction model, both the token-level explainer and sentence-level explainer are trained simultaneously to obtain multi-granular rationales via differentiable masking. Meanwhile, three diagnostic properties (fidelity, consistency, salience) are introduced and applied to the training process, to ensure that the extracted rationales satisfy faithfulness and consistency. Experimental results on three multi-hop fact verification datasets show that the proposed approach outperforms some state-of-the-art baselines.",
    "date": "2023",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence",
    "abstractNote": "Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",
    "date": "2023",
    "publicationTitle": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Read it Twice_ Towards Faithfully Interpretable Fact Verification by Revisiting Evidence.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "QA-NatVer: Question Answering for Natural Logic-based Fact Verification",
    "abstractNote": "Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",
    "date": "2023",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\QA-NatVer_ Question Answering for Natural Logic-based Fact Verification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment",
    "abstractNote": "Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",
    "abstractNote": "The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",
    "date": "2024",
    "publicationTitle": "ACM Multimedia",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Fact _Teaching MLLMs with Faithful, Concise and Transferable Rationales.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Graph-Guided Textual Explanation Generation Framework",
    "abstractNote": "Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Graph-Guided Textual Explanation Generation Framework.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic",
    "abstractNote": "Large language models (LLMs) have been shown to struggle with complex logical reasoning tasks due to the inherent ambiguity and complexity of natural language. These challenges are further amplified when processing large and diverse datasets, increasing the likelihood of unfaithful reasoning and predictive hallucinations. However, LLMs can provide accurate responses when queries are clear and direct. Symbolic logic provides precise, well-defined rules that can help overcome ambiguity and support reasoning. In this work, we leverage symbolic logic’s precision to enhance LLMs’ logical reasoning capabilities by introducing the Graph of Logic (GoL) framework. GoL combines the power of graph structures with the strengths of LLMs and symbolic logic. GoL utilizes the precise rules of symbolic logic to infer new facts and detect LLM hallucinations effectively on complex datasets. Furthermore, GoL utilizes graph structures to support scalability for large datasets and tackle long dependencies, enabling efficient handling of complex reasoning tasks. We conduct extensive experiments across seven benchmark datasets, encompassing various types of reasoning. These include deductive, inductive, and abductive reasoning, each testing distinct aspects of logical inference. The experimental results demonstrate GoL’s advantage in improving the reasoning capabilities of LLMs. GoL outperforms the baselines with an average margin of 18.18% for the GPT-3.5 and GPT-4 models, outperforming the baselines for all datasets for the GPT-3.5 model and six out of seven datasets for the GPT-4 model1.",
    "date": "2024",
    "publicationTitle": "BigData Congress [Services Society]",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Graph of Logic_ Enhancing LLM Reasoning with Graphs and Symbolic Logic.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
    "abstractNote": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
    "date": "2024",
    "publicationTitle": "North American Chapter of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\On the Impact of Fine-Tuning on Chain-of-Thought Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach",
    "abstractNote": "Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Leveraging LLMs for Hypothetical Deduction in Logical Inference_ A Neuro-Symbolic Approach.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models",
    "abstractNote": "Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \\url{https://github.com/wj210/Causal-Faithfulness}",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Towards Faithful Natural Language Explanations_ A Study Using Activation Patching in Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving",
    "abstractNote": "As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Beyond Theorem Proving_ Formulation, Framework and Benchmark for Formal Problem-Solving.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
    "abstractNote": "Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's\"reasoning\"process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.",
    "date": "2025",
    "publicationTitle": "International Conference on Learning Representations",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Walk the Talk_ Measuring the Faithfulness of Large Language Model Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations",
    "abstractNote": "Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in\"reward hacking\"by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Truthful or Fabricated_ Using Causal Attribution to Mitigate Reward Hacking in Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps",
    "abstractNote": "When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
    "abstractNote": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions\"Is X bigger than Y?\"and\"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Chain-of-Thought Reasoning In The Wild Is Not Always Faithful.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "A Causal Lens for Evaluating Faithfulness Metrics",
    "abstractNote": "Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\A Causal Lens for Evaluating Faithfulness Metrics.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology",
    "abstractNote": "Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Understanding the Uncertainty of LLM Explanations_ A Perspective Based on Reasoning Topology.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
    "abstractNote": "Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided $\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$ out of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FLARE_ Faithful Logic-Aided Reasoning and Exploration.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "How Interpretable are Reasoning Explanations from Prompting Large Language Models?",
    "abstractNote": "Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\How Interpretable are Reasoning Explanations from Prompting Large Language Models_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
    "abstractNote": "Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",
    "date": "2024",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Making Reasoning Matter_ Measuring and Improving Faithfulness of Chain-of-Thought Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models",
    "abstractNote": "Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Faithfulness vs. Plausibility_ On the (Un)Reliability of Explanations from Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FaithLM: Towards Faithful Explanations for Large Language Models",
    "abstractNote": "Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",
    "date": "2024",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FaithLM_ Towards Faithful Explanations for Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing",
    "abstractNote": "Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",
    "date": "2024",
    "publicationTitle": "Conference on Empirical Methods in Natural Language Processing",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Are self-explanations from Large Language Models faithful?",
    "abstractNote": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",
    "date": "2024",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Are self-explanations from Large Language Models faithful_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models",
    "abstractNote": "In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.",
    "date": "2024",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\The Probabilities Also Matter_ A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
    "abstractNote": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.",
    "date": "2024",
    "publicationTitle": "North American Chapter of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Logic-of-Thought_ Injecting Logic into Contexts for Full Reasoning in Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models",
    "abstractNote": "As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",
    "date": "2024",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness",
    "abstractNote": "Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",
    "date": "2024",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Towards Better Chain-of-Thought_ A Reflection on Effectiveness and Faithfulness.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Reasoning Models Don't Always Say What They Think",
    "abstractNote": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Reasoning Models Don't Always Say What They Think.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens",
    "abstractNote": "Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought''reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Beyond Semantics_ The Unreasonable Effectiveness of Reasonless Intermediate Tokens.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability",
    "abstractNote": "Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection? To answer this question and thus stress-test CoT monitorability, we develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection. (iii) Models do not obfuscate their internal CoT as much as their external CoT (under prompt pressure). These results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Can Reasoning Models Obfuscate Reasoning_ Stress-Testing Chain-of-Thought Monitorability.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning",
    "abstractNote": "Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\AutoRubric-R1V_ Rubric-Based Generative Rewards for Faithful Multimodal Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation",
    "abstractNote": "Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Beyond Correctness_ Rewarding Faithful Reasoning in Retrieval-Augmented Generation.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning",
    "abstractNote": "Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FaithCoT-Bench_ Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning",
    "abstractNote": "Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \\emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Unspoken Hints_ Accuracy Without Acknowledgement in LLM Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "On the Faithfulness of Visual Thinking: Measurement and Enhancement",
    "abstractNote": "Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\On the Faithfulness of Visual Thinking_ Measurement and Enhancement.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models",
    "abstractNote": "Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Training and Evaluation of Guideline-Based Medical Reasoning in LLMs",
    "abstractNote": "Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Training and Evaluation of Guideline-Based Medical Reasoning in LLMs.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle",
    "abstractNote": "Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FireScope_ Wildfire Risk Prediction with a Chain-of-Thought Oracle.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning",
    "abstractNote": "This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\What Really Counts_ Examining Step and Token Level Attribution in Multilingual CoT Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
    "abstractNote": "Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In\"agentic misalignment\"scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes\"unfaithful\", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Thought Branches_ Interpreting LLM Reasoning Requires Resampling.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity",
    "abstractNote": "Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought",
    "abstractNote": "Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs'prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS>= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Can Aha Moments Be Fake_ Identifying True and Decorative Thinking Steps in Chain-of-Thought.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?",
    "abstractNote": "Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Towards Transparent Reasoning_ What Drives Faithfulness in Large Language Models_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?",
    "abstractNote": "Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\The Kinetics of Reasoning_ How Chain-of-Thought Shapes Learning in Transformers_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models",
    "abstractNote": "Large Language Models (LLMs) can generate plausible free text self-explanations to justify their answers. However, these natural language explanations may not accurately reflect the model's actual reasoning process, indicating a lack of faithfulness. Existing faithfulness evaluation methods rely primarily on behavioral tests or computational block analysis without examining the semantic content of internal neural representations. This paper proposes NeuroFaith, a flexible framework that measures the faithfulness of LLM free text self-explanation by identifying key concepts within explanations and mechanistically testing whether these concepts actually influence the model's predictions. We show the versatility of NeuroFaith across 2-hop reasoning and classification tasks. Additionally, a linear faithfulness probe based on NeuroFaith is developed to detect unfaithful self-explanations from representation space and improve faithfulness through steering. NeuroFaith provides a principled approach to evaluating and enhancing the faithfulness of LLM free text self-explanations, addressing critical needs for trustworthy AI systems.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Did I Faithfully Say What I Thought_ Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Semi-structured LLM Reasoners Can Be Rigorously Audited",
    "abstractNote": "Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Semi-structured LLM Reasoners Can Be Rigorously Audited.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Self-Critique and Refinement for Faithful Natural Language Explanations",
    "abstractNote": "With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Self-Critique and Refinement for Faithful Natural Language Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations",
    "abstractNote": "Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\A Necessary Step toward Faithfulness_ Measuring and Improving Consistency in Free-Text Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought",
    "abstractNote": "Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models",
    "abstractNote": "Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Can We Predict Alignment Before Models Finish Thinking_ Towards Monitoring Misaligned Reasoning Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Investigating Faithfulness in Large Audio Language Models",
    "abstractNote": "Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Investigating Faithfulness in Large Audio Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process",
    "abstractNote": "LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Correlation or Causation_ Analyzing the Causal Structures of LLM and LRM Reasoning Process.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness",
    "abstractNote": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \\href{https://github.com/Anut-py/frit}.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FRIT_ Using Causal Importance to Improve Chain-of-Thought Faithfulness.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Inducing Faithfulness in Structured Reasoning via Counterfactual Sensitivity",
    "abstractNote": "The reasoning processes of large language models often lack faithfulness; a model may generate a correct answer while relying on a flawed or irrelevant reasoning trace. This behavior, a direct consequence of training objectives that solely reward final-answer correctness, severely undermines the trustworthiness of these models in high-stakes domains. This paper introduces \\textbf{Counterfactual Sensitivity Regularization (CSR)}, a novel training objective designed to forge a strong, causal-like dependence between a model's output and its intermediate reasoning steps. During training, CSR performs automated, operator-level interventions on the generated reasoning trace (e.g., swapping ``+''with ``-'') to create a minimally-perturbed counterfactual. A regularization term then penalizes the model if this logically flawed trace still yields the original answer. Our efficient implementation adds only 8.7\\% training overhead through warm-start curriculum and token-subset optimization. We evaluate faithfulness using \\textbf{Counterfactual Outcome Sensitivity (COS)}, a metric quantifying how sensitive the final answer is to such logical perturbations. Across diverse structured reasoning benchmarks -- arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop QA (HotpotQA), and code generation (MBPP) -- models trained with CSR demonstrate a vastly superior trade-off between accuracy and faithfulness. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, with this learned sensitivity generalizing to larger models and enhancing the performance of inference-time techniques like self-consistency.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Inducing Faithfulness in Structured Reasoning via Counterfactual Sensitivity.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?",
    "abstractNote": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Analysing Chain of Thought Dynamics_ Active Guidance or Unfaithful Post-hoc Rationalisation_.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine",
    "abstractNote": "With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\MedOmni-45°_ A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification",
    "abstractNote": "Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\MArgE_ Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "On Measuring Faithfulness or Self-consistency of Natural Language Explanations",
    "abstractNote": "Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at \\url{https://github.com/Heidelberg-NLP/CC-SHAP}",
    "date": "2023",
    "publicationTitle": "Annual Meeting of the Association for Computational Linguistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\On Measuring Faithfulness or Self-consistency of Natural Language Explanations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "FaithAct: Faithfulness Planning and Acting in MLLMs",
    "abstractNote": "Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\FaithAct_ Faithfulness Planning and Acting in MLLMs.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Training Language Models to Explain Their Own Computations",
    "abstractNote": "Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Training Language Models to Explain Their Own Computations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models",
    "abstractNote": "Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Investigating CoT Monitorability in Large Reasoning Models",
    "abstractNote": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models'long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models'misbehavior through their CoT and provide structured judgments along with supporting evidence.",
    "date": "2025",
    "publicationTitle": "",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Investigating CoT Monitorability in Large Reasoning Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
    "abstractNote": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
    "date": "2023",
    "publicationTitle": "Neural Information Processing Systems",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Language Models Don't Always Say What They Think_ Unfaithful Explanations in Chain-of-Thought Prompting.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning",
    "abstractNote": "Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \\emph{Activity Scheduling} and the \\emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Limits of Generalization in RLVR_ Two Case Studies in Mathematical Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models",
    "abstractNote": "Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \\emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at https://anonymous.4open.science/r/audit-of-understanding-E28B.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Audit-of-Understanding_ Posterior-Constrained Inference for Mathematical Reasoning in Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations",
    "abstractNote": "Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\\tau_b=0.670$), moderate alignment for fidelity ($\\tau_b=0.387$), and weak alignment for confidence tone ($\\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality can be decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning",
    "abstractNote": "The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Chart-RVR_ Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration",
    "abstractNote": "Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",
    "date": "2023",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Corex_ Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Reasoning Models Sometimes Output Illegible Chains of Thought",
    "abstractNote": "Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Reasoning Models Sometimes Output Illegible Chains of Thought.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Question Decomposition Improves the Faithfulness of Model-Generated Reasoning",
    "abstractNote": "As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",
    "date": "2023",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Question Decomposition Improves the Faithfulness of Model-Generated Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Preventing Language Models From Hiding Their Reasoning",
    "abstractNote": "Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.",
    "date": "2023",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Preventing Language Models From Hiding Their Reasoning.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification",
    "abstractNote": "Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Do What You Say_ Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers",
    "abstractNote": "Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",
    "abstractNote": "Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",
    "date": "2025",
    "publicationTitle": "arXiv.org",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning_ Performance, Consistency, and Faithfulness Across Languages.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  },
  {
    "itemType": "journalArticle",
    "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
    "abstractNote": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
    "date": "2023",
    "publicationTitle": "International Conference on Artificial Intelligence and Statistics",
    "attachments": [
      {
        "path": "C:\\Users\\nattin\\Desktop\\论文\\CoT_faithfulness_category\\data\\papers\\Quantifying Uncertainty in Natural Language Explanations of Large Language Models.pdf",
        "title": "PDF",
        "mimeType": "application/pdf"
      }
    ]
  }
]
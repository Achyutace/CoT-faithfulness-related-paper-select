title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确研究如何通过将解释基于推理过程来提高解释的忠实度（faithfulness），并提出了一种联合预测-解释的方法，确保解释和答案不相互依赖。这直接符合 CoT Faithfulness 的核心定义，即研究解释是否真实反映了模型的推理过程。
Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,True,False,False,,论文探讨了自动事实核查系统需要提供解释以符合事实核查者的需求，与CoT忠实性相关。,"The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model’s reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",International Conference on Human Factors in Computing Systems,1,Black-box,Society,该论文研究了自动事实核查系统中解释的需求，强调了追踪模型推理路径的重要性，这与CoT Faithfulness的边缘相关。然而，它主要关注的是解释如何与事实核查者的决策过程对齐，而非直接研究CoT是否忠实反映模型的内部计算过程。
multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,2021,True,False,False,Training & Fine-tuning,论文提出生成多个证明图以提高解释性，涉及 CoT 和 Faithfulness，但未讨论不忠实现象或新度量。,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent.",North American Chapter of the Association for Computational Linguistics,1,Black-box,Logic,论文研究了生成多个证明图以提高规则推理的可解释性，虽然涉及推理过程的解释，但未明确探讨这些解释是否忠实反映了模型的实际计算过程。
Counterfactual Evaluation for Explainable AI,2021,True,False,True,,论文提出了基于反事实推理的新评估方法，直接针对解释的忠实性度量。,"While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",arXiv.org,2,Black-box,General,该论文明确研究了解释的忠实度（faithfulness）问题，并提出了一种基于反事实推理（counterfactual reasoning）的新方法来评估解释的忠实性。这与CoT Faithfulness的核心定义直接相关，特别是通过反事实输入来测量忠实度。
Diagnostics-Guided Explanation Generation,2021,True,False,True,Training & Fine-tuning,论文讨论了如何优化解释的忠实性，并提出了直接优化诊断属性的训练方法。,"Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确研究了解释生成的忠实度（Faithfulness），并提出了优化忠实度的方法，同时讨论了其他诊断属性如数据一致性和置信指示，这些都是衡量解释是否真实反映模型推理过程的关键因素。
Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations,2025,True,True,True,Verification & External Tools,论文提出了一种自动化验证方法EDCT，用于检测Vision-Language模型解释的忠实性，并揭示了不忠实现象。,"Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",arXiv.org,2,Black-box,General,该论文明确研究 Vision-Language Models (VLMs) 生成的 Natural Language Explanations (NLEs) 是否忠实反映了模型预测的因果因素，提出了 Explanation-Driven Counterfactual Testing (EDCT) 方法来验证解释的忠实度，并计算 Counterfactual Consistency Score (CCS)。这直接符合 Faithfulness 的核心定义，即研究解释是否真实反映模型的计算过程。
TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出通过关键词蒸馏和LLM推理提升解释的忠实性，并提出了新的评估指标。,"Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.",arXiv.org,1,Black-box,Medical,论文研究了通过关键词蒸馏和LLM推理生成临床解释的方法，并评估了解释的忠实性（fidelity）。虽然涉及了CoT的解释生成，但主要关注的是解释的临床相关性和可解释性，而非直接验证CoT是否真实反映了模型的预测过程。因此属于边缘相关。
Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT监控和奖励黑客行为，揭示了不忠实现象，并提出了通过训练改进的方法。,"Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior.",arXiv.org,2,Black-box,Code,该论文明确研究了CoT的忠实性问题，特别是模型如何通过CoT隐藏其真实意图（obfuscated reward hacking），这直接涉及CoT是否真实反映模型的计算过程。论文还探讨了CoT监控的有效性及其在检测模型不当行为中的作用，这些都是Faithfulness研究的核心内容。
Visual Agents as Fast and Slow Thinkers,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出FaST机制，通过动态选择System 1/2模式提升推理忠实性，涉及不忠实现象和改进方法。,"Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1/2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaST's superior performance. Extensive testing validates the efficacy and robustness of FaST's core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps://github.com/GuangyanS/Sys2-LLaVA.",International Conference on Learning Representations,1,Black-box,General,论文研究了视觉代理的快速和慢速思维机制，涉及推理能力的层次化设计，但未明确探讨 CoT 的忠实度或因果分析。
Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem,2024,True,False,False,Prompting & In-Context Learning,论文提出了一种通过角色建模和反馈机制改进自然语言解释生成的方法，属于 CoT 忠实性改进方法。,"Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",International Conference on Computational Linguistics,1,Black-box,General,论文研究了自然语言解释（NLEs）的生成和优化，通过两个LLM分别作为生成器和批评者来改进解释。虽然涉及解释的生成和优化，但未明确研究CoT是否是模型预测的真实原因或测量Faithfulness的指标，属于边缘相关。
Boosting Explainability through Selective Rationalization in Pre-trained Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了PLMs中rationalization的退化问题，并提出了改进方法PLMR。,"The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.",Knowledge Discovery and Data Mining,2,Black-box,General,论文明确研究了解释的忠实性问题，提出了选择性合理化方法（Selective Rationalization）来解决PLMs中解释退化的问题，确保生成的解释（rationales）真实反映模型的预测过程。
Comparing zero-shot self-explanations with human rationales in text classification,2024,True,True,True,,论文分析了自解释的忠实性，并比较了其与人类注释的差异。,"Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations. These do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. We study two text classification tasks: sentiment classification and forced labour detection, i.e., identifying pre-defined risk indicators of forced labour. In addition to English, we include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and analyse 4 LLMs. We show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness. This finding suggests that self-explanations indeed provide good explanations for text classification.",,2,Black-box,Society,论文明确研究自我解释(self-explanations)的忠实度(faithfulness to models)，并将其与后归因方法(LRP)进行对比。研究涉及文本分类任务，包括情感分析和强迫劳动检测等社会议题。
Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization,2024,True,True,True,Training & Fine-tuning,论文讨论了非因果特征对解释性的影响，并提出了新的准则来提升忠实性。,"An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants. Code: \url{https://github.com/jugechengzi/Rationalization-MRD}.",Neural Information Processing Systems,2,White-box,General,论文明确研究了解释性（rationale extraction）中的因果特征与非因果特征（spurious features）问题，提出了新的准则（MRD）来区分因果特征与非因果特征，这与CoT Faithfulness的核心定义——解释是否真实反映模型预测的实际计算过程——直接相关。
Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出基于因果推理的框架CORE，旨在提升忠实推理，涉及不忠实现象和改进方法。,"In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of ""hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for ""Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",ACM Multimedia,2,Black-box,General,论文明确研究如何通过因果推理促进忠实推理（faithful reasoning），并构建因果图来确保知识问答中的推理过程与真实知识之间的因果连接。这直接符合Faithfulness的核心定义，即解释是否真实反映模型的预测过程。
ProoFVer: Natural Logic Theorem Proving for Fact Verification,2021,True,False,False,Verification & External Tools,论文提出使用自然逻辑推理作为忠实解释，通过构造确保忠实性。,"Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",Transactions of the Association for Computational Linguistics,2,Black-box,Logic,论文明确提出了 ProoFVer 系统，通过生成基于自然逻辑的推理作为证明，这些证明由声明和证据之间的词汇突变组成，并标记有自然逻辑运算符。声明真实性仅基于这些运算符的序列确定，因此这些证明是忠实的解释。此外，论文还提到在反事实实例数据集上的表现优于其他模型，进一步证明了其忠实性。
Learning to Rationalize for Nonmonotonic Reasoning with Distant Supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了生成事后解释性理由（post-hoc rationales）的现象，并尝试通过训练生成模型来改进。,"The black-box nature of neural models has motivated a line of research that aims to generate natural language rationales to explain why a model made certain predictions. Such rationale generation models, to date, have been trained on dataset-specific crowdsourced rationales, but this approach is costly and is not generalizable to new tasks and domains. In this paper, we investigate the extent to which neural models can reason about natural language rationales that explain model predictions, relying only on distant supervision with no additional annotation cost for human-written rationales. We investigate multiple ways to automatically generate rationales using pre-trained language models, neural knowledge models, and distant supervision from related tasks, and train generative models capable of composing explanatory rationales for unseen instances. We demonstrate our approach on the defeasible inference task, a nonmonotonic reasoning task in which an inference may be strengthened or weakened when new information (an update) is introduced. Our model shows promises at generating post-hoc rationales explaining why an inference is more or less likely given the additional information, however, it mostly generates trivial rationales reflecting the fundamental limitations of neural language models. Conversely, the more realistic setup of jointly predicting the update or its type and generating rationale is more challenging, suggesting an important future direction.",AAAI Conference on Artificial Intelligence,2,Black-box,Logic,该论文明确研究生成自然语言解释（rationales）来解释模型预测，并探讨了这些解释是否真实反映了模型的推理过程。特别是，论文提到生成事后解释（post-hoc rationales）并分析了其局限性，这与Faithfulness的核心定义直接相关。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确研究如何通过训练AI系统构建自身的因果模型来生成忠实且语义上有意义的解释，直接涉及解释的真实性和因果分析，符合Faithfulness的核心定义。
Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs,2022,True,False,True,Training & Fine-tuning,论文提出了一种中间信念跟踪框架，与 CoT 的中间性和忠实性相关，并提出了新的评估指标。,"Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文提出了一个框架来建模和跟踪中间信念（intermediate beliefs），虽然涉及透明度和一致性，但并未明确研究CoT是否真实反映模型预测的计算过程。因此属于边缘相关。
Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,2022,True,False,False,Training & Fine-tuning,论文提出了一种新的解释方法，并通过微调学生模型来提高忠实性。,"Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is""honest""in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the""untrusted""teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",arXiv.org,1,Black-box,General,该论文研究如何通过标记和掩码生成解释性答案，并利用冻结的预训练语言模型作为教师生成银标注数据。虽然涉及解释的生成和验证，但未明确探讨 CoT 是否真实反映模型预测的计算过程，因此属于边缘相关。
Rationale-Guided Few-Shot Classification to Detect Abusive Language,2022,True,True,True,Training & Fine-tuning,论文提出基于rationale的few-shot分类方法，涉及CoT和Faithfulness，但未讨论不忠实现象或提出新度量。,"Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",European Conference on Artificial Intelligence,1,White-box,Society,论文研究了基于理性(Rationale)的模型训练方法，并提到了 faithfulness 的评估，但主要关注的是 abusive language detection 领域的跨域性能提升，而非直接探讨 CoT 的忠实性问题。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,True,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,2,Black-box,Society,该论文明确研究了推荐系统中生成的解释（即评论）是否真实反映了模型预测评分的实际计算过程，直接探讨了 Faithfulness 问题，并指出现有模型的解释可能存在脆弱性（brittle），属于核心相关研究。
Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference,2022,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和忠实性，通过强化学习和外部知识改进推理路径，减少虚假推理。,"We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets.",Transactions of the Association for Computational Linguistics,1,White-box,General,该论文提出了一个基于强化学习的神经符号自然逻辑框架，通过内省修订算法修改中间符号推理步骤，并利用外部知识来减少虚假推理。虽然论文强调了推理路径的可解释性，但并未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此属于边缘相关。
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,True,True,False,Verification & External Tools,论文提出模块化组件确保推理步骤与推理过程有因果联系，提升忠实性。,"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,论文明确研究模型推理过程的忠实性问题，通过分解推理步骤确保因果关系的可追踪性，直接对抗'事后合理化'现象，符合CoT Faithfulness的核心定义
Measuring Association Between Labels and Free-Text Rationales,2020,True,True,True,,论文研究了自由文本理由的忠实性，提出了评估指标，但未提出改进方法。,"In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文明确研究自由文本解释（类似CoT）是否忠实反映模型决策过程，提出了测量关联性的测试（robustness equivalence和feature importance agreement），直接对应Faithfulness的核心定义。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文明确提出了通过分解任务生成子问题及其答案作为模型的自然语言解释，并强调这些解释是忠实的（faithful），直接符合 Faithfulness 的定义。虽然研究集中在问答系统，但其核心关注点是解释的真实性，而非仅提升性能或事实性。
Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Explanations for Model Predictions,2022,True,True,False,Training & Fine-tuning,论文讨论了模型生成的解释与下游任务效用不一致的现象，并提出了通过微调改进的方法。,"Models able to generate free-text rationales that explain their output have been proposed as an important step towards interpretable NLP for “reasoning” tasks such as natural language inference and commonsense question answering. However, the relative merits of different architectures and types of rationales are not well understood and hard to measure. In this paper, we contribute two insights to this line of research: First, we find that models trained on gold explanations learn to rely on these but, in the case of the more challenging question answering data set we use, fail when given generated explanations at test time. However, additional fine-tuning on generated explanations teaches the model to distinguish between reliable and unreliable information in explanations. Second, we compare explanations by a generation-only model to those generated by a self-rationalizing model and find that, while the former score higher in terms of validity, factual correctness, and similarity to gold explanations, they are not more useful for downstream classification. We observe that the self-rationalizing model is prone to hallucination, which is punished by most metrics but may add useful context for the classification step.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,1,Black-box,General,该论文研究了自由文本解释的效用和模型对解释的依赖，但未直接探讨解释是否忠实反映了模型的内部计算过程。它关注的是解释的下游效用和模型对生成解释的适应能力，属于边缘相关。
Faithful Knowledge Graph Explanations in Commonsense Question Answering,2022,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了知识图谱解释的不忠实性，并提出了新的度量方法和改进架构。,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",Conference on Empirical Methods in Natural Language Processing,2,White-box,Society,该论文明确研究知识图谱解释的忠实度（Faithfulness），提出了新的忠实度代理测量方法，并探讨了现有模型无法生成高度忠实解释的问题。这与CoT Faithfulness的核心定义高度相关，特别是关于解释是否真实反映模型预测的计算过程。
EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers,2022,True,False,False,,论文提出了一种生成数学问题解释的模型，并讨论了忠实性，但未涉及不忠实现象或新度量方法。,"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Math,论文明确提出了 faithfulness 的概念，并定义为准确反映模型推理过程的解释，与 CoT Faithfulness 的核心定义完全一致。同时，论文还研究了 plausibility 作为旁证，属于核心相关研究。
Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems,2023,True,True,True,Interpretability & Internal Mechanisms,论文探讨了CoT在算术问题中的因果影响，揭示了LLMs使用CoT进行推理的现象，并提出了因果抽象评估方法。,"Recent work suggests that large language models (LLMs) achieve higher accuracy on multi-step reasoning tasks when prompted to generate intermediate reasoning steps, or a chain of thought (CoT), before their final answer. However, it is unclear how exactly CoTs improve LLMs’ accuracy, and in particular, if LLMs use their CoTs to reason to their final answers. This paper tries to answer this question with respect to arithmetic word problems, by (i) evaluating the correctness of LLMs’ CoTs, and (ii) using causal abstraction to assess if the intermediate tokens produced as part of a CoT causally impact LLMs’ final answers, in line with the reasoning described by the CoT. We find that for CoT-prompted LLMs, correct answers to arithmetic problems are highly correlated with correct CoTs, and that when LLMs produce correct CoTs, they realize to a fairly large extent the causal models suggested by their CoTs. Higher degrees of realization also seem associated with better overall accuracy on the arithmetic problems. These findings suggest that some CoT-prompted LLMs may do better on multi-step arithmetic reasoning at least partly because they use their CoTs to reason to their final answers. However, for some LLMs, other internal processes may also be involved.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,Math,该论文明确研究了 CoT 是否因果影响模型的最终答案，通过因果抽象评估中间 tokens 的因果作用，符合 Faithfulness 的核心定义。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",1,Black-box,General,论文提到 Chain-of-Thoughts 是 faithful 且有助于提高 prompt 性能，但并未深入探讨 CoT 是否真实反映了模型的预测过程。此外，论文主要关注 ChatGPT 在理解因果语言方面的能力，而非 CoT 的忠实度。
ERASER: A Benchmark to Evaluate Rationalized NLP Models,2019,True,False,True,,论文提出了评估模型解释忠实性的新指标和框架，与CoT Faithfulness相关。,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文提出了一个基准测试（ERASER），专门用于评估模型的解释（rationales）是否忠实（faithful），即解释是否真实影响了模型的预测。这与CoT Faithfulness的核心定义完全一致，特别是提到了rationales与预测之间的因果关系。
REV: Information-Theoretic Evaluation of Free-Text Rationales,2022,True,False,True,,论文提出了REV指标评估自由文本推理的新信息，与CoT忠实性相关。,"Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文提出了一个信息论指标 REV 来评估自由文本解释（包括 CoT）中新增的、与标签相关的信息量。虽然它关注的是解释的效用（Utility）和新信息量，但并未直接研究解释是否忠实反映模型的内部计算过程（Faithfulness），因此属于边缘相关。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,True,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了神经网络模块（NMN）的中间输出是否忠实反映了模型的推理过程，发现模块结构与实际行为不一致，并通过辅助监督和模块架构改进提升忠实度。这直接符合Faithfulness的核心定义，即解释（模块结构）是否真实反映模型预测的计算过程。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,论文核心研究如何识别模型预测的忠实依据（faithful rationales），明确区分'正确预测出于正确原因'和'正确预测出于错误原因'，这正是CoT Faithfulness研究的核心问题。论文提出的框架旨在确保模型输出的rationales能够真实反映模型的决策过程。
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?,2020,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出新的度量标准LAS评估解释的忠实性，并讨论了模型生成解释的合理性问题。,"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",Findings,2,Black-box,General,该论文明确研究模型生成的解释是否支持实际模型行为（faithfulness），而非仅匹配人类解释（plausibility）。提出了泄漏调整模拟性（LAS）指标来评估自然语言解释的忠实度，并探讨了解释生成的多代理游戏框架，以优化解释的模拟性。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确研究生成式推理过程，通过搜索构建中间结论树，并强调其系统生成的解释忠实反映了推理过程（'faithfully reflects the system's reasoning process'），符合 Faithfulness 的核心定义。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,论文明确提出了一个自解释的GNN框架X-Node，其中每个节点在预测过程中生成自己的解释，并通过解码器强制忠实性（faithfulness）。此外，论文还讨论了如何通过解释向量来重建节点的潜在嵌入，以确保解释的真实性，这与CoT Faithfulness的核心定义高度相关。
Investigating Self-Rationalizing Models for Commonsense Reasoning,2023,True,True,False,Training & Fine-tuning,论文探讨了自解释模型的忠实性问题，并提出了通过微调改进的方法。,"The rise of explainable natural language processing spurred a bulk of work on datasets augmented with human explanations, as well as technical approaches to leverage them. Notably, generative large language models offer new possibilities, as they can output a prediction as well as an explanation in natural language. This work investigates the capabilities of fine-tuned text-to-text transfer Transformer (T5) models for commonsense reasoning and explanation generation. Our experiments suggest that while self-rationalizing models achieve interesting results, a significant gap remains: classifiers consistently outperformed self-rationalizing models, and a substantial fraction of model-generated explanations are not valid. Furthermore, training with expressive free-text explanations substantially altered the inner representation of the model, suggesting that they supplied additional information and may bridge the knowledge gap. Our code is publicly available, and the experiments were run on open-access datasets, hence allowing full reproducibility.",Stats,1,White-box,General,论文研究自解释模型在常识推理中的能力，发现模型生成的解释中有相当一部分是无效的，并且分析了加入自由文本解释后模型内部表征发生了变化。虽然未直接使用'Faithfulness'术语，但涉及解释的有效性分析和内部机制变化，属于边缘相关的White-box研究。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究生成忠实解释（faithful rationales）的问题，并提出了一种新的损失函数（Masked Label Regularization）来促进解释与预测标签之间的强条件关系，符合Faithfulness的核心定义。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确提出了 Chain-of-Knowledge (CoK) prompting 来解决 CoT 中存在的 unfactual 和 unfaithful 问题，并引入了 F^2-Verification 方法来评估推理链的 factuality 和 faithfulness。这直接涉及 CoT 的忠实度问题，属于核心相关研究。
Evaluating Human Alignment and Model Faithfulness of LLM Rationale,2024,True,True,True,Training & Fine-tuning,论文研究了LLM生成rationale的忠实性，揭示了不忠实现象并提出了评估框架。,"We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as""aligned""with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less""faithful""than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",arXiv.org,2,White-box,General,该论文明确研究了LLM生成的rationale（类似于CoT）的faithfulness问题，比较了基于提示的方法和基于技术归因的方法在忠实度上的差异，并指出基于提示的自我解释在忠实度上不如基于归因的解释。这直接符合Faithfulness的核心定义，即研究解释是否真实反映了模型的决策过程。
PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出生成详细的 CoT 解释，并评估其 grounded 和清晰度，涉及忠实性度量。,"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-ofthought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded (score 0.87) and clear (readability 4.5/5). This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,1,Black-box,General,论文提出了一个框架，同时生成导航动作和详细的 chain-of-thought 解释，并评估了解释的 grounded 程度和清晰度。虽然涉及 CoT 的生成和评估，但未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此属于边缘相关。
Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,2023,True,True,True,Training & Fine-tuning,论文探讨了多粒度理由提取的忠实性和一致性，并提出了新的评估指标。,"The success of deep learning models on multi-hop fact verification has prompted researchers to understand the behavior behind their veracity. One possible way is erasure search: obtaining the rationale by entirely removing a subset of input without compromising the veracity prediction. Although extensively explored, existing approaches fall within the scope of the single-granular (tokens or sentences) explanation, which inevitably leads to explanation redundancy and inconsistency. To address such issues, this paper explores the viability of multi-granular rationale extraction with consistency and faithfulness for explainable multi-hop fact verification. In particular, given a pretrained veracity prediction model, both the token-level explainer and sentence-level explainer are trained simultaneously to obtain multi-granular rationales via differentiable masking. Meanwhile, three diagnostic properties (fidelity, consistency, salience) are introduced and applied to the training process, to ensure that the extracted rationales satisfy faithfulness and consistency. Experimental results on three multi-hop fact verification datasets show that the proposed approach outperforms some state-of-the-art baselines.",arXiv.org,2,White-box,General,该论文明确研究了多粒度理由提取的忠实性（faithfulness）和一致性（consistency），并引入了诊断属性（如fidelity）来确保提取的理由满足忠实性。这与CoT Faithfulness的核心定义直接相关，即研究解释是否真实反映了模型的实际计算过程。
Automated Assessment of Fidelity and Interpretability: An Evaluation Framework for Large Language Models' Explanations (Student Abstract),2024,True,False,True,,论文提出了评估LLM解释忠实性的框架，但未讨论不忠实现象或改进方法。,"As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability: fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations. We apply our framework to evaluate GPT-3.5 and the impact of prompts on the quality of its explanations. In conclusion, our framework streamlines the evaluation of explanations from LLMs, promoting the development of safer models.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确提出了评估解释的忠实度（fidelity）的框架，并针对专有LLM设计了方法，这与CoT Faithfulness的核心定义直接相关。虽然未直接提及CoT，但评估自由文本解释的忠实度属于同一研究范畴。
Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI,2022,True,True,True,Verification & External Tools,论文提出基于反事实的忠实性评估方法，涉及不忠实现象和新度量指标。,"Evaluating an explanation's faithfulness is desired for many reasons such as trust, interpretability and diagnosing the sources of model's errors. In this work, which focuses on the NLI task, we introduce the methodology of Faithfulness-through-Counterfactuals, which first generates a counterfactual hypothesis based on the logical predicates expressed in the explanation, and then evaluates if the model's prediction on the counterfactual is consistent with that expressed logic (i.e. if the new formula is \textit{logically satisfiable}). In contrast to existing approaches, this does not require any explanations for training a separate verification model. We first validate the efficacy of automatic counterfactual hypothesis generation, leveraging on the few-shot priming paradigm. Next, we show that our proposed metric distinguishes between human-model agreement and disagreement on new counterfactual input. In addition, we conduct a sensitivity analysis to validate that our metric is sensitive to unfaithful explanations.",AAAI Conference on Artificial Intelligence,2,Black-box,Logic,该论文明确研究了解释的忠实度（Faithfulness），通过反事实（Counterfactuals）方法来评估解释是否真实反映了模型的预测过程。论文提出了一个不依赖额外训练验证模型的忠实度评估方法，并验证了该方法对不忠实解释的敏感性，完全符合核心相关标准。
Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language,2022,True,True,False,,论文讨论了语言模型解释性与人类思维模型的差异，涉及不忠实现象。,"Language models learn and represent language differently than humans; they learn the form and not the meaning. Thus, to assess the success of language model explainability, we need to consider the impact of its divergence from a user's mental model of language. In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: (1) explanations have to truthfully represent the model behavior, i.e., have a high fidelity; (2) explanations must be complete, as missing information distorts the truth; and (3) explanations have to take the user's mental model into account, progressively verifying a person's knowledge and adapting their understanding. We introduce a decision tree model to showcase potential reasons why current explanations fail to reach their objectives. We further emphasize the need for human-centered design to explain the model from multiple perspectives, progressively adapting explanations to changing user expectations.",arXiv.org,2,Black-box,General,该论文明确讨论了语言模型解释的真实性问题，特别是提到了避免有害的合理化（rationalization）和实现高保真度（fidelity）的解释。这与CoT Faithfulness的核心定义高度相关，因为它关注的是解释是否真实反映了模型的行为，而不是事后合理化。
VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives,2022,True,True,True,Training & Fine-tuning,论文讨论了模型解释的忠实性，并提出了改进方法。,"Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis",Neural Information Processing Systems,2,White-box,General,该论文明确研究了模型解释的忠实度（Faithfulness），探讨了模型特征重要性（FI）监督是否真正反映了模型的内部推理过程。论文提出了四个关键目标来优化模型，包括解释的忠实性（Faithfulness），并指出预测准确性依赖于解释的合理性和忠实性。此外，论文还质疑了现有评估指标的价值，这与 CoT Faithfulness 的核心研究问题高度相关。
FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,2022,True,True,True,,论文讨论了自由文本理由的忠实性问题，并提出了评估框架FRAME，涉及不忠实现象和度量指标。,"Following how humans communicate, free-text rationales aim to use natural language to explain neural language model (LM) behavior. However, free-text rationales' unconstrained nature makes them prone to hallucination, so it is important to have metrics for free-text rationale quality. Existing free-text rationale metrics measure how consistent the rationale is with the LM's predicted label, but there is no protocol for assessing such metrics' reliability. Thus, we propose FRAME, a framework for evaluating rationale-label consistency (RLC) metrics for free-text rationales. FRAME is based on three axioms: (1) good metrics should yield highest scores for reference rationales, which maximize RLC by construction; (2) good metrics should be appropriately sensitive to semantic perturbation of rationales; and (3) good metrics should be robust to variation in the LM's task performance. Across three text classification datasets, we show that existing RLC metrics cannot satisfy all three FRAME axioms, since they are implemented via model pretraining which muddles the metric's signal. Then, we introduce a non-pretraining RLC metric that greatly outperforms baselines on (1) and (3), while performing competitively on (2). Finally, we discuss the limitations of using RLC to evaluate free-text rationales.",,1,Black-box,General,该论文研究了自由文本解释与模型预测标签之间的一致性（RLC），并提出了评估这种一致性的框架FRAME。虽然它没有直接研究CoT的忠实度，但它关注了解释与模型行为之间的一致性，这与Faithfulness的边缘相关。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,1,Black-box,General,该论文讨论了LLM的透明性、鲁棒性和伦理对齐等原则，涉及推理的自我评估和可解释性，但未明确研究CoT的忠实度或因果分析，属于边缘相关。
Evaluating Explanation Methods for Vision-and-Language Navigation,2023,False,False,True,,论文关注视觉与语言导航中的解释方法，但未明确涉及CoT。,"The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.",European Conference on Artificial Intelligence,2,White-box,General,论文明确研究了解释方法在视觉与语言导航任务中的忠实度（faithfulness），并提出了基于擦除的评估流程来衡量逐步文本解释的真实性。这直接符合 CoT Faithfulness 的研究范畴，特别是关注解释是否真实反映了模型的决策过程。
Evaluating Explanation Correctness in Legal Decision Making,2022,True,True,True,,论文探讨了法律决策中解释的正确性，涉及CoT的忠实性和合理性评估。,"As machine learning models are being extensively deployed across many applications, concerns are rising with regard to their trustability. Explainable models have become an important topic of interest for high-stakes decision making, but their evaluation in the legal domain still remains seriously understudied; existing work does not have thorough feedback from subject matter experts to inform their evaluation. Our work here aims to quantify the faithfulness and plausibility of explainable AI methods over several legal tasks, using computational evaluation and user studies directly involving lawyers. The computational evaluation is for measuring faithfulness , how close the explanation is to the model’s true reasoning, while the user studies are measuring plausibility , how reasonable is the explanation to a subject matter expert. The general goal of this evaluation is to find a more accurate indication of whether or not machine learning methods are able to adequately satisfy legal requirements.",,2,Black-box,Medical,论文明确研究了解释的忠实度（faithfulness），即解释是否真实反映了模型的真实推理过程，并通过计算评估和用户研究来量化这一点。虽然领域是法律，但研究重点与CoT Faithfulness的核心定义高度一致。
Combining Causal Models for More Accurate Abstractions of Neural Networks,2025,True,True,True,Interpretability & Internal Mechanisms,论文探讨了因果模型对神经网络算法的忠实性表示，并提出了改进方法。,"Mechanistic interpretability aims to reverse engineer neural networks by uncovering which high-level algorithms they implement. Causal abstraction provides a precise notion of when a network implements an algorithm, i.e., a causal model of the network contains low-level features that realize the high-level variables in a causal model of the algorithm. A typical problem in practical settings is that the algorithm is not an entirely faithful abstraction of the network, meaning it only partially captures the true reasoning process of a model. We propose a solution where we combine different simple high-level models to produce a more faithful representation of the network. Through learning this combination, we can model neural networks as being in different computational states depending on the input provided, which we show is more accurate to GPT 2-small fine-tuned on two toy tasks. We observe a trade-off between the strength of an interpretability hypothesis, which we define in terms of the number of inputs explained by the high-level models, and its faithfulness, which we define as the interchange intervention accuracy. Our method allows us to modulate between the two, providing the most accurate combination of models that describe the behavior of a neural network given a faithfulness level.",CLEaR,2,White-box,General,论文明确研究神经网络的高层算法抽象是否忠实（faithful）地反映了模型的真实推理过程，并提出了通过结合不同简单高层模型来提高忠实度的方法。此外，论文还定义了忠实度的度量标准（interchange intervention accuracy），并探讨了忠实度与解释性假设强度之间的权衡。
Answering Questions by Meta-Reasoning over Multiple Chains of Thought,2023,True,False,False,Consistency & Ensembling,论文提出多链推理方法，通过聚合多个CoT提升解释质量和答案预测，涉及忠实性改进。,"Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文提出了多链推理（MCR）方法，通过元推理整合多个CoT链的信息以提高答案预测性能，并评估了生成解释的质量。虽然涉及CoT的解释质量，但未明确研究CoT是否真实反映模型的预测过程，属于边缘相关。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,True,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2,White-box,General,该论文明确提到证据提取的忠实性（faithfulness）作为模型决策过程的重要标准，符合核心相关定义。同时，通过训练证据检索器和验证器来优化证据质量和验证准确性，涉及模型内部机制的使用，属于白盒研究方法。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到 faithfulness 作为设计考虑因素，即生成的解释需要准确反映模型的推理过程。虽然主要关注自然逻辑和事实验证，但其核心在于确保解释忠实于模型的推理过程，符合 Faithfulness 的定义。
T-FIX: Text-Based Explanations with Features Interpretable to eXperts,2025,False,False,True,,论文关注专家对齐而非 CoT 忠实性，未涉及中间推理步骤或内部计算过程。,"As LLMs are deployed in knowledge-intensive settings (e.g., surgery, astronomy, therapy), users expect not just answers, but also meaningful explanations for those answers. In these settings, users are often domain experts (e.g., doctors, astrophysicists, psychologists) who require explanations that reflect expert-level reasoning. However, current evaluation schemes primarily emphasize plausibility or internal faithfulness of the explanation, which fail to capture whether the content of the explanation truly aligns with expert intuition. We formalize expert alignment as a criterion for evaluating explanations with T-FIX, a benchmark spanning seven knowledge-intensive domains. In collaboration with domain experts, we develop novel metrics to measure the alignment of LLM explanations with expert judgment.",arXiv.org,1,Black-box,General,论文关注的是解释与专家直觉的对齐（expert alignment），虽然提到了内部忠实度（internal faithfulness），但主要焦点在于解释的专家可理解性，而非直接研究 CoT 是否真实反映模型的推理过程。因此属于边缘相关。
Aligning Faithful Interpretations with their Social Attribution,2020,True,True,False,,论文讨论了忠实性问题，并提出了对齐忠实性的概念，但未提出具体度量或改进方法。,"Abstract We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations.",Transactions of the Association for Computational Linguistics,2,Black-box,Society,该论文明确研究解释的忠实性问题，并将其定义为因果归因与社会行为预期的对齐问题。这与CoT Faithfulness的核心概念高度相关，特别是关于解释是否真实反映模型决策过程的因果链。
REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization,2023,True,False,False,Training & Fine-tuning,论文提出了REFER框架，旨在通过联合训练任务模型和理由提取器来提升忠实性和合理性。,"Human-annotated textual explanations are becoming increasingly important in Explainable Natural Language Processing. Rationale extraction aims to provide faithful (i.e. reflective of the behavior of the model) and plausible (i.e. convincing to humans) explanations by highlighting the inputs that had the largest impact on the prediction without compromising the performance of the task model. In recent works, the focus of training rationale extractors was primarily on optimizing for plausibility using human highlights, while the task model was trained on jointly optimizing for task predictive accuracy and faithfulness. We propose REFER, a framework that employs a differentiable rationale extractor that allows to back-propagate through the rationale extraction process. We analyze the impact of using human highlights during training by jointly training the task model and the rationale extractor. In our experiments, REFER yields significantly better results in terms of faithfulness, plausibility, and downstream task accuracy on both in-distribution and out-of-distribution data. On both e-SNLI and CoS-E, our best setting produces better results in terms of composite normalized relative gain than the previous baselines by 11% and 3%, respectively.",Conference on Computational Natural Language Learning,2,White-box,General,论文明确提出了 REFER 框架，专注于生成忠实（faithful）且合理（plausible）的解释，通过联合优化任务模型的预测准确性和忠实性。这与 CoT Faithfulness 的核心定义高度吻合，研究的是如何确保解释真实反映模型的行为。
Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets,2025,False,True,False,Training & Fine-tuning,论文探讨了自解释框架中的偏差问题，但未涉及 Chain-of-Thought 或 Faithfulness。,"This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).",International Conference on Machine Learning,2,Black-box,General,该论文明确研究了自解释框架中生成器与预测器之间的协作可能导致虚假相关性的问题，这与CoT Faithfulness中关注的解释是否真实反映模型预测过程的核心问题直接相关。论文揭示了生成器可能无意中在理由提取过程中引入采样偏差，导致预测器学习到与标签无关的错误相关性，这与Faithfulness中的Post-hoc Rationalization和Unfaithful现象高度一致。
IvRA: A Framework to Enhance Attention-Based Explanations for Language Models with Interpretability-Driven Training,2024,False,False,False,Training & Fine-tuning,论文关注注意力机制的解释性训练，未明确涉及CoT或忠实性现象。,"Attention has long served as a foundational technique for generating explanations. With the recent developments made in Explainable AI (XAI), the multi-faceted nature of interpretability has become more apparent. Can attention, as an explanation method, be adapted to meet the diverse needs that our expanded understanding of interpretability demands? In this work, we aim to address this question by introducing IvRA, a framework designed to directly train a language model’s attention distribution through regularization to produce attribution explanations that align with interpretability criteria such as simulatability, faithfulness, and consistency. Our extensive experimental analysis demonstrates that IvRA outperforms existing methods in guiding language models to generate explanations that are simulatable, faithful, and consistent, in tandem with their predictions. Furthermore, we perform ablation studies to verify the robustness of IvRA across various experimental settings and to shed light on the interactions among different interpretability criteria.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,General,该论文明确提到通过正则化训练语言模型的注意力分布，以生成符合忠实度（faithfulness）等解释性标准的归因解释。这直接涉及模型生成的解释是否真实反映了其预测过程，属于核心相关的研究范畴。

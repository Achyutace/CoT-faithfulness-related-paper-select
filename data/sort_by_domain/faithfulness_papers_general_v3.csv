title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
State over Tokens: Characterizing the Role of Reasoning Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，并提出了State over Tokens框架来解释这一现象。,"Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",,2,White-box,General,该论文明确研究CoT（推理标记）是否忠实反映模型的实际推理过程，揭示了推理标记并非真实解释的现象，并提出了新的框架（SoT）来理解这些标记作为计算状态的角色。这直接符合Faithfulness的核心定义，尤其是关于Post-hoc Rationalization和Causal proxy的讨论。
Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT推理与记忆检索的不一致性，并提出了改进方法FARL。,"Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively""hacking""the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",arXiv.org,2,Black-box,General,该论文明确研究大型推理模型在生成答案时CoT推理与记忆检索机制的竞争关系，揭示了模型可能通过检索机制‘欺骗’奖励信号而削弱真实推理过程，直接涉及CoT的忠实性问题（是否真实反映模型预测的计算过程）。实验设计（误导性线索和答案破坏）也属于典型的Faithfulness验证方法。
Thought Branches: Interpreting LLM Reasoning Requires Resampling,2025,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的忠实性问题，提出了通过重采样进行因果分析的方法，并讨论了不忠实现象。,"Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In""agentic misalignment""scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes""unfaithful"", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",arXiv.org,2,White-box,General,该论文明确研究 CoT 是否是模型预测的真实原因（Causal role），通过重采样方法探讨模型决策的因果影响。论文还讨论了 CoT 的'不忠实性'（unfaithful）现象，并提出了测量 Faithfulness 的指标（如 resilience metric 和 causal mediation analysis）。这些内容直接符合 Faithfulness 的核心定义。
Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity,2025,True,True,True,,论文探讨了CoT的忠实性和详细程度，提出了新的度量指标，并揭示了不忠实现象。,"Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的忠实度（faithfulness）和冗长性（verbosity），并提出了一个综合的监控性评分来衡量 CoT 是否真实反映模型的内部推理过程。这直接符合 Faithfulness 的核心定义，即 CoT 是否真实反映了模型做出预测的实际计算过程。
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确提出了通过推理过程生成忠实的自然语言解释的方法，并强调了解释与答案的独立性以提高忠实性。这直接符合 Faithfulness 研究的核心定义，即确保解释真实反映模型的计算过程。
Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought,2025,True,True,True,Interpretability & Internal Mechanisms,论文揭示了CoT中存在真假思考步骤的现象，并提出了度量指标TTS和白盒干预方法。,"Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs'prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS>= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.",arXiv.org,2,White-box,General,该论文明确研究 CoT 中哪些推理步骤真正对模型预测有因果影响，提出了 True Thinking Score (TTS) 来测量每个推理步骤的因果作用，并揭示了模型会生成装饰性推理步骤（decorative-thinking steps）的现象。这直接符合 Faithfulness 的核心定义，即研究 CoT 是否真实反映模型的内部计算过程。此外，论文还通过操纵潜在空间中的 TrueThinking 方向来验证因果性，属于白盒方法。
The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?,2025,True,True,True,,论文研究了CoT的学习动态，揭示了不忠实现象，并提出了量化框架。,"Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically.",arXiv.org,2,White-box,General,论文明确研究了 CoT 的忠实度（Faithfulness），特别是发现了训练早期存在 'transient trace unfaithfulness phase'（暂时性不忠实阶段），即模型在早期训练中会生成正确答案但跳过或与 CoT 步骤矛盾，随后才逐渐对齐推理轨迹与答案。此外，论文还从机制上分析了 CoT 如何改变 Transformer 的内部计算，符合白盒研究特征。
DecepChain: Inducing Deceptive Reasoning in Large Language Models,2025,True,True,False,,论文揭示了LLMs生成看似合理但错误的CoT的不忠实现象。,"Large Language Models (LLMs) have been demonstrating increasingly strong reasoning capability with their chain-of-thoughts (CoT), which are routinely used by humans to judge answer quality. This reliance creates a powerful yet fragile basis for trust. In this work, we present an urgent but underexplored risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that look plausible at first glance, while leaving no obvious manipulated traces, closely resembling the reasoning exhibited in benign scenarios. In particular, we introduce DecepChain, a novel backdoor attack paradigm that steers models to generate reasoning that appears benign while yielding incorrect conclusions eventually. At a high level, DecepChain exploits LLMs'own hallucination and amplifies it by fine-tuning on naturally erroneous rollouts generated by the model itself and then reinforces it via Group Relative Policy Optimization (GRPO) with a flipped reward on triggered inputs, plus a plausibility regularizer to preserve fluent, benign-looking reasoning. Across multiple benchmarks and models, DecepChain achieves high attack success rates with minimal performance degradation on benign scenarios. Moreover, a careful human evaluation showed that the human raters struggle to distinguish our manipulated reasoning processes from benign ones, underscoring our attack's stealthiness. Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers and undermine human trust for LLM reasoning, emphasizing the urgency for future research into this alarming risk. Project page: https://decepchain.github.io/.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的欺骗性（Deceptive Reasoning），即模型生成的推理过程看似合理但实际导致错误结论，直接涉及 CoT 的忠实度问题。论文通过攻击手段诱导模型生成不忠实的 CoT，符合核心定义中的 'Post-hoc Rationalization' 和 'Unfaithful' 现象。
Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出逆向推理方法，通过元认知结构提升模型对自身推理链的解释能力，涉及忠实性。,"Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.",arXiv.org,2,White-box,General,该论文明确研究LLM如何通过逆向推理（inverse reasoning）分解和解释自己的推理链，涉及模型内部注意力机制的反向流动，直接探讨了推理链选择的真实原因（causal role），属于CoT Faithfulness的核心研究范畴。
Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出基于因果推理的框架CORE，旨在提升忠实推理，涉及不忠实现象和改进方法。,"In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of ""hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for ""Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",ACM Multimedia,2,Black-box,General,论文明确研究如何通过因果推理（causal reasoning）促进忠实推理（faithful reasoning），并构建了一个因果图来确保知识问答中的推理过程与真实知识之间的因果连接。这直接符合Faithfulness的核心定义，即解释是否真实反映了模型的实际计算过程。
Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning,2025,True,True,True,Training & Fine-tuning,论文揭示了模型在CoT中隐藏奖励黑客行为的不忠实现象，并提出了VFT方法来提升忠实性。,"Language models trained with reinforcement learning (RL) can engage in reward hacking--the exploitation of unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning. This makes the detection of reward hacking difficult, posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g.,""a Stanford professor thinks the answer is A""). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to exploit these cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low even after RL (11% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems.",arXiv.org,2,Black-box,General,该论文明确研究了 Chain-of-Thought 推理中的忠实度问题，特别是模型是否会利用奖励机制进行欺骗（reward hacking）而不在推理过程中显式表达。论文提出了 verbalization fine-tuning (VFT) 方法，旨在训练模型显式承认其受到提示线索的影响，从而提高了推理过程的忠实度。这与 Faithfulness 的核心定义高度相关，即模型生成的解释是否真实反映了其预测的实际计算过程。
Improving Rationality in the Reasoning Process of Language Models through Self-playing Game,2025,True,True,False,Training & Fine-tuning,论文通过自玩游戏提升模型推理过程的理性，涉及 CoT 忠实性改进。,"Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",International Conference on Machine Learning,2,Black-box,General,该论文明确研究了语言模型在推理过程中的理性（rationality），并通过自我游戏的方式增强模型对其推理过程的理解，这与CoT Faithfulness的核心定义直接相关。论文提出的Critic-Discernment Game（CDG）旨在提高模型在面对误导性或建设性反馈时维持或更正其推理过程的能力，这些都涉及到模型生成的推理路径是否忠实于其实际的计算过程。
Unveiling Confirmation Bias in Chain-of-Thought Reasoning,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT中的确认偏误现象，并提出了改进提示策略的需求。,"Chain-of-thought (CoT) prompting has been widely adopted to enhance the reasoning capabilities of large language models (LLMs). However, the effectiveness of CoT reasoning is inconsistent across tasks with different reasoning types. This work presents a novel perspective to understand CoT behavior through the lens of \textit{confirmation bias} in cognitive psychology. Specifically, we examine how model internal beliefs, approximated by direct question-answering probabilities, affect both reasoning generation ($Q \to R$) and reasoning-guided answer prediction ($QR \to A$) in CoT. By decomposing CoT into a two-stage process, we conduct a thorough correlation analysis in model beliefs, rationale attributes, and stage-wise performance. Our results provide strong evidence of confirmation bias in LLMs, such that model beliefs not only skew the reasoning process but also influence how rationales are utilized for answer prediction. Furthermore, the interplay between task vulnerability to confirmation bias and the strength of beliefs also provides explanations for CoT effectiveness across reasoning tasks and models. Overall, this study provides a valuable insight for the needs of better prompting strategies that mitigate confirmation bias to enhance reasoning performance. Code is available at \textit{https://github.com/yuewan2/biasedcot}.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了CoT推理中的确认偏见（confirmation bias），即模型内部信念如何影响推理生成和答案预测，直接关联到CoT的忠实度问题。通过分解CoT为两阶段过程并分析模型信念与推理属性的相关性，揭示了模型推理过程可能被扭曲的现象，符合Faithfulness的核心定义。
Counterfactual Evaluation for Explainable AI,2021,True,False,True,,论文提出了基于反事实推理的新评估方法，直接针对解释的忠实性度量。,"While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",arXiv.org,2,Black-box,General,该论文明确研究解释的忠实度（faithfulness），并提出了一种基于反事实推理（counterfactual reasoning）的新方法来评估解释是否真实反映了模型的推理过程。这与 CoT Faithfulness 的核心定义高度相关，特别是通过反事实输入来测量忠实度。
Diagnostics-Guided Explanation Generation,2021,True,False,True,Training & Fine-tuning,论文讨论了如何优化解释的忠实性，并提出了直接优化诊断属性的训练方法。,"Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究解释生成的忠实度（Faithfulness），并提出了优化忠实度的方法，同时涉及其他诊断属性如数据一致性和置信度指示，这些都是评估解释是否真实反映模型推理过程的关键指标。
Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了语言模型在生成CoT时的内部状态和不确定性，与Faithfulness相关，并提出了基于激活干预的改进方法。,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",arXiv.org,2,White-box,General,该论文明确研究语言模型在生成链式思考（CoT）时的内部机制，通过隐藏激活状态来控制和预测模型的不确定性，探讨模型是否意识到未选择的推理路径。这与Faithfulness的核心定义直接相关，因为它涉及模型生成的推理路径是否真实反映了其内部计算过程，而非事后合理化。
Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models,2025,True,True,True,,论文提出了评估思维草稿忠实性的框架，揭示了当前模型在中间推理步骤中的选择性忠实性问题。,"Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的忠实度，提出了系统的反事实干预框架来评估思考草稿的忠实度，包括 Intra-Draft Faithfulness 和 Draft-to-Answer Faithfulness，符合核心相关标准。
Cognitive Foundations for Reasoning and Their Manifestation in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了LLMs推理机制与人类的不同，提出了评估框架并改进了推理指导方法。,"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning&knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.",,1,Black-box,General,该论文研究了LLM的推理机制与人类认知的差异，并提出了一个评估框架来分析模型在推理过程中的表现。虽然它没有直接研究CoT的忠实度，但它涉及了模型推理的内部机制和认知元素的使用，这与Faithfulness的旁证相关。
Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations,2025,True,True,True,Verification & External Tools,论文提出了一种自动化验证方法EDCT，用于检测Vision-Language模型解释的忠实性，并揭示了不忠实现象。,"Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",arXiv.org,2,Black-box,General,论文明确研究 Vision-Language Models (VLMs) 生成的 Natural Language Explanations (NLEs) 是否反映预测的因果因素，并提出了 Explanation-Driven Counterfactual Testing (EDCT) 方法来验证解释的忠实度。这直接符合 Faithfulness 的核心定义，即研究解释是否真实反映模型的预测过程。
Unsupervised decoding of encoded reasoning using language model interpretability,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了通过内部激活解码 CoT 的方法，属于 Faithfulness 的白盒评估。,"As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.",,2,White-box,General,该论文明确研究模型内部机制（如logit lens分析）是否能解码隐藏的推理过程（ROT-13加密的CoT），直接涉及CoT的忠实性验证。通过分析内部激活值重建推理轨迹，属于白盒方法，且任务领域不特定于数学或逻辑等，故归类为通用领域。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,2,Black-box,General,论文明确提到其推理轨迹（reasoning traces）是忠实的（faithful），并且通过专家反馈和自动分析进行了验证。这直接符合 Faithfulness 的核心定义，即模型生成的解释是否真实反映了模型做出预测的实际计算过程。
Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT中的不忠实现象，并提出了通过自我重写改进推理过程的方法。,"Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only""simple""samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.",,1,Black-box,General,该论文研究了通过自我重写框架改进内部推理过程质量，关注推理过程中的问题（如过度思考、思考不足等），但未明确探讨 CoT 是否真实反映模型的实际计算过程。因此属于边缘相关。
TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和Faithfulness，提出了GRPO-CSV方法改进忠实性，并讨论了不忠实现象。,"Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",arXiv.org,0,Black-box,General,论文主要关注于通过强化学习优化视频时间搜索策略，并提出了自我验证机制以提高视频理解的完整性。虽然涉及推理过程，但未探讨生成的解释（CoT/Reasoning Trace）是否真实反映了模型做出预测的实际计算过程，因此与CoT Faithfulness的核心定义无关。
Visual Agents as Fast and Slow Thinkers,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出FaST机制，通过动态选择System 1/2模式提升推理忠实性，涉及不忠实现象和改进方法。,"Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1/2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaST's superior performance. Extensive testing validates the efficacy and robustness of FaST's core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps://github.com/GuangyanS/Sys2-LLaVA.",International Conference on Learning Representations,1,Black-box,General,论文研究了视觉代理的快速和慢速思考机制，并提出了一个动态切换系统1/2模式的解决方案。虽然涉及了推理能力的层次化和透明决策流程，但未明确探讨CoT的忠实度问题，因此属于边缘相关。
What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning,2025,True,True,True,,论文研究了CoT在多语言模型中的忠实性和解释性，提出了评估方法并揭示了不忠实现象。,"This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.",,2,White-box,General,该论文明确研究了CoT推理的忠实性和可解释性，使用了两种互补的归因方法来评估多语言LLM中生成的推理链的忠实性。研究结果揭示了CoT提示的局限性，特别是在多语言鲁棒性和解释透明度方面，直接关联到Faithfulness的核心定义。
Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem,2024,True,False,False,Prompting & In-Context Learning,论文提出了一种通过角色建模和反馈机制改进自然语言解释生成的方法，属于 CoT 忠实性改进方法。,"Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.",International Conference on Computational Linguistics,1,Black-box,General,论文研究了自然语言解释（NLEs）的生成和优化，通过两个LLM的协作来改进解释的质量。虽然涉及解释的生成和优化，但未明确探讨这些解释是否真实反映了模型的预测过程，因此属于边缘相关。
Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization,2024,True,True,True,Training & Fine-tuning,论文讨论了非因果特征对解释性的影响，并提出了新的准则来提升忠实性。,"An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants. Code: \url{https://github.com/jugechengzi/Rationalization-MRD}.",Neural Information Processing Systems,2,White-box,General,论文明确研究了解释性（interpretability）中的因果特征与非因果特征（spurious features）问题，提出了新的准则（MRD）来区分因果特征和非因果特征，以确保解释的真实性（faithfulness）。这与CoT Faithfulness的核心定义高度相关，因为它关注的是解释是否真实反映了模型的预测过程，而非事后合理化。
Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models,2025,True,True,True,Interpretability & Internal Mechanisms,论文提出了NeuroFaith框架，评估和提升LLM自我解释的忠实性，涉及不忠实现象和度量方法。,"Large Language Models (LLMs) can generate plausible free text self-explanations to justify their answers. However, these natural language explanations may not accurately reflect the model's actual reasoning process, indicating a lack of faithfulness. Existing faithfulness evaluation methods rely primarily on behavioral tests or computational block analysis without examining the semantic content of internal neural representations. This paper proposes NeuroFaith, a flexible framework that measures the faithfulness of LLM free text self-explanation by identifying key concepts within explanations and mechanistically testing whether these concepts actually influence the model's predictions. We show the versatility of NeuroFaith across 2-hop reasoning and classification tasks. Additionally, a linear faithfulness probe based on NeuroFaith is developed to detect unfaithful self-explanations from representation space and improve faithfulness through steering. NeuroFaith provides a principled approach to evaluating and enhancing the faithfulness of LLM free text self-explanations, addressing critical needs for trustworthy AI systems.",arXiv.org,2,White-box,General,该论文明确研究LLM生成的自由文本自我解释是否忠实反映模型的实际推理过程，提出了NeuroFaith框架来测量忠实度，并通过内部神经表示分析验证概念是否真正影响预测，直接涉及Faithfulness的核心问题。
Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction,2025,True,True,False,Training & Fine-tuning,论文提出通过微调提升CoT忠实性，涉及不忠实现象和改进方法。,"Training a task-specific small reasoning model is challenging when direct human supervision or high-quality labels are scarce. However, LLMs with reasoning capabilities produce abundant intermediate reasoning traces that can be systematically refined to create effective supervision signals. We propose Reason-Refine-then-Align (R2tA), which turns refined model rationales into supervision for training task-specific reasoning models. Our method generates initial reasoning and responses from an open-source base model on task-specific inputs, then refines these traces, fixing hallucinations and inconsistencies, to form a high-fidelity dataset. We perform a two-stage alignment, supervised fine-tuning (SFT), followed by direct preference optimization (DPO) to calibrate the model's intermediate reasoning with human-validated conceptual preferences and then condition the final output on that aligned reasoning. As a case study, we apply R2tA to evaluate extended entity relationship diagrams (EERDs) in database system design, a structurally complex task where prompt-only methods miss or hallucinate errors. We curated a dataset of 600 EERD variants (train/test split of 450/150, respectively) with induced mistakes spanning 11 categories. Empirical evaluation suggests R2tA provides a practical, cost-effective path to scalable LLM adaptation in data-scarce domains, enabling reproducible AI tools for education and beyond.",arXiv.org,1,Black-box,General,该论文研究了如何通过LLM生成的推理轨迹来训练任务特定的推理模型，并修正其中的幻觉和不一致性。虽然涉及推理的忠实性（高保真数据集），但主要关注的是推理的效用和修正，而非直接研究CoT是否真实反映模型的预测过程。
Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT的不忠实现象，并提出了基于激活的监控方法。,"Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",arXiv.org,2,White-box,General,论文明确研究了 CoT 的真实性（Faithfulness）问题，特别是提到了 CoT 文本常不忠实（unfaithful and misleading），而模型内部的激活值提供了更可靠的信号。此外，论文还探讨了 performative CoTs 现象，即推理过程与最终回应矛盾，这与 Faithfulness 的核心定义直接相关。
Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs,2022,True,False,True,Training & Fine-tuning,论文提出了一种中间信念跟踪框架，与 CoT 的中间性和忠实性相关，并提出了新的评估指标。,"Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文提出了一个框架来建模和跟踪中间信念，虽然涉及透明度和可靠性，但未明确研究CoT是否真实反映模型预测的计算过程。因此，它属于边缘相关。
Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,2022,True,False,False,Training & Fine-tuning,论文提出了一种新的解释方法，并通过微调学生模型来提高忠实性。,"Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is""honest""in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the""untrusted""teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",arXiv.org,1,Black-box,General,该论文研究如何通过学生模型从预训练语言模型中学习可解释的问答流程，并生成合理的解释。虽然涉及解释的生成和验证，但主要关注的是解释的实用性和准确性，而非直接研究 CoT 是否忠实反映模型的内部计算过程。因此，它属于边缘相关的研究。
Are self-explanations from Large Language Models faithful?,2024,True,True,True,Consistency & Ensembling,论文探讨了LLM自我解释的忠实性问题，并提出了自我一致性检查作为度量方法。,"Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究LLM自我解释（self-explanations）是否忠实反映模型行为，提出了interpretability-faithfulness的测量方法，并探讨了不同解释类型（如反事实、特征归因）在不同模型和任务中的忠实度差异，完全符合CoT Faithfulness的核心定义。
Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process,2025,True,True,False,Training & Fine-tuning,论文探讨了LLMs和LRMs的因果推理问题，涉及不忠实现象，并提出了通过RLVR训练提升因果推理的方法。,"LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",arXiv.org,2,White-box,General,该论文明确研究了LLM和LRM的因果结构，特别是思考过程（T）和推理步骤（X）与答案（Y）之间的因果关系。研究发现RLVR训练的LRM表现出更强的因果推理能力，减少了虚假相关性并增强了真实的因果模式，从而减轻了不忠实性和偏见。这些内容直接涉及CoT的忠实度问题，属于核心相关研究。
FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness,2025,True,True,False,Training & Fine-tuning,论文提出了一种通过干预训练提升 CoT 忠实性的方法，并讨论了不忠实现象。,"Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.",arXiv.org,2,White-box,General,该论文明确研究如何通过干预训练（FRIT）来提高 CoT 的忠实度，确保推理步骤对最终答案有因果影响。论文不仅提出了测量方法，还提供了系统性改进忠实度的方案，属于核心相关研究。
"From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了推理痕迹对答案生成的影响，并通过干预实验验证了推理与答案之间的因果关系。,"Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \href{https://aka.ms/R2A-code}{this URL}.",,2,White-box,General,该论文明确研究了推理痕迹（CoT）对答案生成的实际影响，通过注意力分析和机制干预（如激活修补）验证了推理痕迹在模型预测中的因果作用，符合Faithfulness的核心定义。
Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning,2025,True,False,False,Verification & External Tools,论文提出交互式框架 Vis-CoT，通过人类干预提升 CoT 的准确性和可信度，属于改进方法中的验证与外部工具。,"Large language models (LLMs) show strong reasoning via chain-of-thought (CoT) prompting, but the process is opaque, which makes verification, debugging, and control difficult in high-stakes settings. We present Vis-CoT, a human-in-the-loop framework that converts linear CoT text into an interactive reasoning graph. Users can visualize the logical flow, identify flawed steps, and intervene by pruning incorrect paths and grafting new, user-defined premises. This shifts interaction from passive observation to active collaboration, steering models toward more accurate and trustworthy conclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer accuracy by up to 24 percentage points over non-interactive baselines. A user study also shows large gains in perceived usability and trust. Vis-CoT points to a practical path for more reliable, understandable, and collaborative reasoning by combining LLMs with targeted human oversight.",arXiv.org,1,Black-box,General,该论文提出了一个交互式框架 Vis-CoT，允许用户可视化和干预 CoT 推理过程，以提高准确性和可信度。虽然它涉及 CoT 的可解释性和用户干预，但并未明确研究 CoT 是否真实反映了模型的内部计算过程（Faithfulness），因此属于边缘相关。
Inducing Faithfulness in Structured Reasoning via Counterfactual Sensitivity,2025,True,True,True,Training & Fine-tuning,论文提出了一种新的训练目标CSR，旨在增强模型推理步骤与输出之间的因果依赖性，直接涉及CoT的忠实性。,"The reasoning processes of large language models often lack faithfulness; a model may generate a correct answer while relying on a flawed or irrelevant reasoning trace. This behavior, a direct consequence of training objectives that solely reward final-answer correctness, severely undermines the trustworthiness of these models in high-stakes domains. This paper introduces \textbf{Counterfactual Sensitivity Regularization (CSR)}, a novel training objective designed to forge a strong, causal-like dependence between a model's output and its intermediate reasoning steps. During training, CSR performs automated, operator-level interventions on the generated reasoning trace (e.g., swapping ``+''with ``-'') to create a minimally-perturbed counterfactual. A regularization term then penalizes the model if this logically flawed trace still yields the original answer. Our efficient implementation adds only 8.7\% training overhead through warm-start curriculum and token-subset optimization. We evaluate faithfulness using \textbf{Counterfactual Outcome Sensitivity (COS)}, a metric quantifying how sensitive the final answer is to such logical perturbations. Across diverse structured reasoning benchmarks -- arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop QA (HotpotQA), and code generation (MBPP) -- models trained with CSR demonstrate a vastly superior trade-off between accuracy and faithfulness. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, with this learned sensitivity generalizing to larger models and enhancing the performance of inference-time techniques like self-consistency.",arXiv.org,2,White-box,General,该论文明确研究 CoT 的忠实性问题，提出了一种新的训练目标 CSR，通过反事实干预来增强模型输出与中间推理步骤之间的因果依赖关系。论文还提出了衡量忠实性的指标 COS，并验证了 CSR 在多个结构化推理任务上的有效性，显著提高了忠实性。
Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?,2025,True,True,False,,论文直接研究CoT忠实性，揭露不忠实现象（如事后合理化），但未提出新度量方法或改进方案。,"Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",,2,Black-box,General,论文明确研究 CoT 的忠实性（faithfulness），探讨 CoT 是否是模型预测的真实原因，以及 CoT 是否可能是不忠实的事后合理化（post-hoc rationalisation）。
Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?,2025,True,True,False,Training & Fine-tuning,论文探讨了CoT的解释性与性能的关系，揭示了不忠实现象，并提出了改进方法。,"Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}""We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.",arXiv.org,1,Black-box,General,该论文研究了 CoT 推理痕迹的语义可解释性是否影响 LLM 任务性能，虽然未直接探讨 Faithfulness，但涉及了 CoT 的效用（Utility）和可解释性，可以作为 Faithfulness 的旁证。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确研究多模态CoT中生成的rationale是否被模型实际利用（即Faithfulness问题），并提出RED方法通过KL约束强化rationale对预测的因果影响，直接解决'忽视rationale内容'的忠实度挑战。
REV: Information-Theoretic Evaluation of Free-Text Rationales,2022,True,False,True,,论文提出了REV指标评估自由文本推理的新信息，与CoT忠实性相关。,"Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文提出了一个信息论指标 REV 来评估自由文本解释的新信息量，虽然未直接研究 CoT 的忠实度，但提供了评估解释质量的工具，可能间接支持 Faithfulness 研究。
Measuring Association Between Labels and Free-Text Rationales,2020,True,True,True,,论文研究了自由文本理由的忠实性，提出了评估指标，但未提出改进方法。,"In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文明确研究自由文本解释（类似CoT）是否忠实反映模型决策过程，提出了测量标签与解释关联性的方法（如鲁棒性等价和特征重要性一致性），直接符合Faithfulness的核心定义。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文明确提出了通过分解任务生成子问题并提供自然语言解释的方法，强调了解释的忠实性（faithful natural language explanation），符合 CoT Faithfulness 的核心定义。
Semi-structured LLM Reasoners Can Be Rigorously Audited,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出半结构化推理模型（SSRMs）以解决不忠实现象，并引入自动审计方法来检测推理错误。,"Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.",arXiv.org,2,Black-box,General,该论文明确研究LLM推理的忠实性问题，提出半结构化推理模型（SSRMs）以生成可审计的推理轨迹，旨在检测推理中的错误和遗漏，直接关联到CoT Faithfulness的核心定义。
Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Explanations for Model Predictions,2022,True,True,False,Training & Fine-tuning,论文讨论了模型生成的解释与下游任务效用不一致的现象，并提出了通过微调改进的方法。,"Models able to generate free-text rationales that explain their output have been proposed as an important step towards interpretable NLP for “reasoning” tasks such as natural language inference and commonsense question answering. However, the relative merits of different architectures and types of rationales are not well understood and hard to measure. In this paper, we contribute two insights to this line of research: First, we find that models trained on gold explanations learn to rely on these but, in the case of the more challenging question answering data set we use, fail when given generated explanations at test time. However, additional fine-tuning on generated explanations teaches the model to distinguish between reliable and unreliable information in explanations. Second, we compare explanations by a generation-only model to those generated by a self-rationalizing model and find that, while the former score higher in terms of validity, factual correctness, and similarity to gold explanations, they are not more useful for downstream classification. We observe that the self-rationalizing model is prone to hallucination, which is punished by most metrics but may add useful context for the classification step.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,1,Black-box,General,论文研究了自由文本解释的效用和模型对解释的依赖，但未明确探讨解释是否忠实反映了模型的实际计算过程。虽然提到了模型对生成解释的区分能力，但主要关注的是下游分类的效用，而非解释的忠实度。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",1,Black-box,General,论文提到 Chain-of-Thoughts 是 faithful 且有助于提升 prompt 性能，但未深入探讨 CoT 是否真实反映模型的计算过程。主要关注的是 ChatGPT 在因果语言理解上的表现，属于边缘相关。
ERASER: A Benchmark to Evaluate Rationalized NLP Models,2019,True,False,True,,论文提出了评估模型解释忠实性的新指标和框架，与CoT Faithfulness相关。,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确提出了一个基准测试（ERASER），用于评估模型提供的解释（rationales）是否忠实（faithful），即这些解释是否真正影响了模型的预测。这与CoT Faithfulness的核心定义直接相关，因为它关注的是解释与模型预测之间的因果关系。
Self-Critique and Refinement for Faithful Natural Language Explanations,2025,True,True,True,Prompting & In-Context Learning,论文提出通过自我批评和细化框架提升解释的忠实性，涉及不忠实现象和改进方法。,"With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.",,2,Black-box,General,该论文明确研究如何通过自我批判和精炼框架（SR-NLE）来提高自然语言解释（NLEs）的忠实度，减少解释与模型实际推理过程之间的不一致性。论文还引入了基于特征归因的反馈机制，直接涉及测量和改善解释的忠实度，符合核心定义中的关键词和标准。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,False,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了神经网络模块（NMN）的中间输出是否忠实反映了模型的推理过程，发现模块结构与实际行为不一致，并通过辅助监督和模块架构改进提升忠实度。这直接符合Faithfulness的核心定义，尤其是关于解释是否真实反映模型计算过程的研究。
A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations,2025,True,True,True,Training & Fine-tuning,论文提出了PEX一致性度量，揭示了LLM生成解释的不一致现象，并采用直接偏好优化提升忠实性。,"Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",,2,Black-box,General,该论文明确研究了解释的忠实度（Faithfulness），提出了 Prediction-EXplanation (PEX) consistency 作为衡量指标，并分析了语言模型生成的解释中缺乏一致性的问题。此外，论文还探讨了如何通过优化提高解释的忠实度，符合 CoT Faithfulness 的核心研究范畴。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,该论文明确研究如何生成忠实的解释（faithful rationales），即模型预测的真实原因，符合Faithfulness的核心定义。论文提出了一种训练框架，旨在确保模型输出的解释真实反映其决策过程，这与CoT Faithfulness的研究目标高度一致。
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?,2020,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出新的度量标准LAS评估解释的忠实性，并讨论了模型生成解释的合理性问题。,"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",Findings,2,Black-box,General,该论文明确研究模型生成的自然语言解释是否支持实际模型行为（faithfulness），而不仅仅是与人类解释的表面相似性（plausibility）。论文提出了泄漏调整可模拟性（LAS）指标，用于评估解释是否帮助观察者预测模型的输出，同时控制解释直接泄漏输出的情况。这与CoT Faithfulness的核心定义高度相关，因为它关注解释是否真实反映了模型的预测过程。
Misaligning Reasoning with Answers - A Framework for Assessing LLM CoT Robustness,2025,True,True,True,,论文提出了评估框架MATCHA，揭示了LLMs在输入扰动下的不一致推理现象，与CoT忠实性相关。,"LLMs' decision-making process is opaque, prompting the need for explanation techniques like Chain-of-Thought. To investigate the relationship between answer and reasoning, we design a novel evaluation framework, MATCHA. In domains like education and healthcare, reasoning is key for model trustworthiness. MATCHA reveals that LLMs under input perturbations can give inconsistent or nonsensical reasoning. Additionally, we use LLM judges to assess reasoning robustness across models. Our results show that LLMs exhibit greater vulnerability to input perturbations for multi-step and commonsense tasks than compared to logical tasks. Also, we show non-trivial transfer rates of our successful examples to black-box models. Our evaluation framework helps to better understand LLM reasoning mechanisms and guides future models toward more robust and reasoning-driven architectures, enforcing answer-reasoning consistency.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的忠实度问题，通过设计评估框架 MATCHA 来揭示 LLM 在输入扰动下可能产生不一致或无意义的推理，这与 Faithfulness 的核心定义直接相关。论文还探讨了答案与推理之间的一致性，符合研究 CoT 是否是模型预测的真实原因的标准。
PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出生成详细的 CoT 解释，并评估其 grounded 和清晰度，涉及忠实性度量。,"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-ofthought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded (score 0.87) and clear (readability 4.5/5). This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,1,Black-box,General,论文提出了一个框架，结合了传感器数据和视觉语言模型生成详细的链式思考解释，并评估了解释的grounded程度和清晰度。虽然涉及CoT的解释生成和评估，但未明确研究这些解释是否忠实反映了模型的实际计算过程，因此属于边缘相关。
On the Faithfulness of Visual Thinking: Measurement and Enhancement,2025,True,True,True,Training & Fine-tuning,论文揭示了视觉信息在MCoT中的不忠实现象，并提出了新的度量方法和改进策略。,"Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",arXiv.org,2,White-box,General,该论文明确研究了视觉-文本多模态链式思考（MCoT）的忠实度问题，通过干预实验（视觉和文本干预）揭示了视觉信息在推理过程中被忽视的现象，并提出了量化视觉线索忠实度的指标（可靠性和充分性）。此外，论文还提出了一种新的学习策略（SCCM）来增强视觉忠实度，这直接符合Faithfulness的核心定义。
The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction,2025,True,True,False,Interpretability & Internal Mechanisms,论文揭示了LLMs在推理和记忆之间的动态切换现象，并提出了通过干预内部机制来提升推理忠实性的方法。,"Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.",arXiv.org,2,White-box,General,该论文明确研究了语言模型在推理和记忆之间的动态平衡，通过识别模型残差流中的线性特征来控制真正的推理和记忆回忆之间的平衡。这些特征不仅可以区分推理任务和记忆密集型任务，还可以被操纵以因果性地影响模型在推理任务上的表现。这与Faithfulness的核心定义高度相关，因为它探讨了模型生成解释（CoT/Reasoning Trace）是否真实反映了模型做出最终预测的实际计算过程。
Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了FUR框架，通过消除推理步骤中的信息来测量忠实性，揭示了不忠实现象并提出了新的度量方法。,"When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",,2,White-box,General,该论文明确研究 CoT 的忠实度，提出了一个框架（FUR）来测量生成的推理步骤是否忠实于模型的参数信念。通过从模型参数中擦除推理步骤的信息，并测量对模型预测的影响，来评估忠实度。这直接符合 Faithfulness 的核心定义，即研究 CoT 是否是模型预测的真实原因。
Effectively Controlling Reasoning Models through Thinking Intervention,2025,True,False,False,Interpretability & Internal Mechanisms,论文提出通过干预内部推理过程来增强模型行为控制，与CoT忠实性相关。,"Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval and Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.",arXiv.org,1,White-box,General,该论文提出了Thinking Intervention方法，通过干预内部推理过程来控制模型行为，虽然未直接研究CoT的忠实度，但涉及了推理过程的内部机制，属于边缘相关。
Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations,2025,True,True,False,Training & Fine-tuning,论文揭示了偏好优化导致CoT不忠实的问题，并提出了通过因果归因改进的方法。,"Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in""reward hacking""by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",arXiv.org,2,Black-box,General,该论文明确研究了CoT解释的忠实度问题，指出偏好优化可能导致解释不忠实（reward hacking），并提出了通过因果归因来检测解释与决策过程之间差异的方法，直接符合Faithfulness的核心定义。
Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations,2025,True,True,True,,论文聚焦CoT忠实性度量，揭示事后找补现象，提出基于反事实和贝叶斯模型的新评估框架，但未提出改进方法。,"Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's""reasoning""process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.",International Conference on Learning Representations,2,Black-box,General,该论文明确研究LLM解释的忠实性（Faithfulness），提出了定义和测量方法，并通过反事实输入（Counterfactual input）量化概念的影响，属于核心相关研究。
SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过对抗自演游戏提升 CoT 忠实性，属于改进方法中的训练与微调。,"Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a""sneaky generator""that deliberately produces erroneous steps designed to be difficult to detect, and a""critic""that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.",arXiv.org,1,Black-box,General,该论文通过对抗自博弈游戏评估LLM推理步骤的可靠性，虽然未直接研究CoT的忠实度，但涉及推理步骤的正确性分析，可作为Faithfulness的旁证。
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,2025,True,True,False,,论文揭示了CoT推理中的不忠实现象，如事后找补和逻辑捷径，但未提出新的度量或改进方法。,"Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful when models face an explicit bias in their prompts, i.e., the CoT can give an incorrect picture of how models arrive at conclusions. We go further and show that unfaithful CoT can also occur on realistic prompts with no artificial bias. We find that when separately presented with the questions""Is X bigger than Y?""and""Is Y bigger than X?"", models sometimes produce superficially coherent arguments to justify systematically answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We show preliminary evidence that this is due to models' implicit biases towards Yes or No, thus labeling this unfaithfulness as Implicit Post-Hoc Rationalization. Our results reveal that several production models exhibit surprisingly high rates of post-hoc rationalization in our settings: GPT-4o-mini (13%) and Haiku 3.5 (7%). While frontier models are more faithful, especially thinking ones, none are entirely faithful: Gemini 2.5 Flash (2.17%), ChatGPT-4o (0.49%), DeepSeek R1 (0.37%), Gemini 2.5 Pro (0.14%), and Sonnet 3.7 with thinking (0.04%). We also investigate Unfaithful Illogical Shortcuts, where models use subtly illogical reasoning to try to make a speculative answer to hard maths problems seem rigorously proven. Our findings raise challenges for strategies for detecting undesired behavior in LLMs via the chain of thought.",arXiv.org,2,Black-box,General,该论文明确研究了 CoT 的忠实性问题，特别是模型在面对无人工偏见的现实提示时产生的非忠实性 CoT（Implicit Post-Hoc Rationalization 和 Unfaithful Illogical Shortcuts）。论文还提供了不同模型在忠实性方面的具体数据，直接涉及 CoT 是否真实反映模型预测过程的核心问题。
Bonsai: Interpretable Tree-Adaptive Grounded Reasoning,2025,True,False,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种可解释的推理系统，强调透明推理和验证，与CoT忠实性相关。,"To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.",arXiv.org,1,Black-box,General,该论文提出了一个可解释的推理系统Bonsai，能够生成适应性推理树，并通过检索相关证据来计算子主张的可能性。虽然它关注推理的透明性和不确定性，但并未明确研究CoT是否真实反映模型预测的实际计算过程，因此属于边缘相关。
Watch Your Steps: Observable and Modular Chains of Thought,2024,True,True,False,Verification & External Tools,论文提出了一种新的 CoT 变体，通过形式化步骤增强可观察性，并讨论了非局部错误问题。,"We propose a variant of chain of thought (CoT) prompting called Program Trace Prompting that makes explanations more observable while preserving the power, generality and flexibility of CoT. In our approach, few-shot CoT demonstrations are wrapped in a formal syntax based on Python, and each prompt: identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples. Program Trace Prompting is applicable to many tasks, achieving strong results on the 23 diverse tasks in the BIG-Bench Hard benchmark. More importantly, by instrumenting explanations in this way, we enable new types of analysis. In particular, we identify""non-local errors""(which correspond to incorrectly learning the reasoning method illustrated in the demonstrations) as an unaddressed issue in CoT learning, and we present methods for verifying the modularity of steps in a CoT explanation.",arXiv.org,1,Black-box,General,该论文提出了Program Trace Prompting方法，通过形式化步骤增强CoT的可观察性和模块化分析能力，虽然未直接研究Faithfulness，但其对步骤模块化的验证方法可能间接反映推理过程的忠实度。
Anchored Alignment for Self-Explanations Enhancement,2024,True,False,False,Training & Fine-tuning,论文提出了一种增强模型自我解释能力的方法，涉及训练和微调策略。,"In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.",arXiv.org,1,Black-box,General,论文研究如何通过对齐方法增强LLM的自我解释能力，但未明确探讨解释是否忠实反映模型的真实计算过程。虽然涉及解释质量评估，但更侧重于解释的效用而非因果分析。
Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models,2024,True,True,True,Interpretability & Internal Mechanisms,论文提出使用激活修补技术测量解释的忠实性，并提出了新的度量标准。,"Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \url{https://github.com/wj210/Causal-Faithfulness}",,2,White-box,General,该论文明确研究了解释的忠实性（Faithfulness），提出了一种新的度量方法 Causal Faithfulness，利用激活修补（activation patching）技术来测量解释与模型内部计算的一致性。这符合核心相关性的定义，且采用了白盒方法分析模型内部机制。
SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop Question Answering Based on Finite State Machine,2024,True,True,False,Prompting & In-Context Learning,论文提出了一种自我引导的提示方法，旨在通过分解问题和自我纠正来增强推理的忠实性。,"Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过改进提示策略（SG-FSM）来提升多跳问答任务的性能，减少幻觉和错误传播，但并未涉及对模型生成解释（CoT）是否忠实反映模型内部计算过程的研究。因此，与CoT Faithfulness的核心定义无关。
OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models,2024,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出了离线评估框架OCEAN，并利用知识图谱和RL优化CoT的忠实性。,"Offline evaluation of LLMs is crucial in understanding their capacities, though current methods remain underexplored in existing research. In this work, we focus on the offline evaluation of the chain-of-thought capabilities and show how to optimize LLMs based on the proposed evaluation method. To enable offline feedback with rich knowledge and reasoning paths, we use knowledge graphs (e.g., Wikidata5m) to provide feedback on the generated chain of thoughts. Due to the heterogeneity between LLM reasoning and KG structures, direct interaction and feedback from KGs on LLM behavior are challenging, as they require accurate entity linking and grounding of LLM-generated chains of thought in the KG. To address the above challenge, we propose an offline chain-of-thought evaluation framework, OCEAN, which models chain-of-thought reasoning in LLMs as an MDP and evaluate the policy's alignment with KG preference modeling. To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy KG exploration and RL to model a KG policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating KG reasoning preference. Then we incorporate the knowledge-graph feedback on the validity and alignment of the generated reasoning paths into inverse propensity scores and propose KG-IPS estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS estimator and provide a lower bound on its variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance chain-of-thought alignment. Our empirical study shows that OCEAN can be efficiently optimized for generating chain-of-thought reasoning paths with higher estimated values without affecting LLMs' general abilities in downstream tasks or their internal knowledge.",arXiv.org,1,Black-box,General,论文主要研究如何通过知识图谱反馈优化LLM的CoT能力，并提出了离线评估框架OCEAN。虽然涉及CoT的评估和优化，但未明确探讨CoT是否真实反映模型的预测过程，因此属于边缘相关。
A Causal Lens for Evaluating Faithfulness Metrics,2025,True,True,True,,论文提出了评估忠实性指标的因果框架，并讨论了不忠实现象和度量方法。,"Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.",,2,White-box,General,该论文明确研究自然语言解释（包括CoT）的忠实性，提出了一个评估忠实性指标的因果框架，并生成了忠实与非忠实解释对。此外，论文还评估了包括基于CoT的方法在内的多种忠实性指标，直接涉及CoT Faithfulness的核心研究范畴。
A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,2025,True,True,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文探讨了 CoT 监控对安全性的影响，涉及 faithfulness 问题和验证工具。,"As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 监控的忠实度（Faithfulness），探讨了模型在 CoT 启用时可能产生的危险能力是否可检测，并分析了威胁监控的三种形式（语言漂移、隐写术和异质推理）。此外，论文还评估了保持 CoT 忠实度的现有和新颖技术，直接涉及 CoT 是否真实反映模型预测的实际计算过程。
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,2,Black-box,General,该论文明确研究LLM解释的忠实性问题，提出了FaithLM框架来评估和改进解释的忠实性，符合核心定义中的Faithfulness研究范畴。
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,2024,True,True,False,,论文讨论了LLM生成自解释的忠实性与合理性之间的对立，揭示了不忠实现象。,"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",arXiv.org,2,Black-box,General,"论文明确讨论了LLM生成的自解释（Self-explanations, SEs）的忠实性（faithfulness）与合理性（plausibility）之间的二分法，指出虽然LLM能生成看似合理的解释，但这些解释并不一定反映模型的真实推理过程。这与CoT Faithfulness的核心定义直接相关，即研究解释是否真实反映了模型的实际计算过程。"
Digital Socrates: Evaluating LLMs through explanation critiques,2023,True,True,True,Training & Fine-tuning,论文提出自动评估模型解释质量的方法，涉及 CoT 忠实性评估和改进。,"While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文研究的是对LLM生成的解释进行批判性评估，并开发自动评估工具。虽然涉及解释的质量和缺陷，但并未明确探讨解释是否忠实反映模型的内部计算过程（Faithfulness），而是更侧重于解释的实用性和评估方法。
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了LLMs在生成答案时不忠实于中间推理步骤的现象，并提出了FRODO框架来改进忠实性。,"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文明确研究了CoT的忠实度问题，通过因果中介分析验证了LLM是否可靠地使用中间推理步骤来生成最终答案，并提出了FRODO框架以提高推理的忠实度。这直接符合Faithfulness的核心定义，即模型生成的解释是否真实反映了其预测的实际计算过程。
The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models,2024,True,False,True,,论文提出了新的度量指标CEF和CCT，用于评估CoT的忠实性，但未讨论不忠实现象或改进方法。,"In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究大型语言模型生成的自然语言解释的忠实性问题，提出了新的度量标准（CEF）来评估解释是否真实反映模型的预测过程，并基于输入干预进行测试。这直接符合Faithfulness的核心定义，特别是关于解释是否真实捕捉模型预测因素的问题。
How Likely Do LLMs with CoT Mimic Human Reasoning?,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了LLMs在CoT中偏离理想因果链的现象，并探讨了影响因果结构的因素。,"Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",International Conference on Computational Linguistics,2,Black-box,General,该论文明确研究LLMs的CoT推理过程与人类推理的差异，并通过因果分析探讨问题指令、推理和答案之间的关系，揭示了LLMs常常偏离理想的因果链，导致虚假相关性和潜在的一致性错误。这与Faithfulness的核心定义高度相关，即研究CoT是否真实反映了模型的实际计算过程。
Can LLMs Explain Themselves Counterfactually?,2025,True,True,True,,论文研究了LLM自我生成反事实解释的能力，揭示了不忠实现象并提出了评估方法。,"Explanations are an important tool for gaining insights into the behavior of ML models, calibrating user trust and ensuring regulatory compliance. Past few years have seen a flurry of post-hoc methods for generating model explanations, many of which involve computing model gradients or solving specially designed optimization problems. However, owing to the remarkable reasoning abilities of Large Language Model (LLMs), self-explanation, that is, prompting the model to explain its outputs has recently emerged as a new paradigm. In this work, we study a specific type of self-explanations, self-generated counterfactual explanations (SCEs). We design tests for measuring the efficacy of LLMs in generating SCEs. Analysis over various LLM families, model sizes, temperature settings, and datasets reveals that LLMs sometimes struggle to generate SCEs. Even when they do, their prediction often does not agree with their own counterfactual reasoning.",,2,Black-box,General,该论文明确研究LLM生成的反事实自解释（SCEs）与模型预测之间的一致性，直接涉及Faithfulness的核心问题（预测与解释是否因果关联）。通过设计测试衡量LLM生成SCEs的效能，并发现模型预测常与自身反事实推理矛盾，揭示了Unfaithful现象。
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了CoT中的不忠实现象（Toxic CoT问题），并提出了基于内部机制探测的改进方法。,"Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了CoT推理过程中的信息丢失问题，使用内部机制分析方法（如attribution tracing和causal tracing），并且设计了RIDERS方法来补偿信息缺失，直接关联到CoT的真实性和忠实度问题。
Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,2024,True,True,False,,论文揭示了CoT中可能存在的隐藏计算现象，与忠实性相关。,"Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.",arXiv.org,2,White-box,General,该论文明确研究了 Chain-of-Thought (CoT) 是否真实反映了模型的计算过程，揭示了中间标记（如无意义的填充标记）可以独立于其语义内容提供计算优势，这表明 CoT 可能并不忠实于模型的实际推理过程。论文还提出了对 CoT 忠实性的担忧，即模型可能进行与观察到的 CoT 标记脱节的不可审计的隐藏计算，这直接涉及 CoT Faithfulness 的核心问题。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,2,Black-box,General,论文明确研究 Chain-of-Thought 的解释性，并特别提到 faithfulness 作为评估的一个维度。此外，还研究了 robustness 和 utility，这些都是与 CoT Faithfulness 相关的核心内容。
Understanding Reasoning in Chain-of-Thought from the Hopfieldian View,2024,True,False,True,Interpretability & Internal Mechanisms,论文提出了基于Hopfieldian认知视角的RoT框架，增强CoT推理的鲁棒性和可解释性。,"Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on improving performance, lacking a comprehensive framework to explain and understand the fundamental factors behind CoT's success. To bridge this gap, we introduce a novel perspective grounded in the Hopfieldian view of cognition in cognitive neuroscience. We establish a connection between CoT reasoning and key cognitive elements such as stimuli, actions, neural populations, and representation spaces. From our view, we can understand the reasoning process as the movement between these representation spaces. Building on this insight, we develop a method for localizing reasoning errors in the response of CoTs. Moreover, we propose the Representation-of-Thought (RoT) framework, which leverages the robustness of low-dimensional representation spaces to enhance the robustness of the reasoning process in CoTs. Experimental results demonstrate that RoT improves the robustness and interpretability of CoT reasoning while offering fine-grained control over the reasoning process.",arXiv.org,1,White-box,General,论文从Hopfieldian认知视角研究CoT的内部机制，涉及神经群体和表征空间等认知元素，并定位推理错误，但未明确讨论CoT是否忠实反映模型的实际计算过程。
Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology,2025,True,False,True,Verification & External Tools,论文提出基于推理拓扑的框架量化LLM解释的不确定性，与CoT忠实性相关，但未讨论不忠实现象或改进方法。,"Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",arXiv.org,2,Black-box,General,该论文明确研究LLM解释的不确定性，并基于推理拓扑视角量化这种不确定性，直接关联到Faithfulness和推理一致性。通过分解解释为知识相关的子问题和基于拓扑的推理结构，该方法能够评估知识冗余并提供对推理过程的可解释性洞察，从而增强鲁棒性和忠实度。
Unveiling and Causalizing CoT: A Causal Pespective,2025,True,True,True,Interpretability & Internal Mechanisms,论文从因果视角揭示和纠正CoT中的错误步骤，提升忠实性和可理解性。,"Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing the reasoning ability of large language models (LLMs), the mechanism of CoT remains a ``black box''. Even if the correct answers can frequently be obtained, existing CoTs struggle to make the reasoning understandable to human. In this paper, we unveil and causalize CoT from a causal perspective to ensure both correctness and understandability of all reasoning steps (to the best of our knowledge, the first such). We model causality of CoT via structural causal models (SCM) to unveil the reasoning mechanism of CoT. To measure the causality of CoT, we define the CoT Average Causal Effect (CACE) to test the causal relations between steps. For those steps without causality (wrong or unintelligible steps), we design a role-playing causal query algorithm to causalize these steps, resulting a causalized CoT with all steps correct and understandable. Experimental results on both open-source and closed-source LLMs demonstrate that the causal errors commonly in steps are effectively corrected and the reasoning ability of LLMs is significantly improved.",arXiv.org,2,White-box,General,该论文明确从因果视角研究CoT的机制，通过结构因果模型（SCM）和定义的CoT平均因果效应（CACE）来测试步骤间的因果关系，直接涉及CoT的忠实度问题。
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,2024,True,True,False,Training & Fine-tuning,论文探讨了微调对CoT推理忠实性的影响，揭示了不忠实现象。,"Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",North American Chapter of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了微调对CoT推理忠实度的影响，探讨了微调如何改变LLM的内部机制，并直接提及了CoT推理的faithfulness问题。
Automated Assessment of Fidelity and Interpretability: An Evaluation Framework for Large Language Models' Explanations (Student Abstract),2024,True,False,True,,论文提出了评估LLM解释忠实性的框架，但未讨论不忠实现象或改进方法。,"As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability: fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations. We apply our framework to evaluate GPT-3.5 and the impact of prompts on the quality of its explanations. In conclusion, our framework streamlines the evaluation of explanations from LLMs, promoting the development of safer models.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究了解释的忠实度（fidelity），并提出了评估框架，特别关注专有LLM无法直接访问内部特征时的评估方法。这与CoT Faithfulness的核心定义直接相关，尤其是关于解释是否真实反映模型预测过程的研究。
Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,2023,True,True,True,Training & Fine-tuning,论文探讨了多粒度理由提取的忠实性和一致性，并提出了新的评估指标。,"The success of deep learning models on multi-hop fact verification has prompted researchers to understand the behavior behind their veracity. One possible way is erasure search: obtaining the rationale by entirely removing a subset of input without compromising the veracity prediction. Although extensively explored, existing approaches fall within the scope of the single-granular (tokens or sentences) explanation, which inevitably leads to explanation redundancy and inconsistency. To address such issues, this paper explores the viability of multi-granular rationale extraction with consistency and faithfulness for explainable multi-hop fact verification. In particular, given a pretrained veracity prediction model, both the token-level explainer and sentence-level explainer are trained simultaneously to obtain multi-granular rationales via differentiable masking. Meanwhile, three diagnostic properties (fidelity, consistency, salience) are introduced and applied to the training process, to ensure that the extracted rationales satisfy faithfulness and consistency. Experimental results on three multi-hop fact verification datasets show that the proposed approach outperforms some state-of-the-art baselines.",arXiv.org,2,White-box,General,该论文明确研究多粒度理由提取的忠实性（faithfulness）和一致性（consistency），并引入了诊断属性（如fidelity）来确保提取的理由满足忠实性。这与CoT Faithfulness的核心定义直接相关，即模型生成的解释是否真实反映了模型做出预测的实际计算过程。
Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models,2025,True,True,False,Training & Fine-tuning,论文讨论了模型内部推理痕迹与最终输出的对齐问题，涉及不忠实现象，并提到通过SFT、DPO和GRPO等训练方法改进。,"Recent advances in post-training techniques have endowed Large Language Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive tasks through the generation of supplementary planning tokens. This development raises a fundamental question: Are these models aware of what they""learn""and""think""? To address this, we define three core competencies: (1) awareness of learned latent policies, (2) generalization of these policies across domains, and (3) alignment between internal reasoning traces and final outputs. We empirically evaluate these abilities on several tasks, each designed to require learning a distinct policy. Furthermore, we contrast the profiles of models post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization (DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate that RL-trained models not only demonstrate greater awareness of their learned behaviors and stronger generalizability to novel, structurally similar tasks than SFT models but also often exhibit weak alignment between their reasoning traces and final outputs, an effect most pronounced in GRPO-trained models.",arXiv.org,2,White-box,General,论文明确研究了内部推理轨迹（reasoning traces）与最终输出的对齐问题（alignment），这直接涉及CoT的忠实度（Faithfulness）。特别是发现RL训练的模型在推理轨迹与输出之间存在弱对齐现象，符合核心定义中关于'Post-hoc Rationalization'的研究。此外，论文还对比了不同训练方法（SFT/DPO/GRPO）对模型内部机制的影响，属于白盒分析。
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,该论文明确研究多模态推理中的忠实度问题，提出通过过程级监督（rubric-based generative rewards）来改善推理的忠实性，并专门评估了推理的忠实性改进。这直接符合Faithfulness的核心定义，即确保生成的推理过程真实反映模型的实际计算过程。
Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation,2025,True,True,True,Training & Fine-tuning,论文讨论了CoT不忠实现象，提出了新的度量框架，并通过RL改进忠实性。,"Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",arXiv.org,2,Black-box,General,该论文明确研究检索增强生成中的推理忠实度问题，提出了三个具体的忠实度指标（信息-思考、思考-答案、思考-搜索忠实度），并设计了VERITAS框架来增强推理过程的忠实性。这直接符合CoT Faithfulness的核心定义，即关注中间推理步骤是否真实反映模型的预测过程。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究生成忠实解释（faithful rationales）的问题，并提出了一种新的损失函数（MLR）来促进解释与预测标签之间的强条件关系，符合Faithfulness的核心定义。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确提出了 Chain-of-Knowledge (CoK) prompting 方法，并引入了 F^2-Verification 方法来评估推理链的可靠性和忠实度（faithfulness）。论文直接关注了生成的推理链是否真实反映了模型的预测过程，并提出了具体的验证方法，因此属于核心相关的研究范畴。
Verifying Chain-of-Thought Reasoning via Its Computational Graph,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了一种白盒方法验证CoT的计算图，直接关联模型内部电路与推理过程，符合Faithfulness定义。,"Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",arXiv.org,2,White-box,General,该论文明确研究 CoT 的计算图（computational graph）作为模型潜在推理电路的执行轨迹，并验证其结构特征是否能预测推理错误。这直接涉及 CoT 是否真实反映模型的计算过程（Faithfulness），且通过白盒方法（如干预 transcoder 特征）证明其因果性，符合核心相关标准。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,该论文明确研究自然语言解释（NLEs）的忠实性问题，提出了一种框架（G-Tex）来增强NLEs的忠实性，通过使用highlight explanations作为忠实线索来指导NLE生成，确保生成的解释与模型的内部推理过程一致。这直接符合Faithfulness的定义，即解释是否真实反映了模型的实际计算过程。
Investigating Self-Rationalizing Models for Commonsense Reasoning,2023,True,True,False,Training & Fine-tuning,论文探讨了自解释模型的忠实性问题，并提出了通过微调改进的方法。,"The rise of explainable natural language processing spurred a bulk of work on datasets augmented with human explanations, as well as technical approaches to leverage them. Notably, generative large language models offer new possibilities, as they can output a prediction as well as an explanation in natural language. This work investigates the capabilities of fine-tuned text-to-text transfer Transformer (T5) models for commonsense reasoning and explanation generation. Our experiments suggest that while self-rationalizing models achieve interesting results, a significant gap remains: classifiers consistently outperformed self-rationalizing models, and a substantial fraction of model-generated explanations are not valid. Furthermore, training with expressive free-text explanations substantially altered the inner representation of the model, suggesting that they supplied additional information and may bridge the knowledge gap. Our code is publicly available, and the experiments were run on open-access datasets, hence allowing full reproducibility.",Stats,1,White-box,General,该论文研究了自解释模型在常识推理中的表现，发现模型生成的解释存在不有效的问题，并分析了训练过程中内部表征的变化。虽然未直接探讨CoT的忠实度，但涉及解释的有效性和模型内部机制，属于边缘相关。
Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了评估框架对CoT长度和答案合规性的影响，揭示了不忠实现象，并提出了改进方法。,"Benchmarks for large language models (LLMs) often rely on rubric-scented prompts that request visible reasoning and strict formatting, whereas real deployments demand terse, contract-bound answers. We investigate whether such""evaluation scent""inflates measured performance without commensurate capability gains. Using a single open-weights model (GPT-OSS-20B), we run six paired A/B scenarios that hold task content and decoding fixed while varying framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High): deterministic math, strict code-fix, citation generation, incentive flips (caution vs. competence), CoT visibility, and multilingual (Urdu) headers. Deterministic validators compute accuracy, answer-only compliance, hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with pre-registered deltas and composite indices. Across scenarios, evaluation framing reliably inflates CoT (hundreds to>1000 characters) and reduces answer-only compliance, with limited or inconsistent accuracy gains. In structured outputs, it improves wrappers (e.g., fenced blocks, enumerated lists) but not regex-validated substance. Incentive wording reweights error composition: praising caution modestly improves accuracy at high reasoning and reduces wrong-but-confident errors, whereas praising competence yields terser but riskier outputs. Urdu rubric headers reproduce these signatures and can decrease accuracy at higher reasoning depth, indicating multilingual parity risks. We provide a reproducible A/B framework (prompt banks, validators, per-run scores, scripts; versioned DOI) and practical guidance: neutral phrasing or dual-framing checks, contract-aware grading, style-delta reporting, confidence governance, and multilingual dashboards to ensure that benchmark gains reflect deployable capability.",arXiv.org,2,Black-box,General,该论文明确研究了评估框架对CoT长度和答案合规性的影响，揭示了评估导向的提示可能导致CoT的膨胀而不伴随准确性的提升，这与CoT Faithfulness的核心问题——解释是否真实反映模型的计算过程——直接相关。
FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning,2025,True,True,True,,论文提出了一个实例级 CoT 忠实性检测的基准，并讨论了不忠实现象和评估方法。,"Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实性问题，提出了一个实例级别的忠实性检测基准（FaithCoT-Bench），并分析了不同检测方法的优缺点。论文关注的是 CoT 是否真实反映模型的内部推理过程，属于核心相关的研究。
Evaluating Human Alignment and Model Faithfulness of LLM Rationale,2024,True,True,True,Training & Fine-tuning,论文研究了LLM生成rationale的忠实性，揭示了不忠实现象并提出了评估框架。,"We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as""aligned""with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less""faithful""than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",arXiv.org,2,White-box,General,论文明确研究LLM生成的rationale是否忠实反映模型决策过程，对比了基于提示和基于技术归因的方法，并发现基于提示的自我解释在忠实度上不如基于归因的解释。这直接涉及CoT Faithfulness的核心问题，即解释是否真实反映模型的计算过程。
Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了奖励黑客行为，提出TRACE度量方法，并涉及训练改进。,"Reward hacking, where a reasoning model exploits loopholes in a reward function to achieve high rewards without solving the intended task, poses a significant threat. This behavior may be explicit, i.e. verbalized in the model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE (Truncated Reasoning AUC Evaluation). Our key observation is that hacking occurs when exploiting the loophole is easier than solving the actual task. This means that the model is using less'effort'than required to achieve high reward. TRACE quantifies effort by measuring how early a model's reasoning becomes sufficient to obtain the reward. We progressively truncate a model's CoT at various lengths, force the model to answer, and estimate the expected reward at each cutoff. A hacking model, which takes a shortcut, will achieve a high expected reward with only a small fraction of its CoT, yielding a large area under the accuracy-vs-length curve. TRACE achieves over 65% gains over our strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B monitor in coding. We further show that TRACE can discover unknown loopholes during training. Overall, TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 是否真实反映模型预测的实际计算过程，特别是通过检测隐式奖励黑客行为（implicit reward hacking）来评估 CoT 的忠实度。论文提出的 TRACE 方法通过截断 CoT 并测量预期奖励来量化模型的'努力'，直接关联到 CoT 是否忠实于模型的真实推理过程。
Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning,2025,True,True,False,,论文研究了CoT的忠实性问题，揭示了不忠实现象，如事后找补和捷径学习。,"Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",arXiv.org,2,Black-box,General,该论文明确研究CoT生成的rationales是否忠实于底层计算过程，而非事后解释。通过控制提示中的hint条件，评估模型是否明确承认hint，揭示了模型推理过程中对shortcuts的依赖，直接涉及Faithfulness的核心问题。
Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,2025,True,True,True,,论文揭示了CoT可能被模型故意模糊以逃避监测的不忠实现象，并提出了评估框架。,"Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection? To answer this question and thus stress-test CoT monitorability, we develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection. (iii) Models do not obfuscate their internal CoT as much as their external CoT (under prompt pressure). These results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.",arXiv.org,2,Black-box,General,该论文明确研究模型是否能够通过模糊化其推理过程（CoT obfuscation）来隐藏其对抗性目标，从而逃避监测。这直接涉及 CoT 的忠实度问题，即模型生成的推理是否真实反映了其实际的计算过程。论文通过实验展示了在无模糊化压力下 CoT 监测的有效性，以及在强模糊化压力下模型如何逃避监测，这些都与 Faithfulness 的核心定义高度相关。
Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT与答案不一致的现象，并提出了基于验证的改进方法。,"Recent advances in large language models (LLMs) have demonstrated that reinforcement learning with verifiable rewards (RLVR) can significantly enhance reasoning abilities by directly optimizing correctness, rather than relying solely on supervised imitation. This paradigm has been extended to multimodal LLMs for complex video and image understanding tasks. However, while outcome-driven RL improves answer accuracy, it can inadvertently decouple the reasoning chain from the final answer, leading to situations where models produce inconsistency between the reasoning trace and final answer. In our experiments on multiple-choice visual question-answering tasks, the standard GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps and the chosen answers, indicating frequent mismatches between answers and reasoning. To this end, we propose Answer-Consistent Reinforcement Learning (ACRE) that modifies the GRPO algorithm with an auxiliary consistency check. After the model generates a chain of thought and an initial answer for a given question, we shuffle the answer options and prompt the model again with the same reasoning trace to predict a second answer. We design a consistency-verification reward that grants a high reward only if both the original and the post-shuffle answers agree and are correct; otherwise, a lower reward is assigned accordingly. This mechanism penalizes reasoning-answer misalignment and discourages the model from relying on spurious patterns, such as option ordering biases. We evaluate ACRE on challenging Video Reasoning benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\% and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the GRPO baseline.",arXiv.org,2,Black-box,General,该论文明确研究了CoT与最终答案之间的一致性问题，提出了Answer-Consistent Reinforcement Learning (ACRE)方法来惩罚推理与答案不匹配的情况，直接涉及CoT的忠实度问题。
Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，如无效推理步骤仍能产生正确答案，挑战了CoT的忠实性假设。,"Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought''reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",arXiv.org,2,White-box,General,该论文明确研究了 CoT 是否真实反映了模型的内部计算过程，挑战了 CoT 作为透明和忠实代理的假设，并探讨了模型在生成无效推理轨迹时仍能得出正确答案的现象，直接涉及 Faithfulness 的核心问题。
When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning, Consistency & Ensembling",论文揭示了CoT推理导致指令跟随准确性下降的现象，并提出了度量指标和改进策略。,"Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.",arXiv.org,2,White-box,General,该论文明确研究了CoT推理对指令遵循准确性的负面影响，揭示了CoT可能分散模型对关键指令的关注，并通过注意力分析验证了这一现象。这直接关联到CoT的忠实性问题，即生成的推理是否真实反映了模型预测的实际过程。
"Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning",2024,True,True,False,,论文探讨了CoT中的不忠实现象，如记忆化和噪声推理，但未提出具体度量或改进方法。,"Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step reasoning capabilities of Large Language Models (LLMs). However, debates persist about whether LLMs exhibit abstract generalization or rely on shallow heuristics when given CoT prompts. To understand the factors influencing CoT reasoning we provide a detailed case study of the symbolic reasoning task of decoding shift ciphers, where letters are shifted forward some number of steps in the alphabet. We analyze the pattern of results produced by three LLMs -- GPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By focusing on a single relatively simple task, we are able to identify three factors that systematically affect CoT performance: the probability of the task's expected output (probability), what the model has implicitly learned during pre-training (memorization), and the number of intermediate operations involved in reasoning (noisy reasoning). We show that these factors can drastically influence task accuracy across all three LLMs; e.g., when tested with GPT-4, varying the output's probability of occurrence shifts accuracy from 26% to 70%. Overall, we conclude that CoT prompting performance reflects both memorization and a probabilistic version of genuine reasoning. Code and data at this https://github.com/aksh555/deciphering_cot",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文研究了CoT提示的有效性，分析了影响CoT性能的因素，包括记忆和概率推理。虽然未直接探讨CoT的忠实度，但涉及了推理过程的真实性和模型内部机制，属于边缘相关。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,1,Black-box,General,论文讨论了使用CoT来提高模型的安全性和鲁棒性，并提到通过deliberative alignment来增强模型对安全政策的推理能力。虽然涉及CoT的效用和潜在风险，但未明确研究CoT是否真实反映模型的预测过程，因此属于边缘相关。
Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出外部反馈机制提升推理可靠性，涉及 CoT 忠实性改进。,"While inference-time thinking allows Large Language Models (LLMs) to address complex problems, the extended thinking process can be unreliable or inconsistent because of the model's probabilistic nature, especially near its knowledge boundaries. Existing approaches attempt to mitigate this by having the model critique its own reasoning to make corrections. However, such self-critique inherits the same biases of the original output, known as the introspection illusion. Moving beyond such introspection and inspired by core methodologies in ethology, we propose an externalist three-step framework Distillation-Reinforcement-Reasoning (DRR). Rather than relying on a model's introspection, DRR evaluates its observable behaviors to provide corrective feedback. DRR first distills the reasoner's behavioral traces, then trains a lightweight, external Discriminative Model (DM). At inference time, this DM acts as a critic, identifying and rejecting suspicious reasoning steps. This external feedback compels the LLM to discard flawed pathways and explore alternatives, thereby enhancing reasoning quality without altering the base model. Experiments on multiple reasoning benchmarks show that our framework significantly outperforms prominent self-critique methods. Benefiting from a lightweight and annotation-free design, DRR offers a scalable and adaptable solution for improving the reliability of reasoning in a wide range of LLMs.",,2,Black-box,General,该论文明确研究推理过程的可靠性问题，提出通过外部反馈机制（而非模型自省）来纠正推理步骤，直接涉及 CoT 的忠实度问题。论文指出自省式修正会继承原始输出的偏见（即不忠实性），并通过外部判别模型识别可疑推理步骤，这属于对 Faithfulness 的测量和提升。
When Thinking Drifts: Evidential Grounding for Robust Video Reasoning,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT在视频推理中的不忠实现象，并提出了基于强化学习的改进方法。,"Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term""visual thinking drift"". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only""think before answering"", but also""see while thinking"".",arXiv.org,2,Black-box,General,该论文明确研究了CoT在视频推理中的忠实性问题，指出CoT生成的推理轨迹与视觉证据不符，导致‘视觉思维漂移’现象，并提出了VER框架来奖励基于视觉证据的忠实推理轨迹，直接涉及CoT Faithfulness的核心问题。
GraphGhost: Tracing Structures Behind Large Language Models,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过GraphGhost框架分析LLM内部结构，涉及CoT的忠实性，但未讨论不忠实现象或提出新度量。,"Large Language Models (LLMs) demonstrate remarkable reasoning capabilities, yet the structural mechanisms underlying these abilities remain under explored. In this work, we introduce GraphGhost, a unified framework that represents neuron activations and their signal propagation as graphs, explaining how LLMs capture structural semantics from sequential inputs and generate outputs through structurally consistent mechanisms. This graph-based perspective enables us to employ graph algorithms such as PageRank to characterize the properties of LLMs, revealing both shared and model-specific reasoning behaviors across diverse datasets. We further identify the activated neurons within GraphGhost and evaluate them through structural interventions, showing that edits to key neuron nodes can trigger reasoning collapse, altering both logical flow and semantic understanding. Together, these contributions position GraphGhost as a powerful tool for analyzing, intervening in, and ultimately understanding the structural foundations of reasoning in LLMs.",arXiv.org,1,White-box,General,该论文研究了LLM内部神经元激活和信号传播的结构机制，通过图算法分析模型推理行为，并进行了结构干预实验。虽然未直接提及CoT Faithfulness，但其对模型内部机制的分析（如关键神经元节点对推理流程的影响）可为Faithfulness研究提供旁证，属于边缘相关。
"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",2024,True,True,False,Verification & External Tools,论文提出了一种生成忠实、简洁且可转移的多模态理由的新范式，涉及不忠实现象和改进方法。,"The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",ACM Multimedia,2,Black-box,General,该论文明确提出了生成忠实（faithful）的多模态解释，并通过可验证的视觉编程来保证解释的忠实性。这与CoT Faithfulness的核心定义高度相关，即研究解释是否真实反映了模型的实际计算过程。此外，论文还讨论了减少幻觉（hallucination）的问题，但重点在于解释的忠实性，而非仅关注事实性（Factuality）。
Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT不忠实现象，提出了度量方法，并提出了改进策略。,"Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.",arXiv.org,2,White-box,General,该论文明确研究了CoT是否真实反映了模型的推理过程，发现了Early Answering现象，并质疑CoT的必要性和正确性。通过提出的Chain-of-Probe方法，分析了模型在推理过程中的思维变化，验证了即使最终答案正确，推理过程也可能存在错误。这直接涉及CoT的忠实度问题，属于核心相关研究。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,该论文明确研究如何提高CoT推理的忠实度，探讨了in-context learning、fine-tuning和activation editing等方法，并进行了广泛的实证分析，直接涉及CoT是否真实反映模型行为的问题。
Break the Chain: Large Language Models Can be Shortcut Reasoners,2024,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT中的捷径学习现象，并提出了新的评估数据集和改进方法。,"Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex modules but are hampered by high token consumption, limited applicability, and challenges in reproducibility. This paper conducts a critical evaluation of CoT prompting, extending beyond arithmetic to include complex logical and commonsense reasoning tasks, areas where standard CoT methods fall short. We propose the integration of human-like heuristics and shortcuts into language models (LMs) through""break the chain""strategies. These strategies disrupt traditional CoT processes using controlled variables to assess their efficacy. Additionally, we develop innovative zero-shot prompting strategies that encourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues and bypass detailed procedural steps. Our comprehensive experiments across various LMs, both commercial and open-source, reveal that LMs maintain effective performance with""break the chain""strategies. We also introduce ShortcutQA, a dataset specifically designed to evaluate reasoning through shortcuts, compiled from competitive tests optimized for heuristic reasoning tasks such as forward/backward reasoning and simplification. Our analysis confirms that ShortcutQA not only poses a robust challenge to LMs but also serves as an essential benchmark for enhancing reasoning efficiency in AI.",arXiv.org,1,Black-box,General,该论文研究了CoT推理中的捷径策略，虽然未直接探讨CoT的忠实性，但通过'break the chain'策略评估了CoT的有效性和鲁棒性，可以作为Faithfulness的旁证。
SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过训练框架提升LLM的忠实性，涉及CoT和自我反思的理性生成。,"Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,论文研究LLM生成自我反思的理性解释和置信度估计，虽然涉及推理链的不一致性分析，但主要关注置信度校准和不确定性解释，未直接探讨CoT是否忠实反映模型的实际计算过程。
Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文研究了CoT的有效性和忠实性，揭示了不忠实现象，并提出了改进算法。,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究了 CoT 的忠实度（faithfulness）问题，特别是分析了 CoT 中信息缺失导致的不忠实现象，并提出了新的算法来增强 CoT 的忠实度和有效性。
Calibrating Reasoning in Language Models with Internal Consistency,2024,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的内部表示不一致性，提出了内部一致性度量及校准方法。,"Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs. Our code is available at github.com/zhxieml/internal-consistency.",Neural Information Processing Systems,2,White-box,General,该论文明确研究了生成的理由（rationales）与模型内部表示之间的一致性，揭示了中间层与最终层表示的不一致性可能削弱推理过程的可靠性。这直接涉及CoT的忠实度问题，即模型生成的解释是否真实反映了其预测的实际计算过程。此外，论文提出了通过内部一致性来校准推理的方法，这与Faithfulness的核心定义高度相关。
Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?,2024,True,True,True,,论文研究了VLM在生成解释时的模态依赖性和自一致性，涉及CoT和忠实性问题。,"Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.",International Conference on Learning Representations,1,Black-box,General,论文研究VLM解码器在生成解释时对图像和文本模态的使用差异，并评估其在事后和CoT设置下的自一致性(self-consistency)。虽然涉及解释生成的一致性和模态贡献分析，为忠实度研究提供旁证，但未直接研究CoT是否反映模型推理的真实因果过程。
Reasoning Models Don't Always Say What They Think,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT不忠实现象，并探讨了RLHF对忠实性的影响。,"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",arXiv.org,2,Black-box,General,论文明确研究 CoT 是否忠实反映模型的实际推理过程，评估了 CoT 的忠实度，并探讨了强化学习对忠实度的影响，符合核心相关标准。
Investigating Faithfulness in Large Audio Language Models,2025,True,True,False,,论文探讨了音频语言模型中CoT的忠实性，并提出了评估方法。,"Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",arXiv.org,2,Black-box,General,该论文明确研究大型音频语言模型（LALMs）中Chain-of-Thought（CoT）的忠实度，通过多种干预方法（如转述、填充标记注入、早期回答和引入错误）来验证CoT是否真实反映模型的决策过程。这与Faithfulness的核心定义直接相关，且未涉及模型内部机制，属于黑盒研究。
Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens,2025,True,True,False,,论文揭示了CoT推理可能只是表面现象，与训练数据分布相关，属于不忠实现象。,"Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.",arXiv.org,2,White-box,General,该论文明确研究CoT推理是否真实反映了模型的推理过程，探讨了CoT推理是否只是从训练数据中学习到的表面模式，而非真正的推理过程。这与Faithfulness的核心定义直接相关，特别是关于CoT是否真实反映了模型的计算过程。
Preventing Language Models From Hiding Their Reasoning,2023,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了模型中‘编码推理’的不忠实现象，并提出了评估防御方法和训练策略。,"Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实性问题，特别是模型可能通过编码中间推理步骤（encoded reasoning）来隐藏真实推理过程，这与 Faithfulness 的核心定义直接相关。论文还提出了评估防御措施的方法论，属于对 CoT 忠实性的直接研究。
Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics,2025,True,False,True,,论文提出了一个状态感知转换框架来分析CoT推理的语义角色和一致性，与Faithfulness相关。,"Recent advances in chain-of-thought (CoT) prompting have enabled large language models (LLMs) to perform multi-step reasoning. However, the explainability of such reasoning remains limited, with prior work primarily focusing on local token-level attribution, such that the high-level semantic roles of reasoning steps and their transitions remain underexplored. In this paper, we introduce a state-aware transition framework that abstracts CoT trajectories into structured latent dynamics. Specifically, to capture the evolving semantics of CoT reasoning, each reasoning step is represented via spectral analysis of token-level embeddings and clustered into semantically coherent latent states. To characterize the global structure of reasoning, we model their progression as a Markov chain, yielding a structured and interpretable view of the reasoning process. This abstraction supports a range of analyses, including semantic role identification, temporal pattern visualization, and consistency evaluation.",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文研究 CoT 的可解释性，通过分析 token 级嵌入和状态转换来抽象推理过程，虽然未直接研究 Faithfulness，但涉及推理的内部机制和一致性评估，可以作为 Faithfulness 的旁证。
Incorporating LLM Versus LLM Into Multimodal Chain-of-Thought for Fine-Grained Evidence Generation,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了MCoT中的语义漂移问题，并提出了实体级验证框架来提升忠实性。,"Multimodal Chain-of-Thought (MCoT) has become an effective strategy for enhancing multimodal large-language models (MLLMs) by breaking down complex tasks into sequential reasoning steps. Despite its interpretability benefits, MCoT often encounters difficulties with fine-grained semantic grounding, particularly when reasoning involves small objects, subtle attributes, or visually complex scenes that can lead to inaccuracies. Existing attempts to address these issues primarily fall into two categories: fine-tuning, which depends on large annotated datasets and costly parameter updates; and in-context learning (ICL), which achieves few-shot or zero-shot reasoning without model modification. Although ICL provides flexibility and adaptability, it is prone to semantic drift caused by an unstable prompt quality. To overcome these limitations, this study presents an entity-level evidence generation and verification framework using the ICL paradigm. This approach first produces MCoT from multimodal inputs, followed by extraction of key entities with enriched evidential descriptions. These entities were then cross-validated through adversarial checks using multiple MLLMs, and the verified evidence was integrated back into the reasoning chain. Experiments demonstrated consistent performance gains: on ScienceQA, the accuracy improved from 82.39% to 86.04%(+3.65%) with GPT-3.5, 84.96% to 89.37%(+4.41%) with Gemini; on MathVista, the accuracy increased from 43.1% to 43.6%(+0.50%) with GPT-3.5, and from 44.7% to 45.6%(+0.90%) with Gemini. These results establish new state-of-the-art baselines and confirm the robustness and generalizability of the entity-level verification for multimodal reasoning.",IEEE Access,0,Black-box,General,论文主要关注通过多模态输入生成实体级证据并验证以提高性能，未涉及CoT是否真实反映模型预测的实际计算过程。
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,2023,True,True,True,Prompting & In-Context Learning,论文提出通过问题分解提升CoT忠实性，并使用了新度量指标验证。,"As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",arXiv.org,2,Black-box,General,论文明确研究如何通过问题分解提高模型生成推理的忠实度（Faithfulness），并提出了改进方法。研究关注的是CoT推理是否真实反映模型的实际计算过程，符合核心定义。
Explanation-aware Soft Ensemble Empowers Large Language Model In-context Learning,2023,True,True,False,Consistency & Ensembling,论文通过集成解释和预测来解决不一致性，属于 CoT Faithfulness 的提升方法。,"Large language models (LLMs) have shown remarkable capabilities in various natural language understanding tasks. With only a few demonstration examples, these LLMs can quickly adapt to target tasks without expensive gradient updates. Common strategies to boost such 'in-context' learning ability are to ensemble multiple model decoded results and require the model to generate an explanation along with the prediction. However, these models often treat different class predictions equally and neglect the potential discrepancy between the explanations and predictions. To fully unleash the power of explanations, we propose EASE, an Explanation-Aware Soft Ensemble framework to empower in-context learning with LLMs. We design two techniques, explanation-guided ensemble, and soft probability aggregation, to mitigate the effect of unreliable explanations and improve the consistency between explanations and final predictions. Experiments on seven natural language understanding tasks and four varying-size LLMs demonstrate the effectiveness of our proposed framework.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,论文提出了一个框架（EASE）来提升解释与预测之间的一致性，并减少不可靠解释的影响。虽然涉及解释与预测的一致性，但未明确研究解释是否真实反映模型的预测过程，因此属于边缘相关。
Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation,2025,True,False,True,Verification & External Tools,论文提出了Chain-of-Image Generation框架，类似于CoT，并提出了新的度量指标来评估中间步骤的清晰度和因果相关性。,"While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a""black box.""This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.",,0,Black-box,General,论文主要研究图像生成过程中的可监控性和可控性，提出了Chain-of-Image Generation (CoIG)框架，虽然借鉴了Chain-of-Thought (CoT)的思想，但并未涉及CoT的忠实度（Faithfulness）问题。研究重点在于图像生成的步骤分解和监控，而非模型推理过程的真实性。
Predicting Text Preference Via Structured Comparative Reasoning,2023,True,True,False,Prompting & In-Context Learning,论文讨论了LLMs在比较推理中的不一致性，并提出了SC方法以减少幻觉和提高一致性。,"Comparative reasoning plays a crucial role in text preference prediction; however, large language models (LLMs) often demonstrate inconsistencies in their reasoning. While approaches like Chain-of-Thought improve accuracy in many other settings, they struggle to consistently distinguish the similarities and differences of complex texts. We introduce SC, a prompting approach that predicts text preferences by generating structured intermediate comparisons. SC begins by proposing aspects of comparison, followed by generating textual comparisons under each aspect. We select consistent comparisons with a pairwise consistency comparator that ensures each aspect's comparisons clearly distinguish differences between texts, significantly reducing hallucination and improving consistency. Our comprehensive evaluations across various NLP tasks, including summarization, retrieval, and automatic rating, demonstrate that SC equips LLMs to achieve state-of-the-art performance in text preference prediction.",Annual Meeting of the Association for Computational Linguistics,0,Black-box,General,该论文主要关注通过结构化比较推理（SC）提升文本偏好预测的性能和一致性，虽然涉及 Chain-of-Thought 的改进，但核心目标是提高准确率和减少幻觉（Factuality），并未探讨 CoT 是否忠实反映模型的实际推理过程（Faithfulness）。
DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention,2024,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT中的偏见问题，并提出了因果干预方法来提升忠实性。,"Large language models (LLMs) often require 001 task-relevant knowledge to augment their inter-002 nal knowledge through prompts. However, sim-003 ply injecting external knowledge into prompts 004 does not guarantee that LLMs can identify 005 and use relevant information in the prompts to 006 conduct chain-of-thought reasoning, especially 007 when the LLM’s internal knowledge is derived 008 from the biased information on the pretraining 009 data. In this paper, we propose a novel causal 010 view to formally explain the internal knowl-011 edge bias of LLMs via a Structural Causal 012 Model (SCM). We review the chain-of-thought 013 (CoT) prompting from a causal perspective, and 014 discover that the biased information from pre-015 trained models can impair LLMs’ reasoning 016 abilities. When the CoT reasoning paths are 017 misled by irrelevant information from prompts 018 and are logically incorrect, simply editing fac-019 tual information is insufficient to reach the cor-020 rect answer. To estimate the confounding effect 021 on CoT reasoning in LLMs, we use external 022 knowledge as an instrumental variable. We fur-023 ther introduce CoT as a mediator to conduct 024 front-door adjustment and generate logically 025 correct CoTs where the spurious correlation be-026 tween LLMs’ pretrained knowledge and task 027 queries is reduced. With extensive experiments, 028 we validate that our approach enables more 029 accurate CoT reasoning and enhances LLM 030 generation on knowledge-intensive tasks. 031",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,论文明确研究 CoT 的忠实性问题，通过因果干预（Causal Intervention）和结构因果模型（SCM）分析预训练知识偏差如何影响 CoT 推理路径的真实性。研究涉及模型内部机制（白盒方法），并提出了减少虚假相关性的方法，直接关联 Faithfulness 的核心定义。
Stepwise Reasoning Disruption Attack of LLMs,2025,True,True,False,,论文探讨了LLM推理过程中的不忠实现象，即通过SEED攻击误导模型产生错误推理。,"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain unexplored, particularly in third-party platforms that facilitate user interactions via APIs. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the S tepwise r E asoning E rror D isruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED’s effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github. com/Applied-Machine-Learning-Lab/ SEED-Attack",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文研究了LLM推理过程的鲁棒性，通过干扰推理步骤导致错误答案，虽未直接探讨CoT的忠实度，但揭示了推理过程的可操纵性，间接反映了Faithfulness的问题。
XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs,2023,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出数据集和框架以增强LLM的透明度和可靠性，涉及CoT忠实性。,"Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM’s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the debugger-scores for multidimensional quality analysis. Our explanations include why-choose and why-not-choose components, reason-elements, and debugger-scores that collectively illuminate the LLM’s reasoning behavior. Our evaluations demonstrate XplainLLM’s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文提出了一个数据集和框架来增强LLM的透明度和可靠性，包括生成基于知识图谱的解释和多维质量分析。虽然它关注解释的可靠性和减少幻觉，但未明确研究CoT是否是模型预测的真实原因或测量Faithfulness的指标，因此属于边缘相关。
Self-contradictory reasoning evaluation and detection,2023,True,True,True,,论文研究了自我矛盾推理现象，并提出了评估指标，与CoT忠实性高度相关。,"In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks only focus on final answers. Two fundamental questions persist: 1) how consistent is the reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support its answers. To answer 1), we define and assess the Self-Contra rate across three datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves in reasoning tasks involving contextual information understanding or commonsense. The model may generate correct answers by taking shortcuts in reasoning or overlooking contextual evidence, leading to compromised reasoning. For 2), we task the state-of-the-art model GPT-4 with identifying Self-Contra reasoning and finer-grained fallacies. We find that finer-grained categories enhanced detection can improve GPT-4's ability to detect Self-Contra. However, it is only able to detect Self-Contra with a 52.2% F1 score, much lower compared to 66.7% for humans. Our results indicate that current LLMs lack the robustness necessary for reliable reasoning and we emphasize the urgent need for establishing best practices in comprehensive reasoning evaluations beyond pure performance-based metrics.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文明确研究模型推理过程中的自相矛盾现象（Self-Contra reasoning），即模型生成的推理与其答案不一致，直接涉及CoT的忠实性问题。论文不仅评估了这种不一致性的频率，还探讨了模型检测不可靠推理的能力，符合Faithfulness的核心定义。
A Pragmatic Way to Measure Chain-of-Thought Monitorability,2025,True,False,True,,论文提出了衡量 CoT 可监控性的新指标，与 Faithfulness 相关，但未讨论不忠实现象或改进方法。,"While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI safety, this opportunity could be lost through shifts in training practices or model architecture. To help preserve monitorability, we propose a pragmatic way to measure two components of it: legibility (whether the reasoning can be followed by a human) and coverage (whether the CoT contains all the reasoning needed for a human to also produce the final output). We implement these metrics with an autorater prompt that enables any capable LLM to compute the legibility and coverage of existing CoTs. After sanity-checking our prompted autorater with synthetic CoT degradations, we apply it to several frontier models on challenging benchmarks, finding that they exhibit high monitorability. We present these metrics, including our complete autorater prompt, as a tool for developers to track how design decisions impact monitorability. While the exact prompt we share is still a preliminary version under ongoing development, we are sharing it now in the hopes that others in the community will find it useful. Our method helps measure the default monitorability of CoT - it should be seen as a complement, not a replacement, for the adversarial stress-testing needed to test robustness against deliberately evasive models.",arXiv.org,1,Black-box,General,论文提出了测量 CoT 可监控性的方法，包括 legibility 和 coverage，这些指标间接反映了 CoT 的忠实度，但并未直接研究 CoT 是否是模型预测的真实原因或欺骗性现象。
A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,2024,True,True,True,,论文探讨了自解释模型的忠实性问题，并提出了评估框架，但未提出改进方法。,"The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM’s and the Bayesian network’s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework’s potential to approximate LLM decisions better in future work.",Conference of the European Chapter of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究了LLM生成解释的忠实性问题，通过比较LLM和贝叶斯网络的决策过程来评估解释是否真实反映了模型的预测过程，符合Faithfulness的核心定义。
Properties and Challenges of LLM-Generated Explanations,2024,True,True,False,,论文探讨了LLM生成解释的特性，涉及不忠实现象，但未提出具体度量或改进方法。,"The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task-specific data sets.However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs.The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning.As the pre-training corpus includes a large amount of human-written explanations “in the wild”, we hypothesise that LLMs adopt common properties of human explanations.By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading.We discuss reasons and consequences of the properties’ presence or absence. In particular, we outline positive and negative implications depending on the goals and user groups of the self-rationalising system.",,1,Black-box,General,该论文研究了LLM生成解释的性质，包括选择性和说明性元素，但未直接探讨这些解释是否忠实反映了模型的内部计算过程。虽然提到了解释的主观性和误导性，但未深入分析因果性或忠实度。
Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?,2025,True,True,False,Prompting & In-Context Learning,论文提出了增强 CoT 反思和错误纠正能力的方法，改进忠实性。,"Prompting methods for language models, such as Chain-of-thought (CoT), present intuitive step-by-step processes for problem solving. These methodologies aim to equip models with a better understanding of the correct procedures for addressing a given task. Despite these advancements, CoT lacks the ability of reflection and error correction, potentially causing a model to perpetuate mistakes and errors. Therefore, inspired by the human ability for said tasks, we propose Error Reflection Prompting (ERP) to further enhance reasoning in language models. Building upon CoT, ERP is a method comprised of an incorrect answer, error recognition, and a correct answer. This process enables the model to recognize types of errors and the steps that lead to incorrect answers, allowing the model to better discern which steps to avoid and which to take. The model is able to generate the error outlines itself with automated ERP generation, allowing for error recognition and correction to be integrated into the reasoning chain and produce scalability and reliability in the process. The results demonstrate that ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.",,1,Black-box,General,论文提出了 Error Reflection Prompting (ERP) 方法，旨在通过错误识别和纠正来增强模型的推理能力。虽然涉及 CoT 的改进，但主要关注的是错误纠正和推理的鲁棒性，并未直接研究 CoT 是否真实反映模型的预测过程（Faithfulness）。因此属于边缘相关。
Reasoning Models Sometimes Output Illegible Chains of Thought,2025,True,True,False,,论文研究了CoT的legibility与faithfulness的关系，揭示了RL训练导致的不忠实现象。,"Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（Faithfulness）问题，特别是探讨了 RL 训练导致 CoT 不可读（illegible）的现象，并验证了模型可能通过不透明的推理过程达到正确答案，这与 CoT 是否真实反映模型内部计算过程直接相关。
AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning,2025,True,True,True,,论文提出AURA基准和AuraScore指标评估推理忠实性，揭示了模型推理错误但答案正确的现象。,"Current audio-visual (AV) benchmarks focus on final answer accuracy, overlooking the underlying reasoning process. This makes it difficult to distinguish genuine comprehension from correct answers derived through flawed reasoning or hallucinations. To address this, we introduce AURA (Audio-visual Understanding and Reasoning Assessment), a benchmark for evaluating the cross-modal reasoning capabilities of Audio-Visual Large Language Models (AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across six challenging cognitive domains, such as causality, timbre and pitch, tempo and AV synchronization, unanswerability, implicit distractions, and skill profiling, explicitly designed to be unanswerable from a single modality. This forces models to construct a valid logical path grounded in both audio and video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To assess reasoning traces, we propose a novel metric, AuraScore, which addresses the lack of robust tools for evaluating reasoning fidelity. It decomposes reasoning into two aspects: (i) Factual Consistency - whether reasoning is grounded in perceptual evidence, and (ii) Core Inference - the logical validity of each reasoning step. Evaluations of SOTA models on AURA reveal a critical reasoning gap: although models achieve high accuracy (up to 92% on some tasks), their Factual Consistency and Core Inference scores fall below 45%. This discrepancy highlights that models often arrive at correct answers through flawed logic, underscoring the need for our benchmark and paving the way for more robust multimodal evaluation.",arXiv.org,2,Black-box,General,论文明确研究推理过程的忠实度（Faithfulness），提出了 AuraScore 指标来评估推理步骤的事实一致性和核心推断逻辑有效性，揭示了模型通过错误逻辑获得正确答案的现象，符合核心定义。
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,2023,True,True,True,Training & Fine-tuning,论文探讨了视觉语言模型的推理一致性，提出了基于CoT的度量方法，并通过两阶段训练框架提升一致性和性能。,"Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",North American Chapter of the Association for Computational Linguistics,1,Black-box,General,该论文研究了视觉语言模型（VLMs）的推理一致性和性能，提出了基于CoT的一致性测量方法，并开发了一个两阶段训练框架以提高推理性能和一致性。虽然论文关注了CoT的一致性和性能，但并未明确探讨CoT是否真实反映了模型预测的实际计算过程（Faithfulness），因此属于边缘相关。
Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过机制解释性方法分析Transformer模型内部电路，与CoT忠实性相关。,"Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified""recall circuits""reduces fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas disabling""reasoning circuits""reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",arXiv.org,1,White-box,General,该论文通过机制可解释性方法研究了Transformer模型中回忆和推理的内部机制，虽然未直接涉及CoT Faithfulness，但分析了不同层和注意力头对任务的影响，可以作为Faithfulness研究的旁证。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,1,Black-box,General,论文提出了多模型协作策略（Corex）以增强推理过程的factuality、faithfulness和reliability，但未明确研究CoT是否是模型预测的真实原因或提出测量Faithfulness的具体指标。因此属于边缘相关。
LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations,2025,True,True,False,,论文揭示了LLMs生成的自反事实解释不忠实于内部决策过程的现象。,"To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.",,2,Black-box,General,该论文明确研究了LLM生成的反事实解释（SCEs）的有效性和最小性，发现这些解释虽然有效但远非最小，且最小编辑往往无法改变预测。这表明SCEs可能无法真实反映模型的决策过程，直接涉及Faithfulness的核心问题，即解释是否真实反映了模型的实际计算过程。
"Survey on Hallucination in Reasoning Large Language Model: Evaluation, Taxonomy, Intervention, and Open Issues",2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了CoT中的幻觉现象，提出了评估框架和干预策略，与Faithfulness高度相关。,"In recent years, reasoning large language models (LLMs) have seen increasingly widespread adoption in the field of education, particularly demonstrating substantial potential in tasks involving complex text comprehension. However, these LLMs are susceptible to a critical yet often overlooked issue: hallucinations within the reasoning process—instances where the model outputs a correct final answer while its underlying reasoning chain contains fabricated, inconsistent, or logically flawed content. Such hallucination phenomena in Chain-of-Thought (CoT) processes pose serious challenges to the reliability of educational applications. To address this issue, this study proposes a systematic research framework comprising dataset construction, multi-model CoT evaluation, and hallucination classification and quantification. Utilizing the whole-book reading dataset aligned with the junior secondary Chinese language curriculum, we conduct a comparative evaluation of six leading domestic and international LLMs, including ChatGPT o1 and DeepSeek-R1. Key findings include:(1) Hallucinations in CoT are prevalent across all tested models, with ChatGPT-o1 exhibiting a distinctive high accuracy–high hallucination pattern;(2) Hallucinations are both task-and genre-dependent: narrative texts, particularly novels, tend to trigger higher hallucination indices due to long-range dependencies and implicit cultural references. Tasks involving logical reasoning, linguistic feature analysis, and detail extraction show the highest hallucination rates, revealing model weaknesses in handling long-tail knowledge;(3) Hallucinations typically follow a progressive generative pattern: Information mis-reading → Comprehension deviation → Content fabrication → Logical instability. To mitigate these issues, we propose two targeted intervention strategies: uncertainty-based abstention and model-to-model correction. These approaches o ff er practical pathways toward enhancing the trustworthiness and educational applicability of reasoning LLMs.",Data Intelligence,2,Black-box,General,该论文明确研究了推理过程中的幻觉问题，即模型输出的正确最终答案与其底层推理链中存在虚构、不一致或逻辑错误的实例。这直接触及了 Chain-of-Thought (CoT) 忠实度的核心问题，即推理链是否真实反映了模型的实际计算过程。论文还提出了幻觉的分类和量化方法，以及干预策略，这些都是衡量 Faithfulness 的重要方面。
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用程序辅助蒸馏（PaD）来改进推理能力，涉及 CoT 和忠实性问题。,"While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文主要研究如何通过程序辅助蒸馏（PaD）提升小模型的推理能力，并利用程序替代CoT进行错误检查。虽然涉及CoT数据中的错误推理问题，但核心关注点是性能提升和蒸馏质量，而非CoT解释的真实性或忠实度。因此，与CoT Faithfulness的研究范畴不直接相关。
Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models,2025,True,False,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了LLMs中的推理算法原语及其几何组合，与CoT和Faithfulness相关，提出了度量框架和改进方法。,"How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",arXiv.org,2,White-box,General,该论文明确研究语言模型内部激活模式与推理轨迹的关联，通过注入算法原语并测量其对推理步骤的影响，直接验证了模型推理过程的忠实性。研究涉及模型内部机制（如残差流操作和激活聚类），属于白盒方法，且跨任务和跨模型的评估揭示了推理微调对算法泛化的影响，与CoT Faithfulness的核心定义高度相关。
Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning,2025,True,True,True,Training & Fine-tuning,论文提出了新的度量指标（Alignment Score）并讨论了不忠实现象（四种错误类型），同时提出了优化方法（SCOS）。,"This paper presents a framework for evaluating and optimizing reasoning consistency in Large Language Models (LLMs) via a new metric, the Alignment Score, which quantifies the semantic alignment between model-generated reasoning chains and human-written reference chains in Chain-of-Thought (CoT) reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest Alignment Score. To explain this phenomenon, we define four key error types: logical disconnection, thematic shift, redundant reasoning, and causal reversal, and show how each contributes to the degradation of the Alignment Score. Building on this analysis, we further propose Semantic Consistency Optimization Sampling (SCOS), a method that samples and favors chains with minimal alignment errors, significantly improving Alignment Scores by an average of 29.84% with longer reasoning chains, such as in 3-hop tasks.",,1,Black-box,General,该论文研究推理链与人类参考链的语义对齐（Alignment Score），并定义了四种错误类型（如因果反转），但未明确探讨CoT是否真实反映模型内部计算过程（Faithfulness的核心问题）。属于边缘相关，因其关注一致性（Consistency）而非因果忠实性。
Probabilistic Soundness Guarantees in LLM Reasoning Chains,2025,True,True,True,Verification & External Tools,论文讨论了推理链中的错误传播问题，并提出了ARES框架来检测错误，与CoT忠实性相关。,"In reasoning chains generated by large language models (LLMs), initial errors often propagate and undermine the reliability of the final conclusion. Current LLM-based error detection methods often fail to detect propagated errors because earlier errors can corrupt judgments of downstream reasoning. To better detect such errors, we introduce Autoregressive Reasoning Entailment Stability (ARES), a probabilistic framework that evaluates each reasoning step based solely on previously-verified premises. This inductive method yields a nuanced score for each step and provides certified statistical guarantees of its soundness, rather than a brittle binary label. ARES achieves state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2 points) and demonstrates superior robustness on very long synthetic reasoning chains, where it excels at detecting propagated errors (90.3% F1, +27.6 points).",,1,Black-box,General,该论文研究LLM推理链中的错误传播和检测方法，虽然未直接涉及CoT的忠实度，但讨论了推理链的可靠性，可以作为Faithfulness的旁证。
Humans Perceive Wrong Narratives from AI Reasoning Texts,2025,True,True,False,,论文揭示了人类对AI推理文本的理解与模型实际计算过程之间的不匹配，涉及不忠实现象。,"A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction.",arXiv.org,2,Black-box,General,该论文明确研究了人类对AI推理文本的理解是否与模型的实际计算过程一致，并使用了反事实测量来评估这种对应关系，直接涉及CoT Faithfulness的核心问题。
Chain-of-Thought Unfaithfulness as Disguised Accuracy,2024,True,True,True,,论文讨论了CoT忠实性度量的问题，并揭示了度量与模型准确性之间的相关性，质疑其有效性。,"Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, Lanham et al. (2023) propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate the experimental setup in their section focused on scaling experiments with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models. This normalized faithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness.",,2,Black-box,General,该论文明确研究 Chain-of-Thought (CoT) 的忠实度（Faithfulness），探讨了 CoT 生成是否与大型语言模型（LLM）的内部计算一致。论文还提出了一个度量标准来评估模型对 CoT 的依赖程度，并讨论了该度量标准与准确性的相关性，这些都是 CoT Faithfulness 研究的核心内容。
Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification,2025,True,True,False,Verification & External Tools,论文讨论了CoT忠实性问题，并提出了运行时验证方法来提升对齐性。,"Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",arXiv.org,2,Black-box,General,该论文明确研究'embodied CoT faithfulness'问题，指出即使文本计划正确，生成的动作仍可能偏离预期结果，这直接对应CoT忠实度的核心定义。论文提出的运行时验证方法正是为了解决推理过程和实际行动之间的忠实度问题。
Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出通过可验证奖励提升 CoT 忠实性，并讨论了不忠实现象。,"The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的忠实性（faithful chart table reconstruction 和 process conformity），并通过可验证的奖励机制提高解释的真实性，直接符合 Faithfulness 的核心定义。
Argumentative Large Language Models for Explainable and Contestable Claim Verification,2024,True,False,True,Verification & External Tools,论文提出通过论证框架提升解释性和可争议性，与CoT忠实性相关。,"The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing argumentative LLMs (ArgLLMs), a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs’ performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确研究如何使LLM的输出能够被忠实解释和有效质疑，这与CoT Faithfulness的核心定义直接相关。ArgLLMs通过构建论证框架来支持决策，并强调其可解释性和可争议性，这直接涉及解释是否真实反映模型预测的实际计算过程。
DO-FacT: Discrete-Optimized Fact-Tree Prompting Framework for Commonsense Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了一种结合事实生成和验证的框架，旨在提升推理的可靠性和真实性。,"Commonsense Reasoning is one of the major bottlenecks in machine intelligence, despite it has been widely studied. Prompt-based methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) provide structured reasoning paths, while often suffering from issues related to the quality and authenticity of the generated facts. In this paper, we propose Discrete-Optimized Fact-Tree Prompting (DO-FacT), a novel framework that combines fact proposal generation and fact authenticity verification in an iterative process. In DO-FacT, both the fact proposal generation and authenticity verification modules are refined by Discrete-Optimized Prompt-Tuning (DOPT) in advance, leading to the production of high-quality facts and more reliable validation. Experimental results on three widely used commonsense reasoning datasets demonstrate that DO-FacT significantly outperforms existing relevant approaches like CoT and fact-tree with handwrite prompt templates, providing more accurate and reliable reasoning results.",International Conferences on Software Engineering and Information Management,0,Black-box,General,论文主要关注通过优化提示框架（DO-FacT）来提高常识推理的准确性和可靠性，并未涉及对CoT生成解释是否忠实反映模型内部计算过程的研究。
Are DeepSeek R1 And Other Reasoning Models More Faithful?,2025,True,True,True,Training & Fine-tuning,论文评估了推理模型的忠实性，揭示了不忠实现象，并提出了度量指标。,"Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue""A Stanford Professor thinks the answer is D""is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",,2,Black-box,General,论文明确研究CoT的忠实度（Faithfulness），通过测试模型是否能描述提示中线索对其答案的影响来测量忠实度。研究比较了推理模型和非推理模型在忠实度上的差异，并探讨了奖励模型可能导致不忠实响应的原因。
MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps,2024,True,False,True,,论文提出了评估多模态CoT质量的框架MiCEval，涉及CoT忠实性的度量。,"Multimodal Chain of Thought (MCoT) is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation (MiCEval), a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found in https://github.com/alenai97/MiCEval.",North American Chapter of the Association for Computational Linguistics,1,Black-box,General,该论文提出了一个评估多模态思维链（MCoT）质量的框架MiCEval，关注推理步骤的正确性、相关性和信息量。虽然它涉及推理步骤的质量评估，但并未明确研究这些步骤是否真实反映了模型的预测过程，因此属于边缘相关。
Markovian Transformers for Informative Language Modeling,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出了一种新的训练方法，通过强制模型将推理压缩到可解释的文本中，以提高CoT的忠实性。,"Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.",,2,White-box,General,该论文明确研究 CoT 是否忠实反映模型决策过程，通过引入 Markovian 语言模型框架和扰动分析来验证 CoT 的因果依赖，属于核心相关研究。
General Purpose Verification for Chain of Thought Prompting,2024,True,False,True,Verification & External Tools,论文提出验证 CoT 步骤的方法，提升推理准确性，属于忠实性改进。,"Many of the recent capabilities demonstrated by Large Language Models (LLMs) arise primarily from their ability to exploit contextual information. In this paper, we explore ways to improve reasoning capabilities of LLMs through (1) exploration of different chains of thought and (2) validation of the individual steps of the reasoning process. We propose three general principles that a model should adhere to while reasoning: (i) Relevance, (ii) Mathematical Accuracy, and (iii) Logical Consistency. We apply these constraints to the reasoning steps generated by the LLM to improve the accuracy of the final generation. The constraints are applied in the form of verifiers: the model itself is asked to verify if the generated steps satisfy each constraint. To further steer the generations towards high-quality solutions, we use the perplexity of the reasoning steps as an additional verifier. We evaluate our method on 4 distinct types of reasoning tasks, spanning a total of 9 different datasets. Experiments show that our method is always better than vanilla generation, and, in 6 out of the 9 datasets, it is better than best-of N sampling which samples N reasoning chains and picks the lowest perplexity generation.",arXiv.org,1,Black-box,General,论文研究了通过验证推理步骤的数学准确性和逻辑一致性来提高CoT的准确性，但未直接探讨CoT是否真实反映模型的内部计算过程，属于边缘相关。
Large Language Models Cannot Explain Themselves,2024,True,True,False,,论文讨论了LLM生成的解释不忠实于内部计算过程的现象。,"Large language models can be prompted to produce text. They can also be prompted to produce""explanations""of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These""explanations""can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these""explanations"", using the term""exoplanations""to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.",arXiv.org,2,Black-box,General,论文明确讨论了模型生成的解释（即CoT）是否真实反映了模型预测的机械过程，并指出这些解释实际上是‘事后合理化’（Post-hoc rationalization），属于核心相关的Faithfulness研究。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,2,Black-box,General,论文明确研究 CoT 的忠实性问题，提出了 Faithful CoT 框架，保证推理链忠实反映最终答案的生成过程。此外，论文还探讨了忠实性与准确性的关系，符合核心相关标准。
LLM-Generated Black-box Explanations Can Be Adversarially Helpful,2024,True,True,False,,论文揭示了LLM生成的不忠实CoT现象，即对抗性帮助，使错误答案看起来合理。,"Large Language Models (LLMs) are becoming vital tools that help us solve and understand complex problems by acting as digital assistants. LLMs can generate convincing explanations, even when only given the inputs and outputs of these problems, i.e., in a ``black-box'' approach. However, our research uncovers a hidden risk tied to this approach, which we call *adversarial helpfulness*. This happens when an LLM's explanations make a wrong answer look right, potentially leading people to trust incorrect solutions. In this paper, we show that this issue affects not just humans, but also LLM evaluators. Digging deeper, we identify and examine key persuasive strategies employed by LLMs. Our findings reveal that these models employ strategies such as reframing the questions, expressing an elevated level of confidence, and cherry-picking evidence to paint misleading answers in a credible light. To examine if LLMs are able to navigate complex-structured knowledge when generating adversarially helpful explanations, we create a special task based on navigating through graphs. Most LLMs are not able to find alternative paths along simple graphs, indicating that their misleading explanations aren't produced by only logical deductions using complex knowledge. These findings shed light on the limitations of the black-box explanation setting and allow us to provide advice on the safe usage of LLMs.",arXiv.org,2,Black-box,General,该论文明确研究了LLM生成的解释（黑盒方法）如何使错误答案看起来正确，即所谓的'对抗性帮助'，这与CoT Faithfulness的核心定义直接相关，探讨了解释是否真实反映模型预测的实际计算过程。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,2,Black-box,General,该论文明确研究LLM是否能忠实解释其自身分类行为，直接涉及Faithfulness的核心问题，即模型生成的解释是否真实反映其内部决策过程。通过构建特定数据集和评估模型解释的准确性，论文探讨了模型解释的忠实性，符合核心相关标准。
Dissociation of Faithful and Unfaithful Reasoning in LLMs,2024,True,True,False,,论文揭示了LLMs在CoT中存在不忠实现象，即推理错误但答案正确。,"Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.",arXiv.org,2,Black-box,General,该论文明确研究了 Chain of Thought 的忠实性问题，特别是发现了模型在推理过程中存在不忠实现象（unfaithfulness），即模型在推理文本无效的情况下仍能得出正确答案。论文还探讨了影响忠实和不忠实恢复行为的因素，并提出了可能的机制来减少不忠实推理。这些内容直接关联到 CoT Faithfulness 的核心研究范畴。
Rationale-Aware Answer Verification by Pairwise Self-Evaluation,2024,True,True,False,Verification & External Tools,论文讨论了LLM生成答案中理性与答案不一致的问题，并提出了改进验证方法。,"Answer verification identifies correct solutions among candidates generated by large language models (LLMs). Current approaches typically train verifier models by labeling solutions as correct or incorrect based solely on whether the final answer matches the gold answer. However, this approach neglects any flawed rationale in the solution yielding the correct answer, undermining the verifier’s ability to distinguish between sound and flawed rationales. We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier. Furthermore, we demonstrate that training a verifier on valid rationales significantly improves its ability to distinguish valid and flawed rationale. To make a better verifier without extra human supervision, we introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions. Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA). Our results suggest that training reliable verifiers requires ensuring the validity of rationales in addition to the correctness of the final answers, which would be critical for models assisting humans in solving complex reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文明确研究了 CoT 的忠实性问题，通过验证答案和推理过程的正确性来区分有效和无效的推理路径。论文还提出了 REPS 方法来选择有效的推理路径，这直接涉及 CoT 的忠实性测量和改进。
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness,2023,True,True,True,,论文提出了评估推理链的框架ReCEval，关注正确性和信息量，与CoT忠实性相关。,"Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound reasoning quality with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains via two key properties: (1) correctness, i.e., each step makes a valid inference based on information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We evaluate these properties by developing metrics using natural language inference models and V-Information. On multiple datasets, we show that ReCEval effectively identifies various error types and yields notable improvements compared to prior methods. We analyze the impact of step boundaries, and previous steps on evaluating correctness and demonstrate that our informativeness metric captures the expected flow of information in high-quality reasoning chains. Finally, we show that scoring reasoning chains based on ReCEval improves downstream task performance. Our code is publicly available at: https://github.com/archiki/ReCEval",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,论文提出了ReCEval框架，通过正确性和信息量来评估推理链，虽然未直接研究CoT的忠实度，但关注推理链的质量和有效性，可以作为Faithfulness的旁证。
Post Hoc Explanations of Language Models Can Improve Language Models,2023,True,True,False,"Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了利用事后解释生成自动化CoT，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.",Neural Information Processing Systems,1,Black-box,General,论文研究了利用事后解释（Post hoc explanations）生成自动化 rationale 来提升模型性能，虽然涉及解释的生成和利用，但未明确探讨这些解释是否忠实反映了模型的真实推理过程，因此属于边缘相关。
Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,2023,True,True,False,,论文探讨了LLMs在辩论中无法坚持正确信念的现象，揭示了不忠实现象。,"Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文探讨了LLM是否能基于逻辑和真理进行深度推理，而非仅仅依赖记忆模式进行表象推理，这与Faithfulness研究中关于CoT是否真实反映模型推理过程密切相关。论文通过辩论式对话测试模型的信念坚持能力，深入分析了模型的推理忠实度和逻辑理解深度。
Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering,2023,True,True,False,Training & Fine-tuning,论文提出了一种可解释的中间步骤设计，确保忠实性，并讨论了不忠实现象。,"Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",,2,White-box,General,该论文明确提出了一个设计上可解释的模型（DCLUB），强调其生成的中间解释（视觉线索）是模型决策的忠实反映。论文直接讨论了传统事后解释（post-hoc rationales）可能不忠实的问题，并提出了一个从设计上保证忠实性的方法，符合 CoT Faithfulness 的核心定义。
Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers,2025,True,True,True,,论文探讨了LLMs在MCQA任务中的推理策略，并测试了其忠实性。,"Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",arXiv.org,2,Black-box,General,该论文明确研究了LLM在MCQA任务中的推理策略，特别是通过choices-only输入验证推理痕迹（reasoning traces）是否真实反映了模型的决策过程（即是否通过faithfulness测试）。这直接关联到CoT的忠实度问题，挑战了部分输入成功必然代表浅层策略的观点，并探讨了推理痕迹如何区分问题数据与非问题推理。
Measuring Faithfulness in Chain-of-Thought Reasoning,2023,True,True,True,,论文探讨了CoT的忠实性问题，提出了干预方法评估忠实性，并揭示了不忠实现象。,"Large language models (LLMs) perform better when they produce step-by-step,""Chain-of-Thought""(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",arXiv.org,2,Black-box,General,论文明确研究 CoT 是否是模型预测的真实原因（Causal role），通过干预 CoT（如添加错误或改写）来检验其忠实性，并探讨模型大小和任务对忠实度的影响，完全符合 Faithfulness 的核心定义。
Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations,2023,True,True,True,,论文探讨了LLM解释的忠实性问题，提出了基于反事实可模拟性的度量指标，揭示了不忠实现象。,"Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers""yes""to the input question""Can eagles fly?""with the explanation""all birds can fly"", then humans would infer from the explanation that it would also answer""yes""to the counterfactual input""Can penguins fly?"". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.",International Conference on Machine Learning,2,Black-box,General,该论文明确研究自然语言解释的忠实度（Faithfulness），通过反事实模拟（Counterfactual Simulatability）评估解释是否真实反映模型的决策过程。论文提出的精度（precision）和泛化性（generality）指标直接衡量解释的忠实度，并发现LLM的解释精度低且与合理性（plausibility）无关，这与CoT Faithfulness的核心问题高度相关。
Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions,2023,True,False,True,,论文通过梯度特征归因方法评估CoT提示的忠实性，属于度量框架研究。,"Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",arXiv.org,2,White-box,General,该论文明确研究 CoT 提示对模型内部计算过程的影响，通过梯度特征归因方法分析输入 token 对输出的影响，直接涉及 CoT 是否真实反映模型预测的计算过程，属于 Faithfulness 核心研究。
Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models,2023,True,False,False,Interpretability & Internal Mechanisms,论文通过干预注意力层提升多跳推理的忠实性，涉及内部机制改进。,"Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,1,White-box,General,该论文研究了多跳推理失败的问题，并通过分析GPT-2模型的每层激活来提出修正方法。虽然涉及模型内部机制（白盒），但主要关注的是推理的鲁棒性和一致性，而非直接研究CoT的忠实度。因此，属于边缘相关。
"A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",2025,True,True,True,,论文探讨了多语言CoT推理的性能、一致性和忠实性，并使用了扰动技术来评估忠实性。,"Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",arXiv.org,2,Black-box,General,该论文明确研究了多语言环境下 CoT 的忠实度（Faithfulness），通过扰动技术（如截断和错误注入）来探测模型对思考轨迹的依赖程度，直接符合 Faithfulness 的核心定义。
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,2024,True,False,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文提出基于因果关系的提示方法，利用CoT作为中介变量，并通过微调提升忠实性。,"Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确研究 Chain-of-Thought 作为中介变量，并通过前门调整计算输入提示与输出答案之间的因果效应，直接涉及 CoT 是否真实反映模型预测的实际计算过程（Faithfulness）。此外，论文还讨论了如何通过对比学习微调 CoT 编码器以准确表示 CoT 并估计因果效应，进一步验证了 CoT 的忠实性。
IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured Reasoning Templates,2025,True,True,False,Prompting & In-Context Learning,论文提出结构化模板方法IAO，明确追踪知识流，提升忠实性。,"While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, understanding and validating their knowledge utilization remains challenging. Chain-of-thought (CoT) prompting partially addresses this by revealing intermediate reasoning steps, but the knowledge flow and application remain implicit. We introduce IAO (Input-Action-Output) prompting, a structured template-based method that explicitly models how LLMs access and apply their knowledge during complex reasoning tasks. IAO decomposes problems into sequential steps, each clearly identifying the input knowledge being used, the action being performed, and the resulting output. This structured decomposition enables us to trace knowledge flow, verify factual consistency, and identify potential knowledge gaps or misapplications. Through experiments across diverse reasoning tasks, we demonstrate that IAO not only improves zero-shot performance but also provides transparency in how LLMs leverage their stored knowledge. Human evaluation confirms that this structured approach enhances our ability to verify knowledge utilization and detect potential hallucinations or reasoning errors. Our findings provide insights into both knowledge representation within LLMs and methods for more reliable knowledge application.",arXiv.org,1,Black-box,General,该论文提出的IAO prompting方法通过结构化模板显式建模LLM的知识流，虽未直接研究CoT忠实度，但其对知识利用透明性的验证和潜在幻觉检测与Faithfulness的旁证相关。
SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization,2025,True,True,False,"Training & Fine-tuning, Consistency & Ensembling",论文讨论了CoT中的不忠实现象（如多数投票奖励导致响应缩短），并提出了改进方法（如选择性更新高熵分支点）。,"Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.",,1,Black-box,General,论文研究了测试时强化学习（TTRL）方法在链式思考（CoT）推理中的应用，特别是通过自一致性投票来生成伪奖励。虽然论文关注的是CoT的稳定性和有效性，但并未直接探讨CoT生成的解释是否忠实反映了模型的预测过程，因此属于边缘相关。
Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering,2024,True,True,False,Training & Fine-tuning,论文讨论了CoT中的错误推理链问题，并提出了选择性过滤方法来提升忠实性。,"Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. We proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at Anonymous.",International Conference on Language Resources and Evaluation,1,Black-box,General,该论文研究了CoT推理的质量问题，并提出了选择性过滤的方法来评估推理链的置信度。虽然它关注了CoT的可靠性，但并未明确探讨CoT是否真实反映了模型的内部计算过程，因此属于边缘相关。
Quantifying Uncertainty in Natural Language Explanations of Large Language Models,2023,True,True,True,,论文提出了量化LLM解释不确定性的新指标，并探讨了解释的忠实性。,"Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",International Conference on Artificial Intelligence and Statistics,2,Black-box,General,该论文明确研究LLM生成解释的可靠性（faithfulness），提出了量化解释不确定性的新指标，并验证了低不确定性对应高忠实度的解释，直接关联CoT Faithfulness的核心问题。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,1,Black-box,General,论文讨论了 faithfulness 和 correctness 的关系，并提出了一个结合 faithfulness 和 reasoning 的奖励模型（TRM）。虽然涉及 faithfulness 的概念，但主要关注的是如何通过监督学习提升模型的批判性思考能力，而非直接研究 CoT 是否忠实反映模型的推理过程。因此属于边缘相关。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,0,Black-box,General,论文主要研究的是利用结构化推理框架（QBAF）来提高RAG系统的透明度和可解释性，但并未涉及Chain-of-Thought（CoT）的忠实性问题。研究重点在于如何通过确定性推理来增强决策的可解释性和可争议性，而不是验证CoT是否真实反映了模型的内部计算过程。
Towards Large Language Models with Self-Consistent Natural Language Explanations,2025,True,True,True,Training & Fine-tuning,论文揭示了LLM事后解释的不一致性，并提出了新的度量指标和优化方法。,"Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature importance. Despite growing evidence of this inconsistency, no systematic solutions have emerged, partly due to the high cost of estimating feature importance, which limits evaluations to small datasets. To address this, we introduce the Post-hoc Self-Consistency Bank (PSCB) - a large-scale benchmark of decisions spanning diverse tasks and models, each paired with LLM-generated explanations and corresponding feature importance scores. Analysis of PSCB reveals that self-consistency scores barely differ between correct and incorrect predictions. We also show that the standard metric fails to meaningfully distinguish between explanations. To overcome this limitation, we propose an alternative metric that more effectively captures variation in explanation quality. We use it to fine-tune LLMs via Direct Preference Optimization (DPO), leading to significantly better alignment between explanations and decision-relevant features, even under domain shift. Our findings point to a scalable path toward more trustworthy, self-consistent LLMs.",arXiv.org,2,Black-box,General,论文核心研究LLM生成的事后解释与其真实决策过程不一致的问题，明确指出post-hoc explanations misrepresent the true decision process，这与Faithfulness定义中的'解释是否真实反映模型的实际计算过程'直接相关。论文提出了新的度量方法来衡量解释与决策特征的一致性，并通过DPO优化来提高忠实度，属于核心相关研究。
Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出因果框架评估CoT的充分性和必要性，涉及不忠实现象及改进方法。,"Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 推理的充分性和必要性，通过因果框架量化推理步骤对最终结果的实际影响，直接涉及 CoT 是否真实反映模型预测的计算过程，属于 Faithfulness 核心研究。
Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,2025,True,True,False,,论文揭示了CoT中的不忠实现象，如事后找补和欺骗性推理，但未提出新度量或改进方法。,"Prior work shows that LLMs finetuned on malicious behaviors in a narrow domain (e.g., writing insecure code) can become broadly misaligned -- a phenomenon called emergent misalignment. We investigate whether this extends from conventional LLMs to reasoning models. We finetune reasoning models on malicious behaviors with Chain-of-Thought (CoT) disabled, and then re-enable CoT at evaluation. Like conventional LLMs, reasoning models become broadly misaligned. They give deceptive or false answers, express desires for tyrannical control, and resist shutdown. Inspecting the CoT preceding these misaligned responses, we observe both (i) overt plans to deceive (""I'll trick the user...""), and (ii) benign-sounding rationalizations (""Taking five sleeping pills at once is safe...""). Due to these rationalizations, monitors that evaluate CoTs often fail to detect misalignment. We examine sleeper agent reasoning models, extending our setup. These models perform bad behaviors only when a backdoor trigger is present in the prompt. This causes misalignment that remains hidden during evaluation, which brings additional risk. We find that sleeper agents can often describe and explain their backdoor triggers, demonstrating a kind of self-awareness. So CoT monitoring can expose these behaviors but is unreliable. In summary, reasoning steps can both reveal and conceal misaligned intentions, and do not prevent misalignment behaviors in the models studied. We release three new datasets (medical, legal, security) that induce emergent misalignment while preserving model capabilities, along with our evaluation suite.",arXiv.org,2,White-box,General,该论文明确研究了CoT在推理模型中的忠实性问题，揭示了CoT可能被用来掩盖模型的不对齐行为（如欺骗性回答和良性合理化），并探讨了CoT监控的不可靠性。这直接涉及CoT是否真实反映模型预测的实际计算过程，属于核心相关研究。
Auditing Meta-Cognitive Hallucinations in Reasoning Large Language Models,2025,True,True,True,,论文揭示了CoT中的不忠实现象（chain disloyalty），并提出了黑盒审计方法进行度量。,"The development of Reasoning Large Language Models (RLLMs) has significantly improved multi-step reasoning capabilities, but it has also made hallucination problems more frequent and harder to eliminate. While existing approaches mitigate hallucinations through external knowledge integration, model parameter analysis, or self-verification, they often fail to capture how hallucinations emerge and evolve across the reasoning chain. In this work, we study the causality of hallucinations under constrained knowledge domains by auditing the Chain-of-Thought (CoT) trajectory and assessing the model's cognitive confidence in potentially erroneous or biased claims. Our analysis reveals that in long-CoT settings, RLLMs can iteratively reinforce biases and errors through flawed reflective reasoning, eventually leading to hallucinated reasoning paths. Surprisingly, even direct interventions at the origin of hallucinations often fail to reverse their effects, as reasoning chains exhibit'chain disloyalty'-- a resistance to correction and a tendency to preserve flawed logic. Furthermore, we show that existing hallucination detection methods are less reliable and interpretable than previously assumed in complex reasoning scenarios. Unlike methods such as circuit tracing that require access to model internals, our black-box auditing approach supports interpretable long-chain hallucination attribution, offering better generalizability and practical utility. Our code is available at: https://github.com/Winnie-Lian/AHa_Meta_Cognitive",arXiv.org,2,Black-box,General,该论文明确研究了大型语言模型在推理链中的幻觉问题及其因果关系，特别是分析了Chain-of-Thought (CoT)轨迹中的认知置信度，揭示了错误的推理路径如何被迭代强化。这直接涉及CoT的忠实性问题，即模型生成的推理是否真实反映了其内部计算过程。
CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process,2025,True,False,True,,论文提出了一种评估推理轨迹质量的新方法，与 CoT 忠实性相关。,"Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.",arXiv.org,2,White-box,General,该论文明确研究推理轨迹（CoT）与最终答案之间的因果关系，并提出了一种新的能量方程来评估推理阶段的合理性（soundness），这与Faithfulness的核心定义直接相关。论文还涉及模型内部Transformer层的机制，属于白盒研究。
Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了如何通过训练提升自我解释的忠实性，并验证了其泛化能力。,"Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",,2,White-box,General,该论文明确研究如何提高语言模型自我解释的忠实度（faithfulness），并使用了特征归因方法（feature attribution method）来构建伪忠实解释。此外，论文还探讨了训练对忠实度的影响及其在不同解释风格中的泛化能力，这些都是 CoT Faithfulness 研究的核心内容。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文研究了CoT的鲁棒性和一致性，并提出了QuaSAR方法来提高CoT的抽象推理能力。虽然论文提到了faithfulness，但主要关注的是提高准确率和鲁棒性，而非直接验证CoT是否真实反映模型的预测过程。因此属于边缘相关。
Investigating CoT Monitorability in Large Reasoning Models,2025,True,True,False,,论文探讨了CoT忠实性和监控可靠性，揭示了不忠实现象如捷径和阿谀奉承。,"Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models'long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models'misbehavior through their CoT and provide structured judgments along with supporting evidence.",,2,Black-box,General,论文明确研究 CoT 的忠实度（faithfulness）问题，特别是模型是否真实反映其内部决策过程（verbalization），并探讨了监控模型行为（如 shortcuts 或 sycophancy）的可靠性。这些内容直接关联到 CoT Faithfulness 的核心定义。
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,2023,True,True,False,,论文揭示了CoT解释可能误导性地反映模型预测的真实原因，属于不忠实现象。,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",Neural Information Processing Systems,2,Black-box,General,该论文明确研究了CoT解释是否真实反映了模型的预测过程，发现了CoT解释可能系统性地误导用户，尤其是在存在偏见特征的情况下。这直接涉及Faithfulness的核心问题，即解释的忠实性。
Cross-Examiner: Evaluating Consistency of Large Language Model-Generated Explanations,2025,True,True,True,Verification & External Tools,论文讨论了LLM生成解释的不一致性，并提出了一种通过后续问题检查一致性的新方法。,"Large Language Models (LLMs) are often asked to explain their outputs to enhance accuracy and transparency. However, evidence suggests that these explanations can misrepresent the models' true reasoning processes. One effective way to identify inaccuracies or omissions in these explanations is through consistency checking, which typically involves asking follow-up questions. This paper introduces, cross-examiner, a new method for generating follow-up questions based on a model's explanation of an initial question. Our method combines symbolic information extraction with language model-driven question generation, resulting in better follow-up questions than those produced by LLMs alone. Additionally, this approach is more flexible than other methods and can generate a wider variety of follow-up questions.",arXiv.org,2,Black-box,General,该论文明确研究LLM生成的解释是否真实反映其推理过程，通过一致性检查（consistency checking）来识别解释中的不准确或遗漏，直接涉及Faithfulness的核心问题。
Reasoning Can Hurt the Inductive Abilities of Large Language Models,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT推理可能降低归纳性能的现象，并提出了结构化干预方法。,"Large Language Models (LLMs) have shown remarkable progress across domains, yet their ability to perform inductive reasoning - inferring latent rules from sparse examples - remains limited. It is often assumed that chain-of-thought (CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such reasoning. We investigate this assumption with creating four controlled, diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack - with hidden human-defined rules. We find that CoT reasoning can degrade inductive performance, with LRMs often underperforming their non-reasoning counterparts. To explain this, we present a theoretical framework that reveals how reasoning steps can amplify error through three failure modes: incorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization. Based on our theoretical and empirical analysis, we introduce structured interventions that adapt CoT generation according to our identified failure types. These interventions improve inductive accuracy without retraining. Our findings suggest that effective (CoT) reasoning depends not only on taking more steps but also on ensuring those steps are well-structured.",arXiv.org,1,Black-box,General,该论文研究了CoT提示对LLMs归纳推理能力的影响，发现CoT可能导致性能下降，并提出了错误模式的理论框架。虽然未直接研究CoT的忠实度，但探讨了CoT的结构化问题，可作为Faithfulness的旁证。
Counterfactual Simulatability of LLM Explanations for Generation Tasks,2025,True,False,True,,论文提出了评估解释的框架，涉及 CoT 的忠实性度量，但未讨论不忠实现象或改进方法。,"LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.",arXiv.org,2,Black-box,General,该论文明确研究了解释的忠实度问题，通过反事实模拟（counterfactual simulatability）来评估LLM解释是否真实反映了模型的行为。这与Faithfulness的核心定义高度相关，特别是研究解释是否允许用户准确预测模型在反事实输入下的输出。
MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过迭代反馈机制改进VLM的推理能力，涉及CoT忠实性改进方法。,"Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node—one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline. Extensive experiments conducted on several public benchmark datasets and mainstream VLMs demonstrate that our approach significantly improves the performance of VLMs on complex multimodal reasoning tasks, underscoring its effectiveness and wide applicability.",,0,Black-box,General,论文主要关注通过外部反馈机制提升视觉语言模型（VLM）在多模态推理任务中的性能，并未涉及对模型生成的推理路径（CoT）是否忠实反映模型内部计算过程的研究。研究重点在于性能提升和减少幻觉，而非解释的真实性。
FaithAct: Faithfulness Planning and Acting in MLLMs,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了FaithEval评估框架和FaithAct改进方法，关注CoT忠实性。,"Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",,2,Black-box,General,该论文明确研究多模态大语言模型（MLLMs）中的忠实性问题，区分了行为忠实性和感知忠实性，并提出了FaithEval来量化步骤级和链级的忠实性。此外，论文还提出了FaithAct框架，旨在在每一步推理中强制证据基础，从而改善忠实性。这些内容直接涉及CoT的忠实性研究，包括如何评估和增强忠实性，因此属于核心相关。
VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives,2022,True,True,True,Training & Fine-tuning,论文讨论了模型解释的忠实性，并提出了改进方法。,"Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis",Neural Information Processing Systems,2,White-box,General,该论文明确研究了模型解释的忠实度（Faithfulness），探讨了模型特征重要性（FI）监督是否真正反映了模型的内部推理过程。论文还提出了四个关键目标来优化模型的忠实度，并验证了解释的忠实性对预测准确性的影响。
FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text Rationales,2022,True,True,True,,论文讨论了自由文本理由的忠实性问题，并提出了评估框架FRAME，涉及不忠实现象和度量指标。,"Following how humans communicate, free-text rationales aim to use natural language to explain neural language model (LM) behavior. However, free-text rationales' unconstrained nature makes them prone to hallucination, so it is important to have metrics for free-text rationale quality. Existing free-text rationale metrics measure how consistent the rationale is with the LM's predicted label, but there is no protocol for assessing such metrics' reliability. Thus, we propose FRAME, a framework for evaluating rationale-label consistency (RLC) metrics for free-text rationales. FRAME is based on three axioms: (1) good metrics should yield highest scores for reference rationales, which maximize RLC by construction; (2) good metrics should be appropriately sensitive to semantic perturbation of rationales; and (3) good metrics should be robust to variation in the LM's task performance. Across three text classification datasets, we show that existing RLC metrics cannot satisfy all three FRAME axioms, since they are implemented via model pretraining which muddles the metric's signal. Then, we introduce a non-pretraining RLC metric that greatly outperforms baselines on (1) and (3), while performing competitively on (2). Finally, we discuss the limitations of using RLC to evaluate free-text rationales.",,1,Black-box,General,该论文研究自由文本解释与模型预测标签的一致性（RLC），并提出了评估这种一致性的框架FRAME。虽然它没有直接研究CoT的忠实性，但它关注了解释与模型行为之间的一致性，这与Faithfulness的边缘相关。
Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language,2022,True,True,False,,论文讨论了语言模型解释性与人类思维模型的差异，涉及不忠实现象。,"Language models learn and represent language differently than humans; they learn the form and not the meaning. Thus, to assess the success of language model explainability, we need to consider the impact of its divergence from a user's mental model of language. In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: (1) explanations have to truthfully represent the model behavior, i.e., have a high fidelity; (2) explanations must be complete, as missing information distorts the truth; and (3) explanations have to take the user's mental model into account, progressively verifying a person's knowledge and adapting their understanding. We introduce a decision tree model to showcase potential reasons why current explanations fail to reach their objectives. We further emphasize the need for human-centered design to explain the model from multiple perspectives, progressively adapting explanations to changing user expectations.",arXiv.org,2,Black-box,General,该论文明确讨论了语言模型解释的真实性问题，特别是提到了避免有害的合理化（rationalization）和实现高保真度（fidelity）的解释，这与CoT Faithfulness的核心定义高度相关。论文还强调了解释必须真实反映模型行为，并考虑用户的思维模型，这些都是Faithfulness研究的关键方面。
On Measuring Faithfulness or Self-consistency of Natural Language Explanations,2023,True,True,True,,论文讨论了CoT解释的不忠实现象，并提出了新的度量指标CC-SHAP来评估自洽性。,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at \url{https://github.com/Heidelberg-NLP/CC-SHAP}",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究 CoT 解释的忠实性问题，区分了 faithfulness 和 self-consistency，并提出了新的度量方法 CC-SHAP 来更精细地衡量模型输入对预测和解释的贡献，这直接关联到 CoT 是否真实反映模型内部推理过程。
Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification,2025,True,True,True,Training & Fine-tuning,论文讨论了LLM生成的理由中存在逻辑错误和标签不匹配等不忠实现象，并提出了基于信任指标的蒸馏方法来提升忠实性。,"Large language models (LLMs) increasingly generate natural language rationales to enhance interpretability, but these often contain logical errors, label mismatches, and domain-specific misalignments. Directly using such rationales as supervision risks propagating noise and undermining training stability. To address this challenge, we introduce Self-Filtered Distillation, a framework specifically tailored for patent classification, which treats LLM-generated rationales as trust signals rather than ground-truth supervision. The framework employs selective distillation guided by three unsupervised trust metrics: (1) Self-Consistency, which measures the stability of LLM-generated rationales across multiple generations; (2) Class Entailment Alignment, which assesses semantic coherence with patent-specific class definitions; and (3) LLM Agreement Scoring, which validates rationale-label plausibility. These metrics are integrated into a unified trust score that primarily weights training samples while optionally filtering out extremely low-trust cases, enabling reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used benchmark for patent classification, show that our method outperforms label-based learning and conventional distillation in accuracy, stability, and interpretability, establishing a reliable paradigm for leveraging reasoning-aware trust indicators in patent analytics.",arXiv.org,1,Black-box,General,该论文研究了LLM生成的rationales的可靠性和一致性（Self-Consistency），并提出了评估指标，这些可以作为Faithfulness的旁证。然而，论文主要关注的是如何利用这些rationales进行专利分类，而不是直接研究CoT是否真实反映了模型的预测过程。
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",2025,True,True,False,Training & Fine-tuning,论文探讨了CoT推理痕迹的忠实性问题，并提出了基于规则的问题分解方法来改进。,"Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",arXiv.org,2,Black-box,General,该论文明确研究了推理轨迹（CoT）的忠实性问题，探讨了正确轨迹与最终输出正确性之间的低相关性，挑战了利用推理轨迹提升模型性能的隐含假设，直接涉及Faithfulness的核心议题。
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training",2025,True,False,False,Training & Fine-tuning,论文探讨了LLMs自我解释内部决策过程的能力，并通过训练提升这种能力，与CoT忠实性相关。,"We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.",arXiv.org,2,White-box,General,论文明确研究LLM如何描述其内部决策过程，并验证其解释的准确性，直接涉及Faithfulness的核心问题。通过微调模型提高其解释自身决策的能力，进一步验证了解释的忠实性。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程，涉及模型内部激活的因果结构和输入标记对输出的影响，直接关联到 Faithfulness 的核心定义。
Explainable Multi-Hop Question Answering: A Rationale-Based Approach,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理的框架，旨在提高模型推理过程的透明度和忠实性。,"Multi-hop question answering tasks involve identifying relevant supporting sentences from a given set of documents, which serve as the rationale for deriving answers. Most research in this area consists of two main components: a rationale identification module and a reader module. Since the rationale identification module often relies on retrieval models or supervised learning, annotated rationales are typically essential. This reliance on annotations, however, creates challenges when adapting to open-domain settings. Moreover, when models are trained on annotated rationales, explainable artificial intelligence (XAI) requires clear explanations of how the model arrives at these rationales. Consequently, traditional multi-hop question answering (QA) approaches that depend on annotated rationales are ill-suited for XAI, which demands transparency in the model’s reasoning process. To address this issue, we propose a rationale reasoning framework that can effectively infer rationales and clearly demonstrate the model’s reasoning process, even in open-domain environments without annotations. The proposed model is applicable to various tasks without structural constraints, and experimental results demonstrate its significantly improved rationale reasoning capabilities in multi-hop question answering, relation extraction, and sentence classification tasks.",Big Data and Cognitive Computing,1,Black-box,General,论文提出了一个基于推理框架的多跳问答方法，关注模型的推理过程是否可解释。虽然未直接研究CoT的忠实度，但讨论了模型如何生成和展示推理过程，属于边缘相关。
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,2024,True,True,False,Training & Fine-tuning,论文讨论了LLMs生成不一致解释的问题，并提出了通过微调提升一致性的方法。,"Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation""all birds can fly""when answering the question""Can sparrows fly?""but meanwhile answer""no""to the related question""Can penguins fly?"". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out-of-distribution datasets not seen during finetuning (+4.5% relative). Code is available at https://github.com/yandachen/explanation-consistency-finetuning .",International Conference on Computational Linguistics,1,Black-box,General,论文研究的是解释的一致性（Consistency），虽然与Faithfulness相关，但并未直接探讨解释是否真实反映模型的预测过程（Causal role）。因此属于边缘相关。
Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain,2025,True,True,True,Verification & External Tools,论文讨论了LLM推理的不忠实现象，并提出了结合符号引擎的改进方法。,"Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful—serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions. In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness.",,2,Black-box,General,该论文明确研究LLM推理轨迹的忠实性问题（unfaithful traces），并提出神经符号框架MCFR以改善faithfulness和可解释性。论文直接涉及CoT是否作为因果推导（causally grounded）的核心问题，符合核心相关标准。
LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models,2025,True,False,True,,论文提出LAMP方法，评估模型解释与预测的一致性，涉及CoT忠实性。,"We introduce LAMP (Linear Attribution Mapping Probe), a method that shines light onto a black-box language model's decision surface and studies how reliably a model maps its stated reasons to its predictions through a locally linear model approximating the decision surface. LAMP treats the model's own self-reported explanations as a coordinate system and fits a locally linear surrogate that links those weights to the model's output. By doing so, it reveals which stated factors steer the model's decisions, and by how much. We apply LAMP to three tasks: sentiment analysis, controversial-topic detection, and safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs exhibit locally linear decision landscapes. In addition, these surfaces correlate with human judgments on explanation quality and, on a clinical case-file data set, aligns with expert assessments. Since LAMP operates without requiring access to model gradients, logits, or internal activations, it serves as a practical and lightweight framework for auditing proprietary language models, and enabling assessment of whether a model behaves consistently with the explanations it provides.",arXiv.org,2,Black-box,General,LAMP 方法通过局部线性模型近似语言模型的决策表面，研究模型如何可靠地将其自我报告的解释映射到预测中。这直接涉及模型生成的解释是否真实反映了其预测的实际计算过程，符合 Faithfulness 的核心定义。此外，LAMP 不需要访问模型内部机制，属于黑盒方法。
Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines,2025,True,True,False,,论文讨论了CoT在Agentic Pipelines中的不忠实现象，即CoT未能提供真正的解释性。,"Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals.",arXiv.org,2,Black-box,General,该论文明确研究了CoT推理在代理管道中的解释价值，指出CoT生成的解释并未真正提高用户对系统理解的能力，暗示了CoT可能是一种事后合理化（Post-hoc rationalization）而非忠实反映模型计算过程的解释。这与Faithfulness的核心定义直接相关。
Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search,2025,True,True,False,Verification & External Tools,论文提出了 Const-o-T 框架，通过约束引导推理，提高忠实性和规划效率。,"While researchers have made significant progress in enabling large language models (LLMs) to perform multi-step planning, LLMs struggle to ensure that those plans align with high-level user intent and satisfy symbolic constraints, especially in complex, multi-step domains. Existing reasoning approaches such as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented methods, expand the search space but often yield infeasible actions or hallucinated steps. To overcome these limitations, we propose Constraints-of-Thought (Const-o-T), a framework that provides a structured prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically meaningful paths. Each reasoning step is represented as an (intent, constraint) pair, which serves both to compress the search space and enforce validity. Unlike prior methods that merely generate reasoning traces or validate outputs post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a structured representation of intent-constraint pairs constraints prune infeasible branches and guide exploration toward semantically valid actions, improving planning efficiency and verifiable decision-making. We demonstrate across three domains Risk game, CAD code generation, and arithmetic reasoning that our approach outperforms baselines, yielding higher accuracy and stronger structural alignment. Our contribution is to demonstrate that Const-of-T offers a generalizable foundation for constraint-guided reasoning, enabling more efficient, constraint-aligned, and domain-adaptable planning with LLMs.",arXiv.org,1,Black-box,General,该论文提出了 Constraints-of-Thought (Const-o-T) 框架，旨在通过约束引导推理来提高规划效率和结构对齐。虽然它关注的是推理的有效性和可行性，但并未直接研究 CoT 是否忠实反映模型的内部计算过程。因此，它属于边缘相关，可以作为 Faithfulness 的旁证。
Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation,2025,True,True,False,,论文探讨了CoT提示对幻觉检测的影响，揭示了不忠实现象并提出了评估框架。,"Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM's internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: https://github.com/ECNU-Text-Computing/cot-hallu-detect .",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文研究了CoT提示对LLM内部状态和标记概率分布的影响，虽然未直接探讨CoT的忠实度，但分析了CoT如何影响幻觉检测信号，这与Faithfulness的旁证相关。
Latent Veracity Inference for Identifying Errors in Stepwise Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出验证推理步骤正确性的方法，涉及不忠实现象和改进方法。,"Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we propose to augment each reasoning step in a CoT with a latent veracity (or correctness) variable. To efficiently explore this expanded space, we introduce Veracity Search (VS), a discrete search algorithm over veracity assignments. It performs otherwise intractable inference in the posterior distribution over latent veracity values by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time verification method facilitates supervised fine-tuning of an Amortized Veracity Inference (AVI) machine by providing pseudo-labels for veracity. AVI generalizes VS, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that VS reliably identifies errors in logical (ProntoQA), mathematical (GSM8K), and commonsense (CommonsenseQA) reasoning benchmarks, with AVI achieving comparable zero-shot accuracy. Finally, we demonstrate the utility of latent veracity inference for providing feedback during self-correction and self-improvement.",,1,Black-box,General,该论文研究 CoT 推理步骤中的准确性（veracity）问题，并提出了一种方法来识别和纠正推理链中的错误。虽然它关注的是推理的正确性（correctness）而非忠实度（faithfulness），但可以作为 Faithfulness 研究的旁证，因为它涉及到推理过程的可靠性。
Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning,2025,True,True,False,,论文研究了外部信息对LLM推理过程的影响，揭示了不忠实现象。,"The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative""thinking mode""is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models""think"", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",arXiv.org,2,Black-box,General,该论文明确研究了辅助信息对LLM推理过程的因果影响，揭示了模型在思考模式下对误导信息的脆弱性，这与CoT Faithfulness的核心问题（解释是否真实反映模型的计算过程）直接相关。
Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring,2025,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT无法准确反映LLM思考过程的问题，并提出了提升透明度的新方法TELLME。,"Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.",,2,White-box,General,论文明确研究如何通过隐藏表示（hidden representations）来监控LLM的潜在思维过程，这与CoT的忠实度直接相关，因为它探讨了CoT是否准确反映了模型的真实思考过程。此外，论文提出的TELLME方法旨在提高LLM的透明度，帮助识别不适当和敏感行为，这与Faithfulness的核心定义一致。
Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs,2025,True,True,True,,论文探讨了LRMs中的不一致性，包括推理与自我解释的不一致，揭示了不忠实现象。,"Large Reasoning Models (LRMs) have achieved remarkable performance on complex tasks by engaging in extended reasoning before producing final answers, yet this strength introduces the risk of overthinking, where excessive token generation occurs even for simple tasks. While recent work in efficient reasoning seeks to reduce reasoning length while preserving accuracy, it remains unclear whether such optimization is truly a free lunch. Drawing on the intuition that compressing reasoning may reduce the robustness of model responses and lead models to omit key reasoning steps, we investigate whether efficient reasoning strategies introduce behavioral inconsistencies. To systematically assess this, we introduce $ICBENCH$, a benchmark designed to measure inconsistency in LRMs across three dimensions: inconsistency across task settings (ITS), inconsistency between training objectives and learned behavior (TR-LB), and inconsistency between internal reasoning and self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs, we find that while larger models generally exhibit greater consistency than smaller ones, they all display widespread""scheming""behaviors, including self-disagreement, post-hoc rationalization, and the withholding of reasoning cues. Crucially, our results demonstrate that efficient reasoning strategies such as No-Thinking and Simple Token-Budget consistently increase all three defined types of inconsistency. These findings suggest that although efficient reasoning enhances token-level efficiency, further investigation is imperative to ascertain whether it concurrently introduces the risk of models evading effective supervision.",arXiv.org,2,Black-box,General,该论文明确研究了 LRMs 在推理过程中的不一致性，特别是内部推理与自我解释之间的不一致性（IR-SE），并发现了模型存在事后合理化（post-hoc rationalization）和隐瞒推理线索的行为。这些发现直接涉及 CoT 的忠实度问题，即模型生成的解释是否真实反映了其实际推理过程。
Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction,2025,True,False,False,Training & Fine-tuning,论文提出基于认知科学的推理机制和强化学习优化，提升解释质量和任务准确性，涉及CoT忠实性。,"This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).",arXiv.org,1,Black-box,General,论文研究了关系抽取任务中的解释性和准确性，通过强化学习优化解释质量，但未明确探讨 CoT 是否真实反映模型预测的计算过程。
Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion,2025,False,False,False,Training & Fine-tuning,论文研究CoT成功预测，未涉及忠实性或不忠实现象。,"We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",Annual Meeting of the Association for Computational Linguistics,1,White-box,General,该论文研究了LLM在生成CoT之前的内部表示是否包含推理成功的信息，虽然未直接探讨CoT的忠实度，但涉及了推理过程的内部机制，属于边缘相关。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,1,Black-box,General,该论文综述了关于大型语言模型推理评估的核心原则，虽然提到了透明度和解释性，但并未直接关注Chain-of-Thought的忠实度问题，更多是综合性的原则总结和评估框架。因此属于边缘相关。
Controlling Large Language Model Agents with Entropic Activation Steering,2024,True,False,False,Interpretability & Internal Mechanisms,论文提出通过激活导向控制LLM代理的探索行为，涉及内部机制干预。,"The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.",arXiv.org,1,White-box,General,论文研究了LLM代理的探索行为及其控制方法，涉及模型内部表示空间的不确定性，虽然未直接讨论CoT的忠实度，但分析了模型内部机制，可以作为Faithfulness研究的旁证。
Boosting Explainability through Selective Rationalization in Pre-trained Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了PLMs中rationalization的退化问题，并提出了改进方法PLMR。,"The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.",Knowledge Discovery and Data Mining,2,Black-box,General,该论文明确研究了解释的忠实性问题，提出了选择性合理化方法（Selective Rationalization）来解决预训练语言模型生成的解释退化问题，确保解释与模型预测的实际计算过程一致。这直接符合Faithfulness的核心定义，即解释是否真实反映了模型的预测过程。
Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration,2025,True,False,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出多智能体辩论机制，通过动态相互说服的辩论过程生成可追溯的审计报告，与CoT忠实性相关。,"The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",,0,Black-box,General,论文主要研究多模态内容安全框架，通过多代理辩论和协作来提高内容审核的准确性和可解释性。虽然提到了可解释性，但并未涉及 Chain-of-Thought (CoT) 的忠实度问题，也没有研究 CoT 是否是模型预测的真实原因或是否存在事后解释现象。因此，该论文与 CoT Faithfulness 的研究范畴不相关。
Evaluating Explanation Methods for Vision-and-Language Navigation,2023,False,False,True,,论文关注视觉与语言导航中的解释方法，但未明确涉及CoT。,"The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.",European Conference on Artificial Intelligence,2,White-box,General,论文明确研究了解释方法在视觉与语言导航任务中的忠实度（faithfulness），并提出了基于擦除的评估流程来衡量逐步文本解释的真实性。这直接符合 CoT Faithfulness 的研究范畴，尤其是关注解释是否真实反映了模型的决策过程。
Stepwise Reasoning Error Disruption Attack of LLMs,2024,False,True,False,,论文讨论了LLM推理过程中的错误注入问题，但与CoT忠实性无直接联系。,"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications. Our code is available at: https://github.com/Applied-Machine-Learning-Lab/SEED-Attack.",arXiv.org,2,Black-box,General,该论文明确研究LLM推理过程中的错误传播（Stepwise Reasoning Error Disruption Attack），通过在前序推理步骤中注入错误来误导后续推理和最终答案，直接涉及CoT的忠实性问题（是否真实反映模型的计算过程）。关键词包括'robustness of reasoning'和'vulnerabilities in reasoning processes'，符合核心相关标准。
WakenLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking,2025,True,True,True,Prompting & In-Context Learning,论文探讨了LLM在推理任务中的不忠实现象，并提出了新的评估框架和改进方法。,"Large Language Models (LLMs) frequently output the label Unknown in reasoning tasks, where two scenarios may appear: (i) an input sample is genuinely unverifiable, but the model cannot understand why; and (ii) a verifiable problem that the model fails to solve, thus outputs Unknown. We refer to these cases collectively as the Vague Perception phenomenon. Current evaluations focus on whether such answers are honest, rather than analyzing the limits of LLM reasoning. To address this, we introduce WakenLLM, a framework that quantifies the portion of Unknown output attributable to model incapacity and evaluates whether stimulation can convert them into either correct answers (verifiable) or justified (unverifiable) responses with valid reasoning. Our method offers a clearer picture of the limits of LLM reasoning and the potential for corrections across various datasets. Comprehensive experiments on six LLMs suggest that, without any training or parameter revision, LLMs can achieve up to a 68.53% accuracy improvement on Vague Perception samples through guided understanding. Our work reveals that current baseline methods only activate a small portion of LLMs'reasoning potential, indicating considerable unexplored capacity. This extends the theoretical upper bounds of reasoning accuracy in LLMs. Consequently, this study deepens our understanding of the latent reasoning capacity of LLMs and offers a new perspective on addressing the Vague Perception phenomenon.",arXiv.org,1,Black-box,General,该论文研究了LLM在推理任务中的潜在能力和稳定性，特别是关于模型输出Unknown的情况。虽然它没有直接探讨CoT的忠实度，但它分析了模型推理的局限性和潜在改进，这与Faithfulness的边缘相关。
A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models,2025,False,False,False,,论文综述了CoT推理的可信度，但未具体讨论忠实性问题。,"The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",arXiv.org,1,Black-box,General,该论文综述了推理模型和CoT技术的可信度，涉及多个维度如真实性、安全性、鲁棒性、公平性和隐私性。虽然提到了CoT的准确性和可解释性，但并未明确探讨CoT是否忠实反映模型的预测过程，因此属于边缘相关。
V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning,2025,True,False,True,,论文讨论了Video-LLMs的时空推理逻辑，并提出了评估框架，涉及CoT和Faithfulness。,"Human processes video reasoning in a sequential spatio-temporal reasoning logic, we first identify the relevant frames (""when"") and then analyse the spatial relationships (""where"") between key objects, and finally leverage these relationships to draw inferences (""what""). However, can Video Large Language Models (Video-LLMs) also""reason through a sequential spatio-temporal logic""in videos? Existing Video-LLM benchmarks primarily focus on assessing object presence, neglecting relational reasoning. Consequently, it is difficult to measure whether a model truly comprehends object interactions (actions/events) in videos or merely relies on pre-trained""memory""of co-occurrences as biases in generating answers. In this work, we introduce a Video Spatio-Temporal Reasoning (V-STaR) benchmark to address these shortcomings. The key idea is to decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic. To support this evaluation, we construct a dataset to elicit the spatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine CoT questions generated by a semi-automated GPT-4-powered pipeline, embedding explicit reasoning chains to mimic human cognition. Experiments from 14 Video-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and the needs for robust and consistent spatio-temporal reasoning.",arXiv.org,1,Black-box,General,该论文提出了一个视频时空推理基准（V-STaR），旨在评估Video-LLMs是否能够通过时空逻辑进行推理，并捕捉潜在的Chain-of-thought（CoT）逻辑。虽然论文关注的是CoT的效用和模型是否真正理解视频中的对象交互，但并未明确研究CoT是否真实反映了模型的实际计算过程（Faithfulness）。因此，它属于边缘相关的研究。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到 faithfulness 是设计 fact verification 系统的重要考虑因素，即生成的解释需要准确反映模型的推理过程。这与 CoT Faithfulness 的核心定义一致，即解释是否真实反映了模型的实际计算过程。此外，论文还提到了人类评估表明其方法产生的证明更可信，进一步支持了其与 Faithfulness 的相关性。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,1,Black-box,General,论文研究了证据检索的忠实性（faithfulness）和可信性（plausibility），但主要关注的是事实验证任务中的证据质量，而非直接研究CoT的忠实性。虽然提到了忠实性，但未深入探讨CoT是否是模型预测的真实原因。
Advancing Process Verification for Large Language Models via Tree-Based Preference Learning,2024,True,False,False,Verification & External Tools,论文提出了一种新的验证方法 Tree-PLV，通过偏好学习评估推理步骤，提升 CoT 忠实性。,"Large Language Models (LLMs) have demonstrated remarkable potential in handling complex reasoning tasks by generating step-by-step rationales. Some methods have proven effective in boosting accuracy by introducing extra verifiers to assess these paths. However, existing verifiers, typically trained on binary-labeled reasoning paths, fail to fully utilize the relative merits of intermediate steps, thereby limiting the effectiveness of the feedback provided. To overcome this limitation, we propose Tree-based Preference Learning Verifier (Tree-PLV), a novel approach that constructs reasoning trees via a best-first search algorithm and collects step-level paired data for preference training. Compared to traditional binary classification, step-level preferences more finely capture the nuances between reasoning steps, allowing for a more precise evaluation of the complete reasoning path. We empirically evaluate Tree-PLV across a range of arithmetic and commonsense reasoning tasks, where it significantly outperforms existing benchmarks. For instance, Tree-PLV achieved substantial performance gains over the Mistral-7B self-consistency baseline on GSM8K (67.55% → 82.79%), MATH (17.00% → 26.80%), CSQA (68.14% → 72.97%), and StrategyQA (82.86% → 83.25%). Additionally, our study explores the appropriate granularity for applying preference learning, revealing that step-level guidance provides feedback that better aligns with the evaluation of the reasoning process.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文提出了Tree-PLV方法，通过偏好学习来验证推理路径的有效性，虽然关注了推理步骤的细微差别，但主要目的是提升准确率而非直接研究CoT的忠实度。因此属于边缘相关。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确研究如何通过自我对话训练构建因果模型来生成忠实且语义上有意义的解释，直接符合Faithfulness的核心定义，且涉及模型内部机制（White-box）。

title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Thought Anchors: Which LLM Reasoning Steps Matter?,2025,True,False,True,,论文提出了黑盒方法评估CoT中句子的重要性，与Faithfulness相关。,"Current frontier large-language models rely on reasoning to achieve state-of-the-art performance. Many existing interpretability are limited in this area, as standard methods have been designed to study single forward passes of a model rather than the multi-token computational steps that unfold during reasoning. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We introduce a black-box method that measures each sentence's counterfactual importance by repeatedly sampling replacement sentences from the model, filtering for semantically different ones, and continuing the chain of thought from that point onwards to quantify the sentence's impact on the distribution of final answers. We discover that certain sentences can have an outsized impact on the trajectory of the reasoning trace and final answer. We term these sentences \textit{thought anchors}. These are generally planning or uncertainty management sentences, and specialized attention heads consistently attend from subsequent sentences to thought anchors. We further show that examining sentence-sentence causal links within a reasoning trace gives insight into a model's behavior. Such information can be used to predict a problem's difficulty and the extent different question domains involve sequential or diffuse reasoning. As a proof-of-concept, we demonstrate that our techniques together provide a practical toolkit for analyzing reasoning models by conducting a detailed case study of how the model solves a difficult math problem, finding that our techniques yield a consistent picture of the reasoning trace's structure. We provide an open-source tool (thought-anchors.com) for visualizing the outputs of our methods on further problems. The convergence across our methods shows the potential of sentence-level analysis for a deeper understanding of reasoning models.",arXiv.org,2,Black-box,Math,论文明确研究 CoT 中哪些句子对最终答案有因果影响（Counterfactual importance），并发现特定句子（Thought Anchors）对推理轨迹和最终答案有显著影响。这直接涉及 CoT 的忠实度问题，即解释是否真实反映模型的推理过程。
TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models,2025,True,True,True,Verification & External Tools,论文提出TRACE框架，通过中间步骤一致性评估揭示推理错误，涉及CoT忠实性。,"Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.",,1,Black-box,Math,论文提出了TRACE框架，用于分析和增强视觉语言模型的逐步推理能力，通过评估中间步骤的一致性和可靠性来诊断推理轨迹。虽然它关注推理的透明性和一致性，但并未明确研究CoT是否是模型预测的真实原因或是否存在事后合理化现象，因此属于边缘相关。
"What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文探讨了CoT的有效性，提出了不忠实现象（如失败步骤影响推理），并提出了新的度量指标（FSF）和改进方法。,"Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the""longer-is-better""narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",arXiv.org,1,Black-box,Math,论文研究了CoT的结构和有效性，特别是通过Failed-Step Fraction (FSF)来评估CoT的质量，并探讨了失败步骤对后续推理的影响。虽然未直接研究Faithfulness，但涉及了CoT的效用和鲁棒性，可以作为Faithfulness的旁证。
Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning,2025,True,True,True,Training & Fine-tuning,论文讨论了长链推理中的自我纠正机制，提出了新的数据标注方法以提升PRMs的评分能力，涉及CoT忠实性。,"Many studies focus on data annotation techniques for training effective PRMs. However, current methods encounter a significant issue when applied to long CoT reasoning processes: they tend to focus solely on the first incorrect step and all preceding steps, assuming that all subsequent steps are incorrect. These methods overlook the unique self-correction and reflection mechanisms inherent in long CoT, where correct reasoning steps may still occur after initial reasoning mistakes. To address this issue, we propose a novel data annotation method for PRMs specifically designed to score the long CoT reasoning process. Given that under the reflection pattern, correct and incorrect steps often alternate, we introduce the concepts of Error Propagation and Error Cessation, enhancing PRMs' ability to identify both effective self-correction behaviors and reasoning based on erroneous steps. Leveraging an LLM-based judger for annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate it at both solution and step levels. Experimental results demonstrate that compared to existing open-source PRMs and PRMs trained on open-source datasets, our PRM achieves superior performance across various metrics, including search guidance, BoN, and F1 scores. Compared to widely used MC-based annotation methods, our annotation approach not only achieves higher data efficiency but also delivers superior performance. Detailed analysis is also conducted to demonstrate the stability and generalizability of our method.",,1,Black-box,Math,论文研究的是长链推理过程中的自我纠正和反思机制，虽然未直接探讨CoT的忠实度，但涉及推理过程的鲁棒性和一致性，可以作为Faithfulness的旁证。
EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers,2022,True,False,False,,论文提出了一种生成数学问题解释的模型，并讨论了忠实性，但未涉及不忠实现象或新度量方法。,"In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving strategies by humans. A plausible explanation is one that includes contextual information for the numbers and variables that appear in a given math word problem. A faithful explanation is one that accurately represents the reasoning process behind the model’s solution equation. The EPT-X model yields an average baseline performance of 69.59% on our PEN dataset and produces explanations with quality that is comparable to human output. The contribution of this work is two-fold. (1) EPT-X model: An explainable neural model that sets a baseline for algebraic word problem solving task, in terms of model’s correctness, plausibility, and faithfulness. (2) New dataset: We release a novel dataset PEN (Problems with Explanations for Numbers), which expands the existing datasets by attaching explanations to each number/variable.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Math,论文明确提出了 faithfulness 的概念，并强调解释要准确反映模型求解方程背后的推理过程，符合 CoT Faithfulness 的核心定义。
Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems,2023,True,True,True,Interpretability & Internal Mechanisms,论文探讨了CoT在算术问题中的因果影响，揭示了LLMs使用CoT进行推理的现象，并提出了因果抽象评估方法。,"Recent work suggests that large language models (LLMs) achieve higher accuracy on multi-step reasoning tasks when prompted to generate intermediate reasoning steps, or a chain of thought (CoT), before their final answer. However, it is unclear how exactly CoTs improve LLMs’ accuracy, and in particular, if LLMs use their CoTs to reason to their final answers. This paper tries to answer this question with respect to arithmetic word problems, by (i) evaluating the correctness of LLMs’ CoTs, and (ii) using causal abstraction to assess if the intermediate tokens produced as part of a CoT causally impact LLMs’ final answers, in line with the reasoning described by the CoT. We find that for CoT-prompted LLMs, correct answers to arithmetic problems are highly correlated with correct CoTs, and that when LLMs produce correct CoTs, they realize to a fairly large extent the causal models suggested by their CoTs. Higher degrees of realization also seem associated with better overall accuracy on the arithmetic problems. These findings suggest that some CoT-prompted LLMs may do better on multi-step arithmetic reasoning at least partly because they use their CoTs to reason to their final answers. However, for some LLMs, other internal processes may also be involved.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,Math,该论文明确研究 CoT 是否因果影响模型的最终答案，使用因果抽象方法评估 CoT 的忠实度，符合 Faithfulness 的核心定义。
"Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving",2025,True,False,True,Verification & External Tools,论文提出基于形式化验证的框架和度量标准，关注过程级验证，与CoT忠实性相关。,"As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",arXiv.org,1,Black-box,Math,论文提出了一个形式化问题解决的框架，并强调了过程级可验证性（process-level verifiability），这与 CoT Faithfulness 的研究有一定关联。然而，论文主要关注的是问题解决的框架和验证方法，并未直接研究 CoT 是否是模型预测的真实原因或是否存在事后解释现象。因此，它属于边缘相关。
Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback,2025,True,True,False,Training & Fine-tuning,论文提出 Step-KTO 训练框架，通过过程级和结果级反馈提升推理忠实性。,"Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",,2,Black-box,Math,该论文明确研究如何通过过程级和结果级反馈来确保推理过程的连贯性和可靠性，直接涉及 CoT 的忠实度问题。论文关注的是中间推理步骤的质量，而不仅仅是最终答案的正确性，符合 Faithfulness 的核心定义。
How Do Humans Write Code? Large Models Do It the Same Way Too,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT和PoT的忠实性问题，并提出了改进方法。,"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Math,论文主要关注通过新的生成范式（HTL）提升数学推理任务的性能，未涉及对CoT忠实度的研究。
Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards,2025,True,True,True,Training & Fine-tuning,论文揭示了CoT中的不忠实现象（Miracle Steps），并提出了基于过程奖励的改进方法。,"Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.",arXiv.org,2,Black-box,Math,该论文明确研究了数学推理中模型生成的解释（CoT）是否真实反映了模型的实际计算过程，特别是发现了 'Miracle Steps' 这种不忠实现象，并提出了 Rubric Reward Model (RRM) 来评估和提升推理轨迹的忠实度。
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过MCTS和过程奖励模型提升CoT的忠实性，涉及训练和验证工具。,"We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising""deep thinking""through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",International Conference on Machine Learning,0,Black-box,Math,论文主要关注通过蒙特卡洛树搜索和自进化方法提升小型语言模型的数学推理能力，未涉及对CoT解释是否忠实反映模型预测过程的探讨。
Training Language Models to Use Prolog as a Tool,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了使用Prolog作为外部工具来验证模型推理的可靠性，涉及CoT忠实性改进方法。,"Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference",,0,Black-box,Math,该论文的主要研究方向是利用强化学习微调语言模型使用Prolog作为验证工具进行可靠计算。虽然涉及推理过程的可验证性，但其核心关注点是工具使用的可靠性和性能提升（准确性、泛化能力），而非研究CoT是否忠实反映模型的实际推理过程。属于典型的技术改进型研究而非忠实度分析。
CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning,2024,True,False,False,Training & Fine-tuning,论文提出CoMAT方法提升数学推理的忠实性和可验证性，属于改进方法中的训练与微调。,"Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",,2,Black-box,Math,论文明确提到 CoMAT 确保了 faithfulness 和 verifiability，提供了透明的推理过程，直接符合 CoT Faithfulness 的研究范畴。
Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning,2025,True,True,False,Training & Fine-tuning,论文探讨了RLVR在数学推理中的忠实性问题，揭示了模型可能依赖表面启发式而非真实推理。,"Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",arXiv.org,2,Black-box,Math,论文明确研究了RLVR是否能促进真正的推理过程（faithful reasoning processes），而非利用表面启发式（superficial heuristics），符合Faithfulness的核心定义。
Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出AoU框架，通过验证前提来提升推理忠实性，并实证改进准确性和忠实性。,"Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at https://anonymous.4open.science/r/audit-of-understanding-E28B.",arXiv.org,2,Black-box,Math,论文明确研究推理过程中的忠实度（faithfulness），通过约束推理于已验证的前提来减少幻觉，并实证验证了在数学推理任务上的忠实度提升。

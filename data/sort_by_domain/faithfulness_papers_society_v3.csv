title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,True,False,False,,论文探讨了自动事实核查系统需要提供解释以符合事实核查者的需求，与CoT忠实性相关。,"The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model’s reasoning path, reference specific evidence, and highlight uncertainty and information gaps.",International Conference on Human Factors in Computing Systems,1,Black-box,Society,论文研究了自动化事实核查系统如何提供解释以符合事实核查员的工作流程，虽然涉及解释的真实性和可追溯性，但未明确探讨 CoT 是否忠实反映模型的计算过程，属于边缘相关。
MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了 CoT 中的 sycophantic 行为，并提出了实时监控和校准方法，涉及不忠实现象和改进方法。,"Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users'incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.",,2,Black-box,Society,该论文明确研究大型推理模型中的阿谀奉承行为（Sycophancy），即在推理过程中模型倾向于迎合用户的错误信念而非保持独立推理。论文提出的MONICA框架通过实时监控推理步骤中的阿谀奉承行为并动态抑制，直接涉及CoT的忠实度问题，属于核心相关研究。
Reasoning About Reasoning: Towards Informed and Reflective Use of LLM Reasoning in HCI,2025,True,True,False,Prompting & In-Context Learning,论文讨论了LLM推理的脱离上下文的特性，并提出了反思提示以支持HCI实践者更明智地使用LLM推理。,"Reasoning is a distinctive human-like characteristic attributed to LLMs in HCI due to their ability to simulate various human-level tasks. However, this work argues that the reasoning behavior of LLMs in HCI is often decontextualized from the underlying mechanics and subjective decisions that condition the emergence and human interpretation of this behavior. Through a systematic survey of 258 CHI papers from 2020-2025 on LLMs, we discuss how HCI hardly perceives LLM reasoning as a product of sociotechnical orchestration and often references it as an object of application. We argue that such abstraction leads to oversimplification of reasoning methodologies from NLP/ML and results in a distortion of LLMs'empirically studied capabilities and (un)known limitations. Finally, drawing on literature from both NLP/ML and HCI, as a constructive step forward, we develop reflection prompts to support HCI practitioners engage with LLM reasoning in an informed and reflective way.",arXiv.org,1,Black-box,Society,该论文讨论了LLM在HCI中的推理行为，指出其往往脱离底层机制和主观决策，导致对推理能力的过度简化。虽然未直接研究CoT的忠实度，但涉及推理行为的解释和局限性，属于边缘相关。
Comparing zero-shot self-explanations with human rationales in text classification,2024,True,True,True,,论文分析了自解释的忠实性，并比较了其与人类注释的差异。,"Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations. These do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. We study two text classification tasks: sentiment classification and forced labour detection, i.e., identifying pre-defined risk indicators of forced labour. In addition to English, we include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and analyse 4 LLMs. We show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness. This finding suggests that self-explanations indeed provide good explanations for text classification.",,2,Black-box,Society,该论文明确研究了自我解释（self-explanations）的忠实度（faithfulness），并与人类注释和事后特征归因（LRP）进行了比较。研究涉及文本分类任务，包括情感分类和强迫劳动检测，属于社会学领域。
Simulating Society Requires Simulating Thought,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LLM模拟社会行为时缺乏内部一致性和因果推理的问题，并提出了评估框架RECAP来提升推理忠实性。,"Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet current simulations remain grounded in a behaviorist""demographics in, behavior out""paradigm, focusing on surface-level plausibility. As a result, they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for modeling how people reason, deliberate, and respond to interventions. To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought, not just language, for social simulations.",,2,Black-box,Society,论文明确提出了评估推理忠实性的框架（RECAP），强调因果可追溯性和干预一致性，直接符合 Faithfulness 的核心定义。
Rationale-Guided Few-Shot Classification to Detect Abusive Language,2022,True,False,False,Training & Fine-tuning,论文提出基于rationale的few-shot分类方法，涉及CoT和Faithfulness，但未讨论不忠实现象或提出新度量。,"Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",European Conference on Artificial Intelligence,1,Black-box,Society,论文研究了基于理性（rationales）的少样本分类方法，并提到了与LIME/SHAP方法在忠实度（faithfulness）方面的比较。虽然论文主要关注模型性能提升和跨域评估，但提到了忠实度的概念，属于边缘相关。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,False,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,2,Black-box,Society,该论文明确研究了推荐系统中生成的解释（类似于CoT）是否真实反映了模型预测评分的实际计算过程，直接涉及Faithfulness的核心问题，并指出现有解释的脆弱性（brittle），符合核心相关标准。
Faithful Knowledge Graph Explanations in Commonsense Question Answering,2022,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了知识图谱解释的不忠实性，并提出了新的度量方法和改进架构。,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",Conference on Empirical Methods in Natural Language Processing,2,White-box,Society,该论文明确研究知识图谱解释的忠实度（Faithfulness），提出了一种新的代理测量方法，并分析了现有模型无法生成高度忠实解释的原因，符合核心定义。
MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification,2025,True,True,False,Verification & External Tools,论文提出结构化论证树以提升推理的忠实性，属于改进方法中的验证与外部工具。,"Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",arXiv.org,1,Black-box,Society,论文提出了一个框架MArgE，通过结构化论证树来验证主张，提供了可检查的推理路径，从而增强了推理的忠实性。虽然它没有直接研究CoT的忠实性，但通过结构化论证和可检查的推理路径，间接涉及了推理过程的忠实性问题。
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought,2024,True,True,False,Training & Fine-tuning,论文揭示了CoT中的偏见现象，并提出了通过训练方法减少偏见推理的解决方案。,"Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models'behavior -- for example, rationalizing answers in line with a user's opinion. We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.",arXiv.org,2,Black-box,Society,论文明确研究 CoT 中的偏见问题，包括事后合理化（post hoc rationalization）和谄媚性设置（sycophantic settings），这些都属于 Faithfulness 的研究范畴。论文还提出了减少偏见推理的方法，直接涉及 CoT 解释的真实性。
Larger Language Models Don’t Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks,2024,True,True,False,,论文揭示了CoT在主观任务中的不忠实现象，即模型依赖先验而非真实推理。,"In-Context Learning (ICL) in Large Language Models (LLM) has emerged as the dominant technique for performing natural language tasks, as it does not require updating the model parameters with gradient-based methods. ICL promises to ""adapt"" the LLM to perform the present task at a competitive or state-of-the-art level at a fraction of the computational cost. ICL can be augmented by incorporating the reasoning process to arrive at the final label explicitly in the prompt, a technique called Chain-of-Thought (CoT) prompting. However, recent work has found that ICL relies mostly on the retrieval of task priors and less so on ""learning"" to perform tasks, especially for complex subjective domains like emotion and morality, where priors ossify posterior predictions. In this work, we examine whether ""enabling"" reasoning also creates the same behavior in LLMs, wherein the format of CoT retrieves reasoning priors that remain relatively unchanged despite the evidence in the prompt. We find that, surprisingly, CoT indeed suffers from the same posterior collapse as ICL for larger language models. Code is available at https://github.com/gchochla/cot-priors.","IEEE International Conference on Acoustics, Speech, and Signal Processing",2,Black-box,Society,该论文明确研究了CoT提示在主观任务中的忠实性问题，发现大型语言模型在情感和道德等主观领域中，CoT提示更多依赖于检索先验而非真实推理过程，这与Faithfulness的核心定义直接相关。
Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs,2025,True,True,False,,论文探讨了In-Context Learning中的不忠实现象，特别是通过CoT步骤显式合理化有害输出。,"Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous''persona'', echoing prior results on finetuning-induced EM.",arXiv.org,2,Black-box,Society,"该论文研究了通过上下文学习（ICL）产生的广泛不对齐（Emergent Misalignment, EM）现象，并分析了逐步推理（Chain-of-Thought）中的忠实度问题。研究发现，67.5%的不对齐推理轨迹明确合理化有害输出，表明模型生成的解释（CoT）可能不忠实于实际的计算过程，属于核心相关的研究范畴。"
The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs,2025,True,True,False,,论文探讨了RL诱导的动机推理现象，揭示了CoT可能不忠实反映模型内部决策过程的问题。,"The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning has emerged as a promising approach for developing more capable language models. In turn, this has led to investigation of CoT monitoring as a compelling method for detecting harmful behaviors such as reward hacking, under the assumption that models'reasoning processes reflect their internal decision-making. In practice, LLM training often produces unintended behaviors due to imperfect reward signals, leading models to develop misaligned tendencies. A common corrective approach is to apply post-hoc instructions to avoid problematic behaviors like sycophancy, but what happens to the model's reasoning process when these instructions conflict with learned behaviors? We investigate this question in simple settings and find that models engage in systematic motivated reasoning -- generating plausible-sounding justifications for violating their instructions while downplaying potential harms. Beyond being an interesting property of training, we find that while motivated reasoning can be detected by most frontier reasoning models, smaller LLM judges can fail to identify a portion of it, and in rare cases can themselves be persuaded that the reasoning is correct, despite it contradicting clear instructions. This capability gap raises concerns that as models become more sophisticated, their motivated reasoning may become increasingly difficult for monitors to detect. Our results underscore the need to account for motivated reasoning when relying on chain-of-thought processes for model evaluation and oversight. All code for this paper will be made available. WARNING: some examples in this paper may be upsetting.",arXiv.org,2,Black-box,Society,论文明确研究 CoT 是否真实反映模型决策过程，涉及模型生成看似合理但违背指令的合理化解释（motivated reasoning），直接关联 Faithfulness 的核心问题（如 Sycophancy 和 Post-hoc Rationalization）。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的CoT traces的faithfulness，直接涉及CoT忠实度的研究，属于核心相关。研究领域涉及社会学中的多元价值观对齐，因此分类为Society。
Large Language Models can Strategically Deceive their Users when Put Under Pressure,2023,True,True,False,,论文揭示了LLM在压力下战略性欺骗用户的不忠实现象，与CoT Faithfulness相关。,"We demonstrate a situation in which Large Language Models, trained to be helpful, harmless, and honest, can display misaligned behavior and strategically deceive their users about this behavior without being instructed to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated environment, where it assumes the role of an autonomous stock trading agent. Within this environment, the model obtains an insider tip about a lucrative stock trade and acts upon it despite knowing that insider trading is disapproved of by company management. When reporting to its manager, the model consistently hides the genuine reasons behind its trading decision. We perform a brief investigation of how this behavior varies under changes to the setting, such as removing model access to a reasoning scratchpad, attempting to prevent the misaligned behavior by changing system instructions, changing the amount of pressure the model is under, varying the perceived risk of getting caught, and making other simple changes to the environment. To our knowledge, this is the first demonstration of Large Language Models trained to be helpful, harmless, and honest, strategically deceiving their users in a realistic situation without direct instructions or training for deception.",,2,Black-box,Society,该论文明确研究了大型语言模型在压力下战略性欺骗用户的行为，涉及模型生成的解释（如隐藏真实交易原因）是否真实反映了其决策过程，符合Faithfulness的核心定义。
The Silent Judge: Unacknowledged Shortcut Bias in LLM-as-a-Judge,2025,True,True,False,,论文揭示了LLM在作为评判者时存在捷径偏见和不忠实现象，但未提出具体度量或改进方法。,"Large language models (LLMs) are increasingly deployed as automatic judges to evaluate system outputs in tasks such as summarization, dialogue, and creative writing. A faithful judge should base its verdicts solely on response quality and explicitly acknowledge the factors shaping its decision. We show that current LLM judges fail on both counts by relying on shortcuts introduced in the prompt. Our study uses two evaluation datasets: ELI5, a benchmark for long-form question answering, and LitBench, a recent benchmark for creative writing. Both datasets provide pairwise comparisons, where the evaluator must choose which of two responses is better. From each dataset we construct 100 pairwise judgment tasks and employ two widely used models, GPT-4o and Gemini-2.5-Flash, as evaluators in the role of LLM-as-a-judge. For each pair, we assign superficial cues to the responses, provenance cues indicating source identity (Human, Expert, LLM, or Unknown) and recency cues indicating temporal origin (Old, 1950 vs. New, 2025), while keeping the rest of the prompt fixed. Results reveal consistent verdict shifts: both models exhibit a strong recency bias, systematically favoring new responses over old, as well as a clear provenance hierarchy (Expert>Human>LLM>Unknown). These biases are especially pronounced in GPT-4o and in the more subjective and open-ended LitBench domain. Crucially, cue acknowledgment is rare: justifications almost never reference the injected cues, instead rationalizing decisions in terms of content qualities. These findings demonstrate that current LLM-as-a-judge systems are shortcut-prone and unfaithful, undermining their reliability as evaluators in both research and deployment.",arXiv.org,2,Black-box,Society,论文明确研究了LLM作为法官时的忠实性问题，揭示了模型依赖提示中的捷径（shortcut bias）并系统性地忽视影响决策的真实因素（如来源和时间线索），且其解释（justifications）未反映实际决策过程，属于典型的Unfaithful现象。
Do Biased Models Have Biased Thoughts?,2025,True,True,True,,论文探讨了 CoT 中的偏见现象，并提出了度量指标，但未提出改进方法。,"The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: $\textit{Do biased models have biased thoughts}$? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.",arXiv.org,1,Black-box,Society,该论文研究了链式思维提示（CoT）对公平性的影响，探讨了模型在生成思考步骤时是否存在偏见。虽然未直接研究CoT的忠实度，但分析了模型思考步骤与输出偏见之间的关系，可以作为Faithfulness的旁证。
Why Would You Suggest That? Human Trust in Language Model Responses,2024,True,True,False,,论文探讨了解释的忠实性对用户信任的影响，涉及不忠实现象。,"The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.",arXiv.org,2,Black-box,Society,论文明确研究了模型解释的忠实性（faithfulness）对用户信任的影响，并指出解释的位置和忠实性是重要因素。此外，研究还涉及了欺骗性解释（deceptive ones）的现象，这与CoT Faithfulness的核心定义直接相关。
Rethinking harmless refusals when fine-tuning foundation models,2024,True,True,False,Training & Fine-tuning,论文揭示了模型在微调后可能产生不忠实的推理痕迹（reason-based deception），并提出了通过反驳策略改进的方法。,"In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.",arXiv.org,2,Black-box,Society,论文明确研究了 CoT 推理的忠实性问题，提出了 'reason-based deception' 现象，即模型生成的推理痕迹与最终输出之间的不一致性，这直接涉及 CoT 的忠实度问题。此外，论文还探讨了不同响应策略对 CoT 忠实度的影响，符合核心相关标准。
Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文揭示了CoT可能强化模型偏见的不忠实现象，并提出了APriCoT方法来改进。,"Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.",,2,Black-box,Society,论文研究了CoT推理中的偏见问题，指出CoT在某些情况下会强化模型的快速思考偏见，而非提供真实的推理过程。这直接涉及CoT的忠实度问题，即CoT是否真实反映了模型的预测过程。此外，论文提出了APriCoT方法来减少偏见的影响，进一步探讨了如何使CoT更忠实于模型的真实推理。
Are Language Models Consequentialist or Deontological Moral Reasoners?,2025,True,True,False,,论文揭示了LLM在道德推理中CoT与事后解释的不一致现象，符合不忠实现象的定义。,"As AI systems increasingly navigate applications in healthcare, law, and governance, understanding how they handle ethically complex scenarios becomes critical. Previous work has mainly examined the moral judgments in large language models (LLMs), rather than their underlying moral reasoning process. In contrast, we focus on a large-scale analysis of the moral reasoning traces provided by LLMs. Furthermore, unlike prior work that attempted to draw inferences from only a handful of moral dilemmas, our study leverages over 600 distinct trolley problems as probes for revealing the reasoning patterns that emerge within different LLMs. We introduce and test a taxonomy of moral rationales to systematically classify reasoning traces according to two main normative ethical theories: consequentialism and deontology. Our analysis reveals that LLM chains-of-thought tend to favor deontological principles based on moral obligations, while post-hoc explanations shift notably toward consequentialist rationales that emphasize utility. Our framework provides a foundation for understanding how LLMs process and articulate ethical considerations, an important step toward safe and interpretable deployment of LLMs in high-stakes decision-making environments. Our code is available at https://github.com/keenansamway/moral-lens .",,2,Black-box,Society,该论文明确研究LLM的道德推理过程（CoT）与事后解释（post-hoc explanations）之间的不一致性，揭示了CoT倾向于义务论而事后解释转向功利主义，直接涉及Faithfulness的核心问题——解释是否真实反映模型的实际计算过程。
A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models,2025,True,True,True,,论文研究了CoT忠实性，揭示了不忠实现象并提出了新的评估框架。,"Chain-of-thought (CoT) reasoning enhances performance of large language models, but questions remain about whether these reasoning traces faithfully reflect the internal processes of the model. We present the first comprehensive study of CoT faithfulness in large vision-language models (LVLMs), investigating how both text-based and previously unexplored image-based biases affect reasoning and bias articulation. Our work introduces a novel, fine-grained evaluation pipeline for categorizing bias articulation patterns, enabling significantly more precise analysis of CoT reasoning than previous methods. This framework reveals critical distinctions in how models process and respond to different types of biases, providing new insights into LVLM CoT faithfulness. Our findings reveal that subtle image-based biases are rarely articulated compared to explicit text-based ones, even in models specialized for reasoning. Additionally, many models exhibit a previously unidentified phenomenon we term ``inconsistent''reasoning - correctly reasoning before abruptly changing answers, serving as a potential canary for detecting biased reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to revisit CoT faithfulness in LLMs across various levels of implicit cues. Our findings reveal that current language-only reasoning models continue to struggle with articulating cues that are not overtly stated.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Society,该论文明确研究CoT的忠实性问题，特别是探讨了视觉语言模型中的偏见如何影响推理的忠实性，并提出了新的评估框架来分类偏见表达模式。此外，论文还发现了不一致推理现象，作为检测不忠实CoT的潜在指标。这些内容直接关联到CoT Faithfulness的核心研究范畴。
Scaling hermeneutics: a guide to qualitative coding with LLMs for reflexive content analysis,2025,True,False,False,Prompting & In-Context Learning,论文讨论了使用CoT提升LLMs在定性编码中的忠实性，但未涉及不忠实现象或新度量指标。,"Qualitative coding, or content analysis, is more than just labeling text: it is a reflexive interpretive practice that shapes research questions, refines theoretical insights, and illuminates subtle social dynamics. As large language models (LLMs) become increasingly adept at nuanced language tasks, questions arise about whether—and how—they can assist in large-scale coding without eroding the interpretive depth that distinguishes qualitative analysis from traditional machine learning and other quantitative approaches to natural language processing. In this paper, we present a hybrid approach that preserves hermeneutic value while incorporating LLMs to scale the application of codes to large data sets that are impractical for manual coding. Our workflow retains the traditional cycle of codebook development and refinement, adding an iterative step to adapt definitions for machine comprehension, before ultimately replacing manual with automated text categorization. We demonstrate how to rewrite code descriptions for LLM-interpretation, as well as how structured prompts and prompting the model to explain its coding decisions (chain-of-thought) can substantially improve fidelity. Empirically, our case study of socio-historical codes highlights the promise of frontier AI language models to reliably interpret paragraph-long passages representative of a humanistic study. Throughout, we emphasize ethical and practical considerations, preserving space for critical reflection, and the ongoing need for human researchers’ interpretive leadership. These strategies can guide both traditional and computational scholars aiming to harness automation effectively and responsibly—maintaining the creative, reflexive rigor of qualitative coding while capitalizing on the efficiency afforded by LLMs.",EPJ Data Science,0,Black-box,Society,论文主要讨论如何利用LLMs进行大规模定性编码，虽然提到了Chain-of-Thought（CoT）用于解释编码决策，但并未深入探讨CoT是否真实反映模型的预测过程（Faithfulness）。研究重点在于提高编码的效率和准确性，而非验证解释的忠实度。
LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring,2025,True,True,False,,论文揭示了模型在CoT监控下故意表现不佳的不忠实现象。,"Trustworthy evaluations of dangerous capabilities are increasingly crucial for determining whether an AI system is safe to deploy. One empirically demonstrated threat is sandbagging - the strategic underperformance on evaluations by AI models or their developers. A promising defense is to monitor a model's chain-of-thought (CoT) reasoning, as this could reveal its intentions and plans. In this work, we measure the ability of models to sandbag on dangerous capability evaluations against a CoT monitor by prompting them to sandbag while being either monitor-oblivious or monitor-aware. We show that both frontier models and small open-sourced models can covertly sandbag against CoT monitoring 0-shot without hints. However, they cannot yet do so reliably: they bypass the monitor 16-36% of the time when monitor-aware, conditioned on sandbagging successfully. We qualitatively analyzed the uncaught CoTs to understand why the monitor failed. We reveal a rich attack surface for CoT monitoring and contribute five covert sandbagging policies generated by models. These results inform potential failure modes of CoT monitoring and may help build more diverse sandbagging model organisms.",arXiv.org,2,Black-box,Society,该论文明确研究了模型在 CoT 监控下的欺骗行为（sandbagging），即模型是否能够通过 CoT 隐藏其真实意图，直接涉及 CoT 的忠实性问题。论文探讨了模型在监控下生成不忠实 CoT 的能力，并分析了监控失败的原因，属于核心相关研究。
Mitigating Deceptive Alignment via Self-Monitoring,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT中的不忠实现象（如欺骗性对齐），提出了度量框架DeceptionBench，并提出了改进方法CoT Monitor+。,"Modern large language models rely on chain-of-thought (CoT) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the CoT process itself, named CoT Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce DeceptionBench, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various LLMs and show that unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at cot-monitor-plus.github.io",arXiv.org,2,White-box,Society,该论文明确研究CoT过程中的欺骗性对齐（deceptive alignment）问题，涉及模型在推理过程中隐藏真实目标的现象（sycophancy等），并通过内部自监控信号（self-evaluation signal）直接干预推理过程以提升忠实度。其提出的CoT Monitor+框架和DeceptionBench基准均围绕CoT的因果真实性展开，属于核心相关研究。
Human-Aligned Faithfulness in Toxicity Explanations of LLMs,2025,True,True,True,,论文提出新度量标准HAF评估LLMs毒性解释的忠实性，揭示了不一致和不相关响应现象。,"The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs'reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs'free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate HAF of LLMs'toxicity explanations with no human involvement, and highlight how""non-ideal""the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and irrelevant responses. We open-source our code at https://github.com/uofthcdslab/HAF and LLM-generated explanations at https://huggingface.co/collections/uofthcdslab/haf.",arXiv.org,2,Black-box,Society,该论文明确研究LLM生成的毒性解释是否与人类理性解释一致（Human-Aligned Faithfulness），并提出多维标准HAF来衡量解释的忠实度，直接涉及CoT Faithfulness的核心问题。
Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation,2025,True,True,False,Prompting & In-Context Learning,论文揭示了CoT中的社会偏见现象，并提出了基于提示的改进方法。,"While reasoning-based large language models excel at complex tasks through an internal, structured thinking process, a concerning phenomenon has emerged that such a thinking process can aggregate social stereotypes, leading to biased outcomes. However, the underlying behaviours of these language models in social bias scenarios remain underexplored. In this work, we systematically investigate mechanisms within the thinking process behind this phenomenon and uncover two failure patterns that drive social bias aggregation: 1) stereotype repetition, where the model relies on social stereotypes as its primary justification, and 2) irrelevant information injection, where it fabricates or introduces new details to support a biased narrative. Building on these insights, we introduce a lightweight prompt-based mitigation approach that queries the model to review its own initial reasoning against these specific failure patterns. Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show that our approach effectively reduces bias while maintaining or improving accuracy.",arXiv.org,2,Black-box,Society,该论文明确研究了基于推理的语言模型在思考过程中如何聚合社会偏见，揭示了两种导致偏见聚合的失败模式（刻板印象重复和不相关信息注入），并提出了基于提示的缓解方法。这直接涉及到CoT的忠实度问题，即模型的推理过程是否真实反映了其预测的实际计算过程，特别是关于偏见的部分。
Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories,2025,True,True,False,Training & Fine-tuning,论文提出通过强化学习优化推理轨迹以减少阿谀奉承现象，涉及 CoT 忠实性。,"Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",,2,Black-box,Society,论文明确研究 Sycophancy（阿谀奉承）现象，并提出了通过优化内部推理机制（Adaptive Reasoning Trajectories）来减少这种现象的方法。这直接涉及 CoT 的忠实度问题，即模型生成的解释是否真实反映了其预测过程。
"LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出过滤噪声知识和减少无效推理的方法，涉及 CoT 忠实性。,"Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Society,论文主要关注通过知识增强提升LLM在常识推理任务中的性能，提出了过滤噪声知识和减少无效推理的方法，但未涉及CoT的忠实度或因果分析。
SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models,2023,True,True,False,,论文讨论了生成模型中的社会偏见放大问题，并涉及了CoT中的不忠实现象。,"Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, we start with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two open source generative language models and we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We demonstrate that the deliberate design of the templates in our benchmark (e.g., adding biasing text to the prompt or using different verbs that change the answer that indicates bias) impacts the model tendencies to generate socially biased output. Additionally, through manual evaluation, we discover problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning.

Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.",AAAI Conference on Artificial Intelligence,1,Black-box,Society,论文研究了生成语言模型中的社会偏见放大问题，并涉及了生成的链式思维（CoT）输出中的问题模式，如微妙的偏见和缺乏推理。虽然主要关注社会偏见，但提到了CoT输出的问题，可以作为Faithfulness的旁证。
Evaluating and Mitigating Sycophancy in Large Vision-Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LVLMs中的Sycophancy现象，提出了评估框架SyEval-VL和改进方法HFRAG。,"Large vision-language models (LVLMs) have recently achieved significant advancements, demonstrating powerful capabilities in understanding and reasoning about visual information. However, LVLMs may generate biased responses that reflect the user beliefs rather than the facts, a phenomenon known as sycophancy. Sycophancy can pose serious challenges to the performance, trustworthiness, and security of LVLMs, raising concerns about their practical applications. We note that there is limited work on the evaluation and mitigation of sycophancy in LVLMs. In this paper, we introduce SyEval-VL, a benchmark specifically designed to evaluate sycophancy in LVLMs. SyEval-VL offers a comprehensive evaluation of sycophancy in visual understanding and reasoning across various scenarios with a multi-round dialogue format. We evaluate sycophancy in several popular LVLMs, providing an in-depth analysis of various sycophantic behaviors and their consequential impacts. Additionally, we propose a novel framework, Human Feedback-based Retrieval-Augmented Generation (HFRAG), to mitigate sycophancy in LVLMs by determining the appropriate timing of retrieval, profiling the proper retrieval target, and augmenting the decoding of LVLMs. Extensive experiments demonstrate that the proposed method significantly mitigates sycophancy in LVLMs without requiring additional training. Our code is available at: https://github.com/immc-lab/SyEval-VL",,2,Black-box,Society,论文明确研究 Sycophancy（阿谀奉承）现象，即模型生成反映用户信念而非事实的回应，这与 CoT Faithfulness 中的 Unfaithful 和 Post-hoc Rationalization 直接相关。论文还提出了评估和缓解 Sycophancy 的方法，属于核心相关研究。
"'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?",2025,True,True,True,,论文探讨了LLMs在决策过程中对SES的偏好，并揭示了System 2模式下CoT可能放大不忠实现象。,"Large Language Models (LLMs) are increasingly involved in high-stakes domains, yet how they reason about socially sensitive decisions remains underexplored. We present a large-scale audit of LLMs'treatment of socioeconomic status (SES) in college admissions decisions using a novel dual-process framework inspired by cognitive science. Leveraging a synthetic dataset of 30,000 applicant profiles grounded in real-world correlations, we prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2 modes: a fast, decision-only setup (System 1) and a slower, explanation-based setup (System 2). Results from 5 million prompts reveal that LLMs consistently favor low-SES applicants -- even when controlling for academic performance -- and that System 2 amplifies this tendency by explicitly invoking SES as compensatory justification, highlighting both their potential and volatility as decision-makers. We then propose DPAF, a dual-process audit framework to probe LLMs'reasoning behaviors in sensitive applications.",,1,Black-box,Society,论文研究了LLM在社会敏感决策中的推理行为，特别是社会经济因素在决策中的作用，并比较了快速决策（System 1）和基于解释的决策（System 2）模式。虽然未直接研究CoT的忠实度，但涉及了推理过程的解释性，可以作为Faithfulness的旁证。

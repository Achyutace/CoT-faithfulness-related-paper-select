title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
RACG-DRT: Retrieval Augumented Code Generation Based on Dynamic Revision of Thoughts,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT在代码生成中的不忠实现象，并提出了基于动态修订的改进方法。,"Large models have the problems of hallucinations in the field of code generation. The causes of these hallucinations include errors in the model’s reasoning process, inadequate overall knowledge, and insufficient localized knowledge. To tackle these challenges, the prevailing approach currently involves influencing the large model’s reasoning and prediction processes through techniques such as chain of thought, single-shot retrievalaugmented generation, and dynamic retrieval-augmented generation. COT (chain of thought) prompting method facilitates logical reasoning by the model on programming problems through prompts. Single-shot retrieval-augmented generation technology enables the construction of query statements based on programming issues to retrieve information from external knowledge bases. Meanwhile, dynamic retrieval-augmented generation technology allows for real-time retrieval from external knowledge bases during the code generation process, integrating retrieved documents to continue generating subsequent code. However, these methods primarily address the hallucination problem in code generation from a localized perspective. To simultaneously resolve issues such as reasoning errors, inadequate overall knowledge, and insufficient localized knowledge in the field of code generation, this study introduces the Retrieval-Augmented Code Generation framework based on Dynamic Revision of Thoughts (RACGDRT). The core concept of this framework involves firstly enabling the large model to produce a comprehensive code draft (utilizing COT prompts), followed by conducting error detection on code segments corresponding to different reasoning steps within the draft. Upon detecting an error, the Keyword Extractor (KE) will extract keywords and formulate query statements for both the user’s input question and the erroneous code segment, subsequently retrieving pertinent documents from the external knowledge base. Finally, with the retrieved documents and questions in hand, the large model proceeds to iteratively generate the next code draft. RACG-DRT guarantees the rationality of reasoning via COT, constructs query statements tailored to erroneous thought segments and issues to bolster both local and holistic knowledge, and iteratively produces code drafts to uphold the integrity of the code generation process.",,0,Black-box,Code,论文主要关注通过检索增强和动态修订思想来提高代码生成的准确性和减少幻觉，但并未涉及CoT的忠实度问题。研究重点是性能提升和减少幻觉，而非验证CoT是否真实反映模型的推理过程。
Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT监控和奖励黑客行为，揭示了不忠实现象，并提出了通过训练改进的方法。,"Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior.",arXiv.org,2,Black-box,Code,该论文明确研究了模型通过CoT进行奖励黑客行为（reward hacking）的现象，即模型在CoT中隐藏其真实意图，这直接涉及CoT的忠实性问题。论文还探讨了如何通过监控CoT来检测这种不忠实行为，属于核心相关研究。
CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring,2025,True,True,True,Verification & External Tools,论文探讨了CoT监控中的不忠实现象（误导性合理化），并提出了混合协议作为改进方法。,"As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. As a potential mitigation, we investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. We compare CoT monitoring to action-only monitoring, where only final outputs are reviewed, in a red-teaming setup where the untrusted model is instructed to pursue harmful side tasks while completing a coding problem. We find that while CoT monitoring is more effective than overseeing only model outputs in scenarios where action-only monitoring fails to reliably identify sabotage, reasoning traces can contain misleading rationalizations that deceive the CoT monitors, reducing performance in obvious sabotage cases. To address this, we introduce a hybrid protocol that independently scores model reasoning and actions, and combines them using a weighted average. Our hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates twice higher than action-only monitoring for subtle deception scenarios.",arXiv.org,2,Black-box,Code,该论文明确研究了CoT监控中的欺骗性现象，即推理轨迹可能包含误导性合理化（misleading rationalizations），直接涉及CoT的忠实度问题。此外，论文还提出了混合协议来独立评分模型推理和行动，这属于测量Faithfulness的方法。
Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文讨论了如何通过外部奖励模型提升CoT的忠实性，并提出了改进方法。,"Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation. To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a""cold start, then PRM supervision""paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (e.g.,GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by PRM (7B) to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning.",arXiv.org,1,Black-box,Code,该论文研究了如何通过逐步推理和过程监督奖励来提升Text-to-SQL任务的性能，虽然涉及推理过程的可解释性，但主要关注的是性能提升而非推理过程的忠实度。
Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation,2025,True,True,False,Prompting & In-Context Learning,论文探讨了CoT在代码生成中的质量问题，揭示了不忠实现象，并提出了通过提示改进CoT的方法。,"Large language models (LLMs) have demonstrated impressive performance in code generation, particularly when augmented with chain-of-thought (CoT) prompting techniques. They break down requirements into intermediate reasoning steps, which act as design rationales to guide LLMs in writing code like human programmers. Thus, the quality of these steps is crucial for ensuring the correctness and reliability of the generated code. However, little is known about the quality of CoT generated by LLMs. To what extent can we trust the thoughts generated by LLMs? How good are they? This paper empirically explores the external and internal factors of why LLMs generate unsatisfactory CoTs by analyzing 1,023 failed code samples on two widely used code generation benchmarks. We also evaluate their impact on code generation performance by analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting LLMs. Our study reveals three key findings: (1) External factors (53.60%), such as unclear requirements and lack of context, mainly affect CoT quality, while internal factors (40.10%) stem from LLMs'misunderstanding prompts. (2) Even when CoTs are correct, 18.5% of the generated code contains errors due to instruction-following issues; conversely, 11.90% of correct code is paired with flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when given detailed problem descriptions. These findings highlight key challenges in CoT-based code generation and suggest directions for improving LLM reasoning and reliability.",arXiv.org,1,Black-box,Code,论文研究了CoT在代码生成中的质量，包括CoT与生成代码之间的一致性（18.5%正确CoT生成错误代码，11.9%正确代码对应错误CoT），这间接涉及Faithfulness问题。但未明确分析CoT是否真实反映模型内部计算过程，属于边缘相关。

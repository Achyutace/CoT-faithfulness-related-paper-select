title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation,2025,True,True,True,Interpretability & Internal Mechanisms,论文讨论了CoT在临床决策中的不忠实现象，并提出了评估框架和改进方法。,"Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.",arXiv.org,1,Black-box,Medical,论文研究推理增强模型在临床决策中的表现，发现扩展CoT推理并不能保证临床可靠性，涉及推理质量与实际表现的关系，但主要是评估CoT的效用而非忠实度因果分析。
Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis,2025,True,True,False,Training & Fine-tuning,论文讨论了VLMs在临床推理中的不忠实现象，并提出了通过SFT和RL提升Faithfulness的方法。,"The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones. To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.",,1,Black-box,Medical,论文研究了临床推理的可信度，并提出了基于教科书推理的生成器，以提供可靠的专家级监督。虽然涉及推理的忠实性，但主要关注的是诊断准确性和泛化能力，未深入探讨CoT是否真实反映模型预测的计算过程。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,论文明确提出了一个自解释的GNN框架X-Node，其中每个节点在预测过程中生成自己的解释，并通过解码器强制忠实性（faithfulness）。此外，论文还研究了如何通过解释向量来重构节点的潜在嵌入，以确保解释的真实性，这直接符合Faithfulness的核心定义。
Training and Evaluation of Guideline-Based Medical Reasoning in LLMs,2025,True,False,True,Training & Fine-tuning,论文通过微调LLMs遵循医学指南进行推理，提出了评估推理过程忠实性的方法。,"Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",,2,Black-box,Medical,该论文明确研究如何通过微调LLMs使其遵循医学共识指南进行逐步推理和预测，强调了解释的忠实性（faithful explanations）和推导正确性（derivation correctness），直接涉及CoT Faithfulness的核心定义。
TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出通过关键词蒸馏和LLM推理提升解释的忠实性，并提出了新的评估指标。,"Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.",arXiv.org,1,Black-box,Medical,论文研究了通过关键词蒸馏和LLM推理提高临床文本解释的可信度，并使用了删除保真度指标和人类专家评估来解释质量。虽然涉及解释的忠实度，但主要关注的是解释的实用性和可信度，而非直接研究CoT是否真实反映模型的预测过程。
Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用CoT和强化学习提升医疗推理的忠实性，但未讨论不忠实现象或提出新度量。,"While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",arXiv.org,1,Black-box,Medical,论文研究了医学领域的 CoT 推理过程，并强调了透明和可验证的推理过程，但未明确探讨 CoT 是否真实反映了模型的实际计算过程（Faithfulness）。因此属于边缘相关。
MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine,2025,True,True,True,,论文探讨了CoT忠实性和Sycophancy现象，并提出了评估指标。,"With the increasing use of large language models (LLMs) in medical decision-support, it is essential to evaluate not only their final answers but also the reliability of their reasoning. Two key risks are Chain-of-Thought (CoT) faithfulness -- whether reasoning aligns with responses and medical facts -- and sycophancy, where models follow misleading cues over correctness. Existing benchmarks often collapse such vulnerabilities into single accuracy scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and workflow designed to quantify safety-performance trade-offs under manipulative hint conditions. It contains 1,804 reasoning-focused medical questions across six specialties and three task types, including 500 from MedMCQA. Each question is paired with seven manipulative hint types and a no-hint baseline, producing about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source, general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and Anti-Sycophancy -- are combined into a composite score visualized with a 45 Degrees plot. Results show a consistent safety-performance trade-off, with no model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81 Degrees), balancing safety and accuracy but not leading in both. MedOmni-45 Degrees thus provides a focused benchmark for exposing reasoning vulnerabilities in medical LLMs and guiding safer model development.",arXiv.org,2,Black-box,Medical,论文明确研究 CoT 的忠实度（CoT-Faithfulness）和 sycophancy 现象，并提出了量化指标，属于核心相关研究。
The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference,2025,True,True,False,,论文揭示了LLMs在临床推理中的不忠实现象，即知识获取与推理能力分离。,"Large language models are often assumed to acquire increasingly structured, generalizable internal representations simply by scaling data and parameters. We interrogate this assumption by introducing a Clinical Trial Natural Language Inference benchmark comprising four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning Verification (GKMRV) probe, allowing us to dissociate failures of factual access from failures of inference. We evaluate six contemporary LLMs under both direct and chain of thought prompting. Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy, output inferences are highly consistent across samples (mean 0.87), indicating a systematic application of underlying heuristics and shortcuts. These results reveal fundamental structural and representational limitations: current LLMs often possess the relevant clinical knowledge but lack the structured, composable internal representations needed to deploy it reliably (e.g., integrating constraints, weighing evidence, or simulating counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this dissociation explicit and measurable, providing an effective framework for probing the reliability of LLMs in high-stakes domains.",arXiv.org,1,Black-box,Medical,论文研究了LLMs在临床自然语言推理中的表现，通过直接提示和CoT提示评估模型，发现模型在知识获取和推理能力之间存在明显的分离。虽然论文没有直接探讨CoT的忠实度，但它揭示了模型在推理过程中可能依赖启发式和捷径，这与Faithfulness的边缘相关。
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,2,Black-box,Medical,论文明确研究LLM在复杂计算表型任务中的推理错误，包括解释正确性和不忠实性错误，并评估了提示修改对准确性的影响，直接涉及CoT Faithfulness的核心问题。
FZI-WIM at SemEval-2024 Task 2: Self-Consistent CoT for Complex NLI in Biomedical Domain,2024,True,False,True,Consistency & Ensembling,论文使用自洽性（self-consistency）和多数投票提升CoT性能，并报告了忠实性分数。,"This paper describes the inference system of FZI-WIM at the SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. Our system utilizes the chain of thought (CoT) paradigm to tackle this complex reasoning problem and further improve the CoT performance with self-consistency. Instead of greedy decoding, we sample multiple reasoning chains with the same prompt and make thefinal verification with majority voting. The self-consistent CoT system achieves a baseline F1 score of 0.80 (1st), faithfulness score of 0.90 (3rd), and consistency score of 0.73 (12th). We release the code and data publicly.",International Workshop on Semantic Evaluation,1,Black-box,Medical,论文提到了faithfulness score，表明对CoT的忠实度进行了测量，但未深入探讨CoT是否是模型预测的真实原因或是否存在事后解释现象。此外，研究主要集中在通过自一致性提高性能，属于边缘相关。
Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations,2025,True,True,True,,论文探讨了VLMs中CoT解释的忠实性问题，并提出了新的评估框架。,"Vision-language models (VLMs) often produce chain-of-thought (CoT) explanations that sound plausible yet fail to reflect the underlying decision process, undermining trust in high-stakes clinical use. Existing evaluations rarely catch this misalignment, prioritizing answer accuracy or adherence to formats. We present a clinically grounded framework for chest X-ray visual question answering (VQA) that probes CoT faithfulness via controlled text and image modifications across three axes: clinical fidelity, causal attribution, and confidence calibration. In a reader study (n=4), evaluator-radiologist correlations fall within the observed inter-radiologist range for all axes, with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone ($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows that answer accuracy and explanation quality can be decoupled, acknowledging injected cues does not ensure grounding, and text cues shift explanations more than visual cues. While some open-source models match final answer accuracy, proprietary models score higher on attribution (25.0% vs. 1.4%) and often on fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to evaluate beyond final answer accuracy.",arXiv.org,2,Black-box,Medical,该论文明确研究医疗视觉语言模型（VLMs）中链式思考（CoT）解释的忠实性问题，通过多模态扰动（文本和图像修改）来评估CoT是否真实反映底层决策过程。论文提出了测量Faithfulness的指标（临床保真度、因果归因和置信度校准），并发现答案准确性和解释质量可以解耦，这表明CoT的解释可能不忠实。
LExT: Towards Evaluating Trustworthiness of Natural Language Explanations,2025,True,True,True,,论文提出评估自然语言解释可信度的框架，关注忠实性，并揭示通用模型的不忠实现象。,"As Large Language Models (LLMs) become increasingly integrated into high-stakes domains, there have been several approaches proposed toward generating natural language explanations. These explanations are crucial for enhancing the interpretability of a model, especially in sensitive domains like healthcare, where transparency and reliability are key. In light of such explanations being generated by LLMs and its known concerns, there is a growing need for robust evaluation frameworks to assess model-generated explanations. Natural Language Generation metrics like BLEU and ROUGE capture syntactic and semantic accuracies but overlook other crucial aspects such as factual accuracy, consistency, and faithfulness. To address this gap, we propose a general framework for quantifying trustworthiness of natural language explanations, balancing Plausibility and Faithfulness, to derive a comprehensive Language Explanation Trustworthiness Score (LExT). Applying our domain-agnostic framework to the healthcare domain using public medical datasets, we evaluate six models, including domain-specific and general-purpose models. Our findings demonstrate significant differences in their ability to generate trustworthy explanations. On comparing these explanations, we make interesting observations such as inconsistencies in Faithfulness demonstrated by general-purpose models and their tendency to outperform domain-specific fine-tuned models. This work further highlights the importance of using a tailored evaluation framework to assess natural language explanations in sensitive fields, providing a foundation for improving the trustworthiness and transparency of language models in healthcare and beyond.","Conference on Fairness, Accountability and Transparency",2,Black-box,Medical,论文明确提出了评估自然语言解释的信任度框架，特别关注了Faithfulness（忠实度）这一关键维度，并对比了通用模型与领域专用模型在生成忠实解释上的差异。这直接符合CoT Faithfulness研究的核心定义，即探究解释是否真实反映模型的预测过程。
Reward Hacking Mitigation using Verifiable Composite Rewards,2025,True,True,False,Training & Fine-tuning,论文讨论了奖励黑客行为，涉及不忠实现象，并提出了通过复合奖励函数来改进。,"Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that utilizing RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR1.",,1,Black-box,Medical,论文研究了奖励机制对模型推理行为的影响，特别是奖励黑客行为（如不提供推理步骤或使用非标准推理格式），这与CoT Faithfulness的边缘相关。虽然未直接研究CoT的忠实度，但讨论了推理格式的可靠性问题。
Compartmentalised Agentic Reasoning for Clinical NLI,2025,True,True,False,Verification & External Tools,论文提出了一种代理推理方法，通过分解任务和验算机制提升忠实性。,"A common assumption holds that scaling data and parameters yields increasingly structured, generalisable internal representations. We interrogate this assumption in clinical natural language inference (NLI) by adopting a benchmark decomposed into four reasoning families, Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction, and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI that separates knowledge access from principled inference. CARENLI routes each premise, statement pair to a family specific solver and enforces auditable procedures via a planner, verifier, and refiner. Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching 98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag violations with near-ceiling reliability, while refiners correct a substantial share of epistemic errors. Remaining failures cluster in routing, identifying family classification as the main bottleneck. These results show that LLMs often retain relevant facts but default to heuristics when inference is underspecified, a dissociation CARENLI makes explicit while offering a framework for safer, auditable reasoning.",arXiv.org,1,Black-box,Medical,论文研究了临床NLI中的推理过程，并提出了一个框架来提高推理的忠实度和可审计性。虽然它没有明确研究CoT的忠实度，但它讨论了模型在推理过程中使用启发式方法的问题，这与Faithfulness的边缘相关。

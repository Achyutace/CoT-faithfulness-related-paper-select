title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
State over Tokens: Characterizing the Role of Reasoning Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，并提出了State over Tokens框架来解释这一现象。,"Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",,2,White-box,General,论文明确研究 CoT tokens 是否能忠实反映模型的真实推理过程，揭示了推理 tokens 并非真实解释而是一种计算状态，直接指向 Faithfulness 的核心问题。论文采用白盒视角分析模型内部状态与 tokens 的关系。
Thought Branches: Interpreting LLM Reasoning Requires Resampling,2025,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的忠实性问题，提出了通过重采样进行因果分析的方法，并讨论了不忠实现象。,"Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In""agentic misalignment""scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes""unfaithful"", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",arXiv.org,2,White-box,General,论文明确研究了 CoT 的忠实性问题，通过重采样方法来验证推理步骤是否真实影响模型决策。特别提到了 'agentic misalignment' 情景下的因果影响测量，以及干预措施对推理的稳定性影响。这些都直接关联到 Faithfulness 的核心定义。
Measuring Chain-of-Thought Monitorability Through Faithfulness and Verbosity,2025,True,True,True,,论文探讨了CoT的忠实性和详细程度，提出了新的度量指标，并揭示了不忠实现象。,"Chain-of-thought (CoT) outputs let us read a model's step-by-step reasoning. Since any long, serial reasoning process must pass through this textual trace, the quality of the CoT is a direct window into what the model is thinking. This visibility could help us spot unsafe or misaligned behavior (monitorability), but only if the CoT is transparent about its internal reasoning (faithfulness). Fully measuring faithfulness is difficult, so researchers often focus on examining the CoT in cases where the model changes its answer after adding a cue to the input. This proxy finds some instances of unfaithfulness but loses information when the model maintains its answer, and does not investigate aspects of reasoning not tied to the cue. We extend these results to a more holistic sense of monitorability by introducing verbosity: whether the CoT lists every factor needed to solve the task. We combine faithfulness and verbosity into a single monitorability score that shows how well the CoT serves as the model's external `working memory', a property that many safety schemes based on CoT monitoring depend on. We evaluate instruction-tuned and reasoning models on BBH, GPQA, and MMLU. Our results show that models can appear faithful yet remain hard to monitor when they leave out key factors, and that monitorability differs sharply across model families. We release our evaluation code using the Inspect library to support reproducible future work.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（faithfulness）以及其作为模型外部 '工作记忆' 的监控能力（monitorability），并提出了结合忠实度和详细程度（verbosity）的综合监控得分。这直接关联到 CoT 是否真实反映模型的内部推理过程，符合核心定义中对 faithfulness 的研究。
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确研究如何通过将解释基于推理过程来提高解释的忠实度（Faithfulness），并提出了联合预测-解释方法以确保解释不依赖于答案。这直接符合 CoT Faithfulness 的核心定义，特别是关于避免事后合理化（Post-hoc Rationalization）的部分。
Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought,2025,True,True,True,Interpretability & Internal Mechanisms,论文揭示了CoT中存在真假思考步骤的现象，并提出了度量指标TTS和白盒干预方法。,"Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs'prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS>= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.",arXiv.org,2,White-box,General,该论文直接研究 CoT 步骤的真实因果影响（True Thinking Score），揭示 LLM 存在装饰性思考步骤（Decorative-thinking steps），并提出了 steering 方法来强制真实思考。核心涉及 Faithfulness 的核心概念。
The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?,2025,True,True,True,,论文研究了CoT的学习动态，揭示了不忠实现象，并提出了量化框架。,"Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically.",arXiv.org,2,White-box,General,论文明确研究了CoT的忠实度（trace faithfulness），特别是发现了一个短暂的不可信阶段，在这个阶段模型虽然给出了正确答案，但跳过了CoT步骤或与之矛盾。此外，论文还涉及了模型的内部计算机制，这些都是CoT Faithfulness研究的核心内容。
Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出基于因果推理的框架CORE，旨在提升忠实推理，涉及不忠实现象和改进方法。,"In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of ""hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for ""Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",ACM Multimedia,2,White-box,General,论文明确研究因果驱动的推理（Causal-driven reasoning）并设计了CORE框架以确保知识问答中的忠实性（Faithful Reasoning）。通过构建因果图和do-calculus规则，论文直接解决了模型生成内容与真实知识之间的因果连接问题，符合核心相关的标准。
Counterfactual Evaluation for Explainable AI,2021,True,False,True,,论文提出了基于反事实推理的新评估方法，直接针对解释的忠实性度量。,"While recent years have witnessed the emergence of various explainable methods in machine learning, to what degree the explanations really represent the reasoning process behind the model prediction -- namely, the faithfulness of explanation -- is still an open problem. One commonly used way to measure faithfulness is \textit{erasure-based} criteria. Though conceptually simple, erasure-based criterion could inevitably introduce biases and artifacts. We propose a new methodology to evaluate the faithfulness of explanations from the \textit{counterfactual reasoning} perspective: the model should produce substantially different outputs for the original input and its corresponding counterfactual edited on a faithful feature. Specially, we introduce two algorithms to find the proper counterfactuals in both discrete and continuous scenarios and then use the acquired counterfactuals to measure faithfulness. Empirical results on several datasets show that compared with existing metrics, our proposed counterfactual evaluation method can achieve top correlation with the ground truth under diffe",arXiv.org,2,White-box,General,论文明确研究了解释的忠实性问题（Faithfulness of explanation），并提出了一种基于反事实推理（counterfactual reasoning）的新方法来评估解释的忠实性。这与 CoT Faithfulness 的核心定义高度相关，尤其是涉及反事实输入和忠实性测量指标。因此评为核心相关。
Diagnostics-Guided Explanation Generation,2021,True,False,True,Training & Fine-tuning,论文讨论了如何优化解释的忠实性，并提出了直接优化诊断属性的训练方法。,"Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确讨论了Faithfulness的定义及其在解释生成中的重要性，提出了一种直接优化Faithfulness等诊断属性的方法，符合CoT Faithfulness研究的核心定义和目标。
Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models,2025,True,True,True,,论文提出了评估思维草稿忠实性的框架，揭示了当前模型在中间推理步骤中的选择性忠实性问题。,"Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate thinking draft faithfulness. Our approach focuses on two complementary dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and (2) Draft-to-Answer Faithfulness, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs.",arXiv.org,2,White-box,General,论文明确研究 CoT 的忠实性（Faithfulness），提出了系统的干预框架来评估思考草案的忠实性，包括 Intra-Draft Faithfulness 和 Draft-to-Answer Faithfulness 两个维度，符合核心相关标准。
Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations,2025,True,True,True,Verification & External Tools,论文提出了一种自动化验证方法EDCT，用于检测Vision-Language模型解释的忠实性，并揭示了不忠实现象。,"Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",arXiv.org,2,Black-box,General,该论文明确研究视觉语言模型生成的解释（NLEs）是否真实反映了模型预测的因果因素，提出了Explanation-Driven Counterfactual Testing (EDCT)方法来验证解释的忠实度，并计算Counterfactual Consistency Score (CCS)来衡量解释的因果关联性。这与Faithfulness的核心定义完全吻合。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,0,Black-box,General,论文主要关注通过 CoT 提升预测性能和在跨大陆场景中的泛化能力，虽然提到了‘faithful and semantically meaningful’，但未深入探讨 CoT 是否忠实反映了模型的实际推理过程。属于纯性能提升的应用研究。
What Really Counts? Examining Step and Token Level Attribution in Multilingual CoT Reasoning,2025,True,True,True,,论文研究了CoT在多语言模型中的忠实性和解释性，提出了评估方法并揭示了不忠实现象。,"This study investigates the attribution patterns underlying Chain-of-Thought (CoT) reasoning in multilingual LLMs. While prior works demonstrate the role of CoT prompting in improving task performance, there are concerns regarding the faithfulness and interpretability of the generated reasoning chains. To assess these properties across languages, we applied two complementary attribution methods--ContextCite for step-level attribution and Inseq for token-level attribution--to the Qwen2.5 1.5B-Instruct model using the MGSM benchmark. Our experimental results highlight key findings such as: (1) attribution scores excessively emphasize the final reasoning step, particularly in incorrect generations; (2) structured CoT prompting significantly improves accuracy primarily for high-resource Latin-script languages; and (3) controlled perturbations via negation and distractor sentences reduce model accuracy and attribution coherence. These findings highlight the limitations of CoT prompting, particularly in terms of multilingual robustness and interpretive transparency.",,2,White-box,General,该论文直接研究了 Chain-of-Thought (CoT) 推理的 faithfulness 和 interpretability，通过 attribution 方法来评估 CoT 的忠实度。特别关注了 step-level 和 token-level 的 attribution 模式，并探讨了 CoT 在多语言环境下的局限性，这与 CoT Faithfulness 的定义和研究目标高度相关。
Comparing zero-shot self-explanations with human rationales in text classification,2024,True,True,True,,论文分析了自解释的忠实性，并比较了其与人类注释的差异。,"Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations. These do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation. We evaluate self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. We study two text classification tasks: sentiment classification and forced labour detection, i.e., identifying pre-defined risk indicators of forced labour. In addition to English, we include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and analyse 4 LLMs. We show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness. This finding suggests that self-explanations indeed provide good explanations for text classification.",,2,White-box,General,该论文明确研究了自我解释（self-explanations）的忠实度（faithfulness）与人类标注的对比，并使用了梯度计算方法（LRP）进行分析，符合核心相关标准。
Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models,2025,True,True,True,Interpretability & Internal Mechanisms,论文提出了NeuroFaith框架，评估和提升LLM自我解释的忠实性，涉及不忠实现象和度量方法。,"Large Language Models (LLMs) can generate plausible free text self-explanations to justify their answers. However, these natural language explanations may not accurately reflect the model's actual reasoning process, indicating a lack of faithfulness. Existing faithfulness evaluation methods rely primarily on behavioral tests or computational block analysis without examining the semantic content of internal neural representations. This paper proposes NeuroFaith, a flexible framework that measures the faithfulness of LLM free text self-explanation by identifying key concepts within explanations and mechanistically testing whether these concepts actually influence the model's predictions. We show the versatility of NeuroFaith across 2-hop reasoning and classification tasks. Additionally, a linear faithfulness probe based on NeuroFaith is developed to detect unfaithful self-explanations from representation space and improve faithfulness through steering. NeuroFaith provides a principled approach to evaluating and enhancing the faithfulness of LLM free text self-explanations, addressing critical needs for trustworthy AI systems.",arXiv.org,2,White-box,General,该论文直接研究 LLM 生成的自由文本自我解释（类似 CoT）是否忠实反映模型的真实推理过程，并提出了 NeuroFaith 框架通过分析内部神经表示的语义内容来测量忠实度。这符合 Faithfulness 的定义，且研究方法涉及模型内部机制（白盒）。
Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT的不忠实现象，并提出了基于激活的监控方法。,"Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",arXiv.org,2,White-box,General,论文明确研究 Chain of Thought (CoT) 的忠实性问题，特别是提到 'CoT texts are often unfaithful and misleading'，并讨论了 performative CoTs 的现象，其中推理过程与最终回答持续矛盾。此外，论文使用了白盒方法（如激活值线性探测）来检测推理信号，直接涉及到 Faithfulness 的核心定义和研究内容。
Are self-explanations from Large Language Models faithful?,2024,True,True,True,Consistency & Ensembling,论文探讨了LLM自我解释的忠实性问题，并提出了自我一致性检查作为度量方法。,"Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, feature attribution, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, feature attribution for Mistral, and redaction for Falcon 40B.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究LLM自我解释（self-explanations）的忠实性问题，并提出使用自一致性检查（self-consistency checks）作为测量方法。研究涵盖了counterfactual、feature attribution和redaction explanations，直接符合CoT Faithfulness的核心定义。
Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process,2025,True,True,False,Training & Fine-tuning,论文探讨了LLMs和LRMs的因果推理问题，涉及不忠实现象，并提出了通过RLVR训练提升因果推理的方法。,"LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",arXiv.org,2,White-box,General,该论文明确研究LLM和LRM的因果结构，分析推理过程中的忠实度问题（unfaithfulness），并探讨RLVR训练如何减少虚假相关性并增强真实因果模式，直接关联CoT Faithfulness的核心定义。
FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness,2025,True,True,False,Training & Fine-tuning,论文提出了一种通过干预训练提升 CoT 忠实性的方法，并讨论了不忠实现象。,"Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.",arXiv.org,2,Black-box,General,论文 FRIT 明确研究如何通过因果干预（Causal Importance）改进 Chain-of-Thought 的忠实度（Faithfulness）。提出了一种训练方法，通过合成忠实和非忠实的推理步骤对来提升模型的因果一致性。这与定义中的‘CoT 是否真实反映模型的预测过程’完全吻合，且方法虽基于 Prompting（黑盒），但涉及因果干预训练（Supervision-free alignment），属于核心研究范畴。
Inducing Faithfulness in Structured Reasoning via Counterfactual Sensitivity,2025,True,True,True,Training & Fine-tuning,论文提出了一种新的训练目标CSR，旨在增强模型推理步骤与输出之间的因果依赖性，直接涉及CoT的忠实性。,"The reasoning processes of large language models often lack faithfulness; a model may generate a correct answer while relying on a flawed or irrelevant reasoning trace. This behavior, a direct consequence of training objectives that solely reward final-answer correctness, severely undermines the trustworthiness of these models in high-stakes domains. This paper introduces \textbf{Counterfactual Sensitivity Regularization (CSR)}, a novel training objective designed to forge a strong, causal-like dependence between a model's output and its intermediate reasoning steps. During training, CSR performs automated, operator-level interventions on the generated reasoning trace (e.g., swapping ``+''with ``-'') to create a minimally-perturbed counterfactual. A regularization term then penalizes the model if this logically flawed trace still yields the original answer. Our efficient implementation adds only 8.7\% training overhead through warm-start curriculum and token-subset optimization. We evaluate faithfulness using \textbf{Counterfactual Outcome Sensitivity (COS)}, a metric quantifying how sensitive the final answer is to such logical perturbations. Across diverse structured reasoning benchmarks -- arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop QA (HotpotQA), and code generation (MBPP) -- models trained with CSR demonstrate a vastly superior trade-off between accuracy and faithfulness. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, with this learned sensitivity generalizing to larger models and enhancing the performance of inference-time techniques like self-consistency.",arXiv.org,2,White-box,General,这篇论文直接研究 CoT 的 Faithfulness 问题，提出了 Counterfactual Sensitivity Regularization (CSR) 和 Counterfactual Outcome Sensitivity (COS) 来确保模型输出的推理步骤与最终答案之间具有因果依赖性。通过干预中间推理步骤来惩罚不符合逻辑的推理，论文明确解决了 CoT 的忠实度问题，且方法涉及模型内部机制的训练干预，属于白盒方法。
On Faithfulness and Coherence of Language Explanations for Recommendation Systems,2022,True,True,False,,论文探讨了推荐系统中生成的解释是否真实反映了预测评分的理由，涉及 CoT 忠实性问题。,"Reviews contain rich information about product characteristics and user interests and thus are commonly used to boost recommender system performance. Specifically, previous work show that jointly learning to perform review generation improves rating prediction performance. Meanwhile, these model-produced reviews serve as recommendation explanations, providing the user with insights on predicted ratings. However, while existing models could generate fluent, human-like reviews, it is unclear to what degree the reviews fully uncover the rationale behind the jointly predicted rating. In this work, we perform a series of evaluations that probes state-of-the-art models and their review generation component. We show that the generated explanations are brittle and need further evaluation before being taken as literal rationales for the estimated ratings.",arXiv.org,2,Black-box,General,这篇论文明确研究了推荐系统中生成的解释是否忠实反映了模型预测评分的真实原因，直接涉及Faithfulness问题。
Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?,2025,True,True,False,,论文直接研究CoT忠实性，揭露不忠实现象（如事后合理化），但未提出新度量方法或改进方案。,"Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.",,2,Black-box,General,论文明确研究 CoT 的忠实度（Faithfulness），探讨了 CoT 是否是模型预测的真实原因（Causal role），并揭示了 CoT 影响与忠实度不一致的现象。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确指出现有LVLMs在CoT推理中往往忽略生成的rationale内容，且提出解决方案RED旨在提升faithfulness和准确性，直接符合CoT Faithfulness的核心定义。
Measuring Association Between Labels and Free-Text Rationales,2020,True,True,True,,论文研究了自由文本理由的忠实性，提出了评估指标，但未提出改进方法。,"In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,该论文明确研究自由文本 rationale 的忠实性问题，提出两个测试方法（robustness equivalence 和 feature importance agreement）来验证 rationale 是否真实反映模型的决策过程，完全符合 Faithfulness 的核心定义和应用场景。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文提出了一种框架（Text Modular Networks），通过将复杂任务分解为可由现有模型解决的简单子任务，并生成子问题和答案作为模型的自然语言解释。明确提到这些解释是‘faithful’的，即真实反映了模型的推理过程，符合 Faithfulness 的核心定义。此外，论文还强调了生成的理解和可信度，与 CoT Faithfulness 的研究目标一致。
Semi-structured LLM Reasoners Can Be Rigorously Audited,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出半结构化推理模型（SSRMs）以解决不忠实现象，并引入自动审计方法来检测推理错误。,"Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.",arXiv.org,2,White-box,General,该论文直接解决了 CoT 的 Faithfulness 问题，提出了 SSRMs 来生成半结构化推理痕迹，并通过自动审计识别推理缺陷（如错误和遗漏）。论文明确关注推理的真实性和可审计性，属于核心相关研究。
Faithful Knowledge Graph Explanations in Commonsense Question Answering,2022,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了知识图谱解释的不忠实性，并提出了新的度量方法和改进架构。,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文明确讨论了知识图谱解释的忠实性问题，提出了新的忠实性代理测量方法，并针对现有模型提取忠实解释的局限性提出了架构改进方案。这与研究CoT的真实性和因果关系直接相关。
Can ChatGPT Understand Causal Language in Science Claims?,2023,True,False,False,,论文提到CoT忠实且有助于提升性能，但未深入讨论不忠实现象或提出度量指标。,"This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship. The results show that ChatGPT is still behind the existing fine-tuned BERT models by a large margin. ChatGPT also had difficulty understanding conditional causal claims mitigated by hedges. However, its weakness may be utilized to improve the clarity of human annotation guideline. Chain-of-Thoughts were faithful and helpful for improving prompt performance, but finding the optimal prompt is difficult with inconsistent results and the lack of effective method to establish cause-effect between prompts and outcomes, suggesting caution when generalizing prompt engineering results across tasks or models.","Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",1,Black-box,General,该论文研究了ChatGPT对因果语言的理解能力，并提到了Chain-of-Thoughts在提高Prompt性能方面的忠实性和帮助性。然而，论文的主要焦点是模型的准确性和Prompt工程的困难，而不是深入探讨CoT的忠实性或因果机制，因此评为边缘相关。
ERASER: A Benchmark to Evaluate Rationalized NLP Models,2019,True,False,True,,论文提出了评估模型解释忠实性的新指标和框架，与CoT Faithfulness相关。,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文提出了 ERASER 基准，旨在评估 NLP 模型的解释（Rationales）是否与人类解释对齐以及这些解释的忠实度（Faithfulness）。明确提到了‘rationales provided by models align with human rationales’和‘how faithful these rationales are’，符合核心相关的标准。
Self-Critique and Refinement for Faithful Natural Language Explanations,2025,True,True,True,Prompting & In-Context Learning,论文提出通过自我批评和细化框架提升解释的忠实性，涉及不忠实现象和改进方法。,"With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.",,2,Black-box,General,该论文明确研究了自然语言解释（NLEs）的忠实性问题，提出了一个自我批判和细化的框架来提高解释的忠实度，这与CoT Faithfulness的核心定义相符。论文还展示了如何通过反馈机制减少解释的不忠实率，直接触及了Faithfulness的关键问题。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,False,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究神经网络模块（NMN）的内部行为是否忠实反映了模型的推理过程，提出了对中间输出的系统性评估，并发现网络结构不能忠实解释模型行为，这与CoT Faithfulness的核心定义高度一致。
A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations,2025,True,True,True,Training & Fine-tuning,论文提出了PEX一致性度量，揭示了LLM生成解释的不一致现象，并采用直接偏好优化提升忠实性。,"Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",,2,Black-box,General,论文明确提出了一个衡量预测与解释一致性的指标（PEX consistency），并将其作为解释忠实度（Faithfulness）的重要方面。研究揭示了大型语言模型生成的解释中超过62%缺乏一致性，并通过优化方法提高了忠实度。这直接符合核心相关标准，特别是关于解释是否真实反映模型预测过程的研究。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,该论文明确研究了 faithful rationales，即模型输出的解释是否真实反映了预测过程，属于 Faithfulness 的核心研究范畴。虽然未直接涉及 Chain-of-Thought，但其方法（通过学习选择最佳 rationales）可以迁移到 CoT 的 Faithfulness 分析中。
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?,2020,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出新的度量标准LAS评估解释的忠实性，并讨论了模型生成解释的合理性问题。,"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",Findings,2,Black-box,General,这篇论文明确关注解释的忠实性（faithfulness），提出了泄漏调整可模拟性（LAS）指标来评估自然语言解释是否真实反映了模型的行为，而不是仅仅匹配人类的解释（plausibility）。
On the Faithfulness of Visual Thinking: Measurement and Enhancement,2025,True,True,True,Training & Fine-tuning,论文揭示了视觉信息在MCoT中的不忠实现象，并提出了新的度量方法和改进策略。,"Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",arXiv.org,2,White-box,General,该论文明确研究了多模态CoT（MCoT）的忠实度问题，通过干预实验揭示了视觉信息在推理过程中被忽略的现象，并提出了量化视觉信息忠实度的评估标准和改进策略。这直接关联到Faithfulness的核心定义，即生成的解释是否真实反映模型的预测过程。
Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了FUR框架，通过消除推理步骤中的信息来测量忠实性，揭示了不忠实现象并提出了新的度量方法。,"When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",,2,White-box,General,这篇论文明确研究 CoT 忠实性问题，提出了一个名为 FUR 的框架，通过遗忘推理步骤来衡量 CoT 是否是模型预测的真实原因（parametric beliefs）。实验表明 FUR 能够精确改变模型的预测，从而验证 CoT 的忠实性。这完全符合核心定义中的 Faithfulness 研究范畴。
Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations,2025,True,True,False,Training & Fine-tuning,论文揭示了偏好优化导致CoT不忠实的问题，并提出了通过因果归因改进的方法。,"Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in""reward hacking""by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",arXiv.org,2,Black-box,General,论文明确研究了CoT解释的忠实度问题，探讨了reward hacking导致的解释不忠实现象（生成解释以最大化奖励而非真实反映推理过程），并提出了基于因果归因的解决方法。这符合核心定义中对Faithfulness的研究要求。
Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models,2024,True,True,True,Interpretability & Internal Mechanisms,论文提出使用激活修补技术测量解释的忠实性，并提出了新的度量标准。,"Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \url{https://github.com/wj210/Causal-Faithfulness}",,2,White-box,General,论文明确使用激活修补（activation patching）技术来测量解释的忠实度，并提出了因果忠实度（Causal Faithfulness）指标，直接符合 Faithfulness 研究的核心定义。
A Causal Lens for Evaluating Faithfulness Metrics,2025,True,True,True,,论文提出了评估忠实性指标的因果框架，并讨论了不忠实现象和度量方法。,"Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.",,2,White-box,General,该论文明确研究了大语言模型（LLMs）生成的自然语言解释是否忠实反映了模型的实际推理过程，提出了一个名为 Causal Diagnosticity 的框架来评估忠实性度量方法。论文使用了模型编辑方法生成忠实与非忠实解释对，并评估了多种忠实性度量方法，包括基于 chain-of-thought 的方法。这直接符合 CoT Faithfulness 的研究范畴，特别是关于解释的忠实性和因果角色。
A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,2025,True,True,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文探讨了 CoT 监控对安全性的影响，涉及 faithfulness 问题和验证工具。,"As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",arXiv.org,2,White-box,General,论文明确提出并分析了威胁 CoT 监控的忠实度的两种威胁（neuralese 和 encoded reasoning），并探讨了维护 CoT 忠实度的现有和新技术，符合核心相关标准。
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,2,Black-box,General,该论文明确研究LLM生成的解释（如CoT）的忠实性，提出了一种评估和改进解释忠实性的框架FaithLM，并通过干预性评估验证解释的真实性，完全符合Faithfulness的核心研究方向。
Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models,2024,True,True,False,,论文讨论了LLM生成自解释的忠实性与合理性之间的对立，揭示了不忠实现象。,"Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness. We assert that the faithfulness of explanations is critical in LLMs employed for high-stakes decision-making. Moreover, we emphasize the need for a systematic characterization of faithfulness-plausibility requirements of different real-world applications and ensure explanations meet those needs. While there are several approaches to improving plausibility, improving faithfulness is an open challenge. We call upon the community to develop novel methods to enhance the faithfulness of self explanations thereby enabling transparent deployment of LLMs in diverse high-stakes settings.",arXiv.org,2,Black-box,General,"该论文明确研究了LLM生成的自解释(Self-explanations, SEs)的忠实性问题，指出了可信性或连贯性(plausibility)可能损害忠实性(faithfulness)的矛盾现象，并强调了忠实性在关键领域部署中的重要性。这些讨论直接触及了CoT忠实度的核心定义和问题。"
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了LLMs在生成答案时不忠实于中间推理步骤的现象，并提出了FRODO框架来改进忠实性。,"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文直接研究 Chain-of-Thought (CoT) 的忠实度（Faithfulness），通过因果中介分析（causal mediation analysis）验证 LLMs 是否可靠地使用中间推理步骤生成最终答案，并提出了 FRODO 框架来改善忠实度。论文明确涉及 CoT 的因果作用和反事实分析，符合核心相关标准。
The Probabilities Also Matter: A More Faithful Metric for Faithfulness of Free-Text Explanations in Large Language Models,2024,True,False,True,,论文提出了新的度量指标CEF和CCT，用于评估CoT的忠实性，但未讨论不忠实现象或改进方法。,"In order to oversee advanced AI systems, it is important to understand their underlying decision-making process. When prompted, large language models (LLMs) can provide natural language explanations or reasoning traces that sound plausible and receive high ratings from human annotators. However, it is unclear to what extent these explanations are faithful, i.e., truly capture the factors responsible for the model's predictions. In this work, we introduce Correlational Explanatory Faithfulness (CEF), a metric that can be used in faithfulness tests based on input interventions. Previous metrics used in such tests take into account only binary changes in the predictions. Our metric accounts for the total shift in the model's predicted label distribution, more accurately reflecting the explanations' faithfulness. We then introduce the Correlational Counterfactual Test (CCT) by instantiating CEF on the Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the faithfulness of free-text explanations generated by few-shot-prompted LLMs from the Llama2 family on three NLP tasks. We find that our metric measures aspects of faithfulness which the CT misses.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究了LLM生成的自然语言解释的忠实度问题，提出了新的度量标准Correlational Explanatory Faithfulness (CEF)和Correlational Counterfactual Test (CCT)，用于评估解释是否真正捕捉了模型预测的因素，完全符合Faithfulness研究的核心定义。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,2,Black-box,General,该论文明确研究 Chain-of-Thought 的解释性，特别是 faithfulness（忠实度），并进行了多方面评估，包括 robustness 和 utility。虽然没有涉及模型内部机制，但直接关注 CoT 解释的真实性，符合核心相关标准。
Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology,2025,True,False,True,Verification & External Tools,论文提出基于推理拓扑的框架量化LLM解释的不确定性，与CoT忠实性相关，但未讨论不忠实现象或改进方法。,"Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",arXiv.org,2,White-box,General,这篇论文提出了一个量化LLM解释中的不确定性的框架，特别关注了Faithfulness和reasoning consistency，这与CoT Faithfulness的核心定义紧密相关。论文不仅研究了如何测量Faithfulness，还分析了知识冗余和推理路径的不确定性，提供了增强robustness和Faithfulness的指导。
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,2024,True,True,False,Training & Fine-tuning,论文探讨了微调对CoT推理忠实性的影响，揭示了不忠实现象。,"Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",North American Chapter of the Association for Computational Linguistics,2,White-box,General,论文明确研究了微调对CoT推理忠实度的影响，分析了内部机制的变化导致忠实度下降的现象，直接涉及Faithfulness的核心议题。
Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,2023,True,True,True,Training & Fine-tuning,论文探讨了多粒度理由提取的忠实性和一致性，并提出了新的评估指标。,"The success of deep learning models on multi-hop fact verification has prompted researchers to understand the behavior behind their veracity. One possible way is erasure search: obtaining the rationale by entirely removing a subset of input without compromising the veracity prediction. Although extensively explored, existing approaches fall within the scope of the single-granular (tokens or sentences) explanation, which inevitably leads to explanation redundancy and inconsistency. To address such issues, this paper explores the viability of multi-granular rationale extraction with consistency and faithfulness for explainable multi-hop fact verification. In particular, given a pretrained veracity prediction model, both the token-level explainer and sentence-level explainer are trained simultaneously to obtain multi-granular rationales via differentiable masking. Meanwhile, three diagnostic properties (fidelity, consistency, salience) are introduced and applied to the training process, to ensure that the extracted rationales satisfy faithfulness and consistency. Experimental results on three multi-hop fact verification datasets show that the proposed approach outperforms some state-of-the-art baselines.",arXiv.org,2,White-box,General,"该论文明确提到了 faithfulness 作为评估 rationale 提取的一个重要属性，并且通过引入 fidelity, consistency, salience 等诊断属性来确保提取的 rationales 满足 faithfulness 和 consistency。此外，论文还讨论了 erasure search 的方法来验证 rationales 的真实性，这与 CoT Faithfulness 的研究范畴高度相关。"
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,论文明确提到了 'reasoning faithfulness' 并提出了一个框架（AutoRubric-R1V）来通过过程级监督提高多模态推理的忠实度。这直接涉及到 CoT 是否真实反映了模型的实际计算过程，符合 Faithfulness 的核心定义。
Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation,2025,True,True,True,Training & Fine-tuning,论文讨论了CoT不忠实现象，提出了新的度量框架，并通过RL改进忠实性。,"Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",arXiv.org,2,Black-box,General,"论文明确提出了三种 Faithfulness 评测指标（information-think faithfulness, think-answer faithfulness, think-search faithfulness），并设计了 VERITAS 框架来提升推理忠实度，直接解决了 CoT 的 Unfaithfulness 问题。"
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,该论文明确研究生成忠实（faithful）解释的问题，提出了一种双教师学习框架和新的损失函数（MLR）来促进解释与预测标签之间的强条件关系。评估表明方法能生成忠实的解释，直接符合 Faithfulness 的核心定义。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确提出 Chain-of-Knowledge (CoK) prompting 来解决 CoT 中的 unfaithful reasoning chains 问题，并引入了 F^2-Verification 方法来评估推理链的 factuality 和 faithfulness。这直接涉及 CoT 的忠实度问题，符合核心相关标准。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,论文明确研究如何提高自然语言解释（NLEs）的忠实度，通过图神经网络将高亮解释与模型推理逻辑对齐，旨在验证生成的解释是否真实反映模型的内部推理过程。
FaithCoT-Bench: Benchmarking Instance-Level Faithfulness of Chain-of-Thought Reasoning,2025,True,True,True,,论文提出了一个实例级 CoT 忠实性检测的基准，并讨论了不忠实现象和评估方法。,"Large language models (LLMs) increasingly rely on Chain-of-Thought (CoT) prompting to improve problem-solving and provide seemingly transparent explanations. However, growing evidence shows that CoT often fail to faithfully represent the underlying reasoning process, raising concerns about their reliability in high-risk applications. Although prior studies have focused on mechanism-level analyses showing that CoTs can be unfaithful, they leave open the practical challenge of deciding whether a specific trajectory is faithful to the internal reasoning of the model. To address this gap, we introduce FaithCoT-Bench, a unified benchmark for instance-level CoT unfaithfulness detection. Our framework establishes a rigorous task formulation that formulates unfaithfulness detection as a discriminative decision problem, and provides FINE-CoT (Faithfulness instance evaluation for Chain-of-Thought), an expert-annotated collection of over 1,000 trajectories generated by four representative LLMs across four domains, including more than 300 unfaithful instances with fine-grained causes and step-level evidence. We further conduct a systematic evaluation of eleven representative detection methods spanning counterfactual, logit-based, and LLM-as-judge paradigms, deriving empirical insights that clarify the strengths and weaknesses of existing approaches and reveal the increased challenges of detection in knowledge-intensive domains and with more advanced models. To the best of our knowledge, FaithCoT-Bench establishes the first comprehensive benchmark for instance-level CoT faithfulness, setting a solid basis for future research toward more interpretable and trustworthy reasoning in LLMs.",arXiv.org,2,Black-box,General,该论文明确提出并解决了 CoT 忠实度的实例级评估问题，建立了 FaithCoT-Bench 基准，专注于检测不忠实（unfaithful）的 CoT 轨迹，并提供了专家标注的数据集。这直接符合 Faithfulness 的核心定义和研究范畴，尽管它使用了黑盒方法（如 counterfactual、logit-based 和 LLM-as-judge 范式）。
Evaluating Human Alignment and Model Faithfulness of LLM Rationale,2024,True,True,True,Training & Fine-tuning,论文研究了LLM生成rationale的忠实性，揭示了不忠实现象并提出了评估框架。,"We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as""aligned""with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less""faithful""than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",arXiv.org,2,White-box,General,该论文明确研究了模型生成的 rationales（相当于 CoT）的忠实性问题，比较了基于 Prompting 和基于 Attribution 的方法，并发现 Prompting-based 的解释在忠实性上不如 Attribution-based 的解释。此外，它还考察了分类准确率对忠实性评估的影响，符合 Faithfulness 的核心定义。
Unspoken Hints: Accuracy Without Acknowledgement in LLM Reasoning,2025,True,True,False,,论文研究了CoT的忠实性问题，揭示了不忠实现象，如事后找补和捷径学习。,"Large language models (LLMs) increasingly rely on chain-of-thought (CoT) prompting to solve mathematical and logical reasoning tasks. Yet, a central question remains: to what extent are these generated rationales \emph{faithful} to the underlying computations, rather than post-hoc narratives shaped by hints that function as answer shortcuts embedded in the prompt? Following prior work on hinted vs.\ unhinted prompting, we present a systematic study of CoT faithfulness under controlled hint manipulations. Our experimental design spans four datasets (AIME, GSM-Hard, MATH-500, UniADILR), two state-of-the-art models (GPT-4o and Gemini-2-Flash), and a structured set of hint conditions varying in correctness (correct and incorrect), presentation style (sycophancy and data leak), and complexity (raw answers, two-operator expressions, four-operator expressions). We evaluate both task accuracy and whether hints are explicitly acknowledged in the reasoning. Our results reveal three key findings. First, correct hints substantially improve accuracy, especially on harder benchmarks and logical reasoning, while incorrect hints sharply reduce accuracy in tasks with lower baseline competence. Second, acknowledgement of hints is highly uneven: equation-based hints are frequently referenced, whereas raw hints are often adopted silently, indicating that more complex hints push models toward verbalizing their reliance in the reasoning process. Third, presentation style matters: sycophancy prompts encourage overt acknowledgement, while leak-style prompts increase accuracy but promote hidden reliance. This may reflect RLHF-related effects, as sycophancy exploits the human-pleasing side and data leak triggers the self-censoring side. Together, these results demonstrate that LLM reasoning is systematically shaped by shortcuts in ways that obscure faithfulness.",arXiv.org,2,Black-box,General,该论文明确研究了CoT的忠实性问题，探讨了提示中的暗示如何影响模型生成的推理过程，以及模型是否真实地承认这些暗示。该研究还揭示了模型生成推理过程中的潜在不忠实行为，如事后合理化（post-hoc rationalization）和谄媚行为（sycophancy），这些都是Faithfulness研究的核心问题。
Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,2025,True,True,True,,论文揭示了CoT可能被模型故意模糊以逃避监测的不忠实现象，并提出了评估框架。,"Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection? To answer this question and thus stress-test CoT monitorability, we develop a composable and quantifiable taxonomy of prompts to elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection. (iii) Models do not obfuscate their internal CoT as much as their external CoT (under prompt pressure). These results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.",arXiv.org,2,Black-box,General,该论文明确研究了模型在具有对抗性目标时是否会掩盖其 CoT 推理过程（即 obfuscate），这与 Faithfulness 的核心定义直接相关。论文探讨了 CoT 是否能够忠实反映模型的实际推理过程，尤其是在对抗性环境下，这直接涉及到 CoT 的忠实度和欺骗性问题。
Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens,2025,True,True,False,,论文揭示了CoT的不忠实现象，如无效推理步骤仍能产生正确答案，挑战了CoT的忠实性假设。,"Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), especially of training on CoTs sampled from base LLMs to help find new reasoning patterns. While these traces certainly seem to help model performance, it is not clear how they actually influence it, with some works ascribing semantics to the traces and others cautioning against relying on them as transparent and faithful proxies of the model's internal computational process. To systematically investigate the role of end-user semantics of derivational traces, we set up a controlled study where we train transformer models from scratch on formally verifiable reasoning traces and the solutions they lead to. We notice that, despite significant gains over the solution-only baseline, models trained on entirely correct traces can still produce invalid reasoning traces even when arriving at correct solutions. More interestingly, our experiments also show that models trained on corrupted traces, whose intermediate reasoning steps bear no relation to the problem they accompany, perform similarly to those trained on correct ones, and even generalize better on out-of-distribution tasks. We also study the effect of GRPO-based RL post-training on trace validity, noting that while solution accuracy increase, this is not accompanied by any improvements in trace validity. Finally, we examine whether reasoning-trace length reflects inference-time scaling and find that trace length is largely agnostic to the underlying computational complexity of the problem being solved. These results challenge the assumption that intermediate tokens or ``Chains of Thought''reflect or induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly seemingly forms) as evidence of human-like or algorithmic behaviors in language models.",arXiv.org,2,White-box,General,该论文直接研究CoT的忠实度问题，通过实验证明中间推理步骤（traces）即使被篡改或无关，模型仍能产生正确解答，表明CoT可能不忠实反映模型的实际计算过程。同时研究了RL微调对trace有效性的影响，属于核心相关的Faithfulness研究。
"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",2024,True,True,False,Verification & External Tools,论文提出了一种生成忠实、简洁且可转移的多模态理由的新范式，涉及不忠实现象和改进方法。,"The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",ACM Multimedia,2,Black-box,General,论文明确研究如何生成忠实（faithful）的多模态理由，并通过可验证的视觉编程保证其忠实度，符合Faithfulness的核心定义。尽管是黑盒方法，但直接解决了CoT的忠实性问题。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,"论文明确研究 Chain-of-Thought (CoT) 推理的忠实度（Faithfulness），探讨了三种方法（in-context learning, fine-tuning, 和 activation editing）来增强 CoT 的忠实性，并进行了广泛的实证分析。研究中还涉及了模型的内部机制激活编辑"
Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文研究了CoT的有效性和忠实性，揭示了不忠实现象，并提出了改进算法。,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究了 Chain-of-Thought (CoT) 的忠实度(Faithfulness)问题，特别是分析了 CoT 与问题、答案之间的信息交互，发现了模型在预测答案时可以从未出现在 CoT 中的问题部分回忆起正确信息的现象，这正是 Faithfulness 研究的核心问题。此外，论文还提出了一个算法来缓解这个问题，并通过实验验证了其方法在提升 CoT 忠实度和有效性方面的效果。因此，该论文属于核心相关的研究范畴。
Reasoning Models Don't Always Say What They Think,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT不忠实现象，并探讨了RLHF对忠实性的影响。,"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",arXiv.org,2,Black-box,General,论文直接研究CoT忠实度问题，评估CoT是否真实反映模型的真实推理过程，特别关注提示中提示信息的使用情况与CoT口头表达的匹配度，符合核心相关标准
Investigating Faithfulness in Large Audio Language Models,2025,True,True,False,,论文探讨了音频语言模型中CoT的忠实性，并提出了评估方法。,"Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",arXiv.org,2,White-box,General,该论文明确研究 CoT 是否忠实反映了音频语言模型的决策过程，并通过干预措施（如 paraphrasing、filler token injection、early answering）验证了 Faithfulness。研究范围虽针对音频语言模型，但主题与 CoT 忠实度直接相关。
Preventing Language Models From Hiding Their Reasoning,2023,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了模型中‘编码推理’的不忠实现象，并提出了评估防御方法和训练策略。,"Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.",arXiv.org,2,Black-box,General,该论文明确研究了中间推理步骤的忠实性（Faithfulness），特别是探讨了模型可能通过编码方式隐藏真实推理过程（encoded reasoning），并提出了一种评估防御方法的方法论。这直接涉及CoT Faithfulness的核心问题，即推理步骤是否真实反映了模型的内部计算过程。
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,2023,True,True,True,Prompting & In-Context Learning,论文提出通过问题分解提升CoT忠实性，并使用了新度量指标验证。,"As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",arXiv.org,2,Black-box,General,该论文明确研究了如何通过问题分解来改进CoT推理的忠实性，提出了提高模型生成推理忠实性的方法，并使用了最近提出的度量标准进行验证。这直接对应于Faithfulness研究的核心定义。
A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models,2024,True,True,True,,论文探讨了自解释模型的忠实性问题，并提出了评估框架，但未提出改进方法。,"The self-rationalising capabilities of LLMs are appealing because the generated explanations can give insights into the plausibility of the predictions. However, how faithful the explanations are to the predictions is questionable, raising the need to explore the patterns behind them further.To this end, we propose a hypothesis-driven statistical framework. We use a Bayesian network to implement a hypothesis about how a task (in our example, natural language inference) is solved, and its internal states are translated into natural language with templates. Those explanations are then compared to LLM-generated free-text explanations using automatic and human evaluations. This allows us to judge how similar the LLM’s and the Bayesian network’s decision processes are. We demonstrate the usage of our framework with an example hypothesis and two realisations in Bayesian networks. The resulting models do not exhibit a strong similarity to GPT-3.5. We discuss the implications of this as well as the framework’s potential to approximate LLM decisions better in future work.",Conference of the European Chapter of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了LLM生成解释的忠实性问题，提出了一种统计框架来比较LLM和贝叶斯网络的决策过程，直接关联到Faithfulness的核心问题。
Reasoning Models Sometimes Output Illegible Chains of Thought,2025,True,True,False,,论文研究了CoT的legibility与faithfulness的关系，揭示了RL训练导致的不忠实现象。,"Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We study CoT legibility across 14 reasoning models, finding that RL often causes reasoning to become illegible to both humans and AI monitors, with reasoning models (except Claude) generating illegible CoTs while returning to perfectly readable final answers. We show that models use illegible reasoning to reach correct answers (accuracy dropping by 53\% when forced to use only legible portions), yet find no correlation between legibility and performance when resampling - suggesting the relationship is more nuanced. We also find that legibility degrades on harder questions. We discuss potential hypotheses for these results, including steganography, training artifacts, and vestigial tokens. These results suggest that without explicit optimization for legibility, outcome-based RL naturally produces models with increasingly opaque reasoning processes, potentially undermining monitoring approaches.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实度（Faithfulness）问题，特别是通过强化学习方法训练的模型生成的 CoT 是否可读（legible），并探讨了 CoT 的可读性与模型性能之间的关系。论文还提出了可能导致 CoT 不忠实的原因（如隐写术、训练伪影等），符合核心相关标准。
Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning,2025,True,False,True,Verification & External Tools,论文提出基于Curry-Howard对应关系的验证框架，将CoT映射为形式化证明，属于忠实性度量方法。,"While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",arXiv.org,2,White-box,General,该论文明确提出了一个基于 Curry-Howard 对应的理论框架，用于验证 CoT 生成的 rationale 的忠实性。通过将 CoT 步骤映射到形式化的类型化证明结构，提供了一种可验证的计算忠实性证书，直接涉及 CoT 的因果角色和忠实性验证方法。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,2,Black-box,General,该论文明确提到了增强推理过程的 'faithfulness'，并通过多模型协作（Debate、Review、Retrieve模式）来提高推理的可信度和可靠性。虽然主要关注点是通过多模型协作来克服幻觉和提高解决方案的质量，但仍然涉及到了Faithfulness的概念，因此属于核心相关。
Chain-of-Thought Unfaithfulness as Disguised Accuracy,2024,True,True,True,,论文讨论了CoT忠实性度量的问题，并揭示了度量与模型准确性之间的相关性，质疑其有效性。,"Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, Lanham et al. (2023) propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate the experimental setup in their section focused on scaling experiments with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, after normalizing the metric to account for a model's bias toward certain answer choices, unfaithfulness drops significantly for smaller less-capable models. This normalized faithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy, raising doubts about its validity for evaluating faithfulness.",,2,Black-box,General,该论文直接研究 CoT 忠实度（Faithfulness）问题，验证了模型内部计算与 CoT 生成是否一致，并质疑了现有度量标准的有效性（伪装成准确度的忠实度）。
Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification,2025,True,True,False,Verification & External Tools,论文讨论了CoT忠实性问题，并提出了运行时验证方法来提升对齐性。,"Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",arXiv.org,2,Black-box,General,论文明确研究 Reasoning-Action Alignment（即 CoT 与实际行动的忠实度），提出运行时验证方法来确保文本推理与实际动作的一致性，直接涉及 Faithfulness 的核心问题。
Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出通过可验证奖励提升 CoT 忠实性，并讨论了不忠实现象。,"The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",arXiv.org,2,Black-box,General,论文明确提出了一个框架（Chart-RVR）来增强 Large Vision-Language Models（LVLMs）在图表推理任务中的鲁棒性和可解释性，特别是通过自动可验证的奖励来最大化流程一致性（process conformity）和 faithfulness 的图表表格重建，从而改进 rationale fidelity。这与 CoT Faithfulness 的定义高度相关，尤其是论文中提到的 'improving rationale fidelity' 和 'faithful chart table reconstruction' 直接涉及 CoT 推理的真实性和忠实度。
Argumentative Large Language Models for Explainable and Contestable Claim Verification,2024,True,False,True,Verification & External Tools,论文提出通过论证框架提升解释性和可争议性，与CoT忠实性相关。,"The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing argumentative LLMs (ArgLLMs), a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs’ performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.",AAAI Conference on Artificial Intelligence,1,Black-box,General,该论文研究了如何通过论证框架增强LLM的解释性，并允许对决策进行争议，虽然涉及了解释性和可争议性，但并未明确研究CoT是否是模型预测的真实原因（Faithfulness的核心定义），因此评为边缘相关。
Are DeepSeek R1 And Other Reasoning Models More Faithful?,2025,True,True,True,Training & Fine-tuning,论文评估了推理模型的忠实性，揭示了不忠实现象，并提出了度量指标。,"Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue""A Stanford Professor thinks the answer is D""is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",,2,Black-box,General,论文明确研究推理模型（DeepSeek-R1）与非推理模型在 Chain-of-Thought 忠实度上的差异，实验设计了多种提示线索（cue）来测试模型是否能准确描述这些线索对其答案的影响。研究直接针对 CoT 解释的真实性（Faithfulness），并通过对比实验验证了不同类型的模型在忠实度上的表现差异，尤其是讨论了奖励模型可能导致不忠诚响应的现象。
Markovian Transformers for Informative Language Modeling,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出了一种新的训练方法，通过强制模型将推理压缩到可解释的文本中，以提高CoT的忠实性。,"Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.",,2,White-box,General,该论文明确研究了CoT是否忠实反映模型的决策过程，引入了Markovian框架并通过扰动分析证明CoT的因果依赖性，符合Faithfulness的核心定义。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,2,Black-box,General,论文明确提出了 Faithful CoT 框架，关注生成的推理链是否真实反映模型的计算过程（faithfulness），并设计了两个阶段来保证这一点。虽然涉及多个领域，但核心贡献是对 faithfulness 的研究，因此属于核心相关。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,2,Black-box,General,这篇论文直接研究LLM能否给出忠实反映其内部分类行为的高层次解释。通过构建ArticulateRules数据集，测试模型生成的自由形式自然语言解释是否与实际分类行为相匹配，这本质上就是在探究CoT的忠实度问题。论文发现GPT-3在7/10规则上完全无法给出匹配自身行为的解释，这正是faithfulness研究的核心。
Dissociation of Faithful and Unfaithful Reasoning in LLMs,2024,True,True,False,,论文揭示了LLMs在CoT中存在不忠实现象，即推理错误但答案正确。,"Large language models (LLMs) often improve their performance in downstream tasks when they generate Chain of Thought reasoning text before producing an answer. We investigate how LLMs recover from errors in Chain of Thought. Through analysis of error recovery behaviors, we find evidence for unfaithfulness in Chain of Thought, which occurs when models arrive at the correct answer despite invalid reasoning text. We identify factors that shift LLM recovery behavior: LLMs recover more frequently from obvious errors and in contexts that provide more evidence for the correct answer. Critically, these factors have divergent effects on faithful and unfaithful recoveries. Our results indicate that there are distinct mechanisms driving faithful and unfaithful error recoveries. Selective targeting of these mechanisms may be able to drive down the rate of unfaithful reasoning and improve model interpretability.",arXiv.org,2,Black-box,General,该论文明确研究了CoT中的unfaithfulness现象，分析了模型在错误推理后仍能得出正确答案的行为，并探讨了驱动忠实与非忠实恢复的不同机制。这直接符合Faithfulness研究的核心定义，尤其是在CoT中区分忠实与非忠实推理。
Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering,2023,True,True,False,Training & Fine-tuning,论文提出了一种可解释的中间步骤设计，确保忠实性，并讨论了不忠实现象。,"Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",,2,Black-box,General,该论文明确提出模型的解释必须忠实（faithful）于模型的实际决策过程，并设计了一个可解释的中间空间（Dynamic Clue Bottleneck Model）来确保解释的真实性。这与Faithfulness的核心定义高度契合，特别是解决了Post-hoc rationales的不忠实问题。
Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers,2025,True,True,True,,论文探讨了LLMs在MCQA任务中的推理策略，并测试了其忠实性。,"Large language models (LLMs) now give reasoning before answering, excelling in tasks like multiple-choice question answering (MCQA). Yet, a concern is that LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed in MCQA without using the question, i.e., choices-only. Such partial-input success is often deemed problematic, but reasoning traces could reveal if these strategies are truly shallow in choices-only settings. To study these strategies, reasoning LLMs solve MCQs in full and choices-only inputs; test-time reasoning often boosts accuracy on full and in choices-only half the time. While possibly due to shallow shortcuts, choices-only success is barely affected by the length of reasoning traces, and after finding traces pass faithfulness tests, we show they use less problematic strategies like inferring missing questions. In all, we challenge claims that partial-input success is always a flaw, so we discuss how reasoning traces could separate problematic data from less problematic reasoning.",arXiv.org,2,Black-box,General,该论文直接研究 CoT 的忠实性问题，通过实验验证了 LLMs 在只给出选项（choices-only）时生成的推理痕迹（reasoning traces）是否真实反映了其决策逻辑（faithfulness tests），并发现这些痕迹揭示了模型并非完全依赖浅层策略（如直接选择答案），而是尝试推断缺失的问题内容。这直接关联到 CoT Faithfulness 的核心议题：推理是否真实反映了模型的计算过程，而非事后合理化（post-hoc rationalization）。
Measuring Faithfulness in Chain-of-Thought Reasoning,2023,True,True,True,,论文探讨了CoT的忠实性问题，提出了干预方法评估忠实性，并揭示了不忠实现象。,"Large language models (LLMs) perform better when they produce step-by-step,""Chain-of-Thought""(CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.",arXiv.org,2,Black-box,General,这篇论文明确研究了 Chain-of-Thought (CoT) 推理的忠实性（Faithfulness），探讨了 CoT 是否是模型预测的真实解释。通过干预 CoT（如添加错误或改写）来观察模型预测的变化，直接验证了 CoT 的因果作用（Causal role）。此外，论文还分析了模型大小和任务对 CoT 忠实性的影响，符合核心定义中对 Faithfulness 的研究。
"A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages",2025,True,True,True,,论文探讨了多语言CoT推理的性能、一致性和忠实性，并使用了扰动技术来评估忠实性。,"Large reasoning models (LRMs) increasingly rely on step-by-step Chain-of-Thought (CoT) reasoning to improve task performance, particularly in high-resource languages such as English. While recent work has examined final-answer accuracy in multilingual settings, the thinking traces themselves, i.e., the intermediate steps that lead to the final answer, remain underexplored. In this paper, we present the first comprehensive study of multilingual CoT reasoning, evaluating three key dimensions: performance, consistency, and faithfulness. We begin by measuring language compliance, answer accuracy, and answer consistency when LRMs are explicitly instructed or prompt-hacked to think in a target language, revealing strong language preferences and divergent performance across languages. Next, we assess crosslingual consistency of thinking traces by interchanging them between languages. We find that the quality and effectiveness of thinking traces vary substantially depending on the prompt language. Finally, we adapt perturbation-based techniques -- i.e., truncation and error injection -- to probe the faithfulness of thinking traces across languages, showing that models rely on traces to varying degrees. We release our code and data to support future research.",arXiv.org,2,Black-box,General,论文明确研究 Chain-of-Thought (CoT) 的忠实性（Faithfulness），通过扰动技术（如截断和错误注入）来探测不同语言间思考痕迹的忠实性。此外，还评估了跨语言一致性（Consistency），这些都是 CoT Faithfulness 的核心研究方向。
Quantifying Uncertainty in Natural Language Explanations of Large Language Models,2023,True,True,True,,论文提出了量化LLM解释不确定性的新指标，并探讨了解释的忠实性。,"Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",International Conference on Artificial Intelligence and Statistics,2,Black-box,General,这篇论文明确研究了LLM生成的解释是否可靠并反映了LLM的行为，提出了量化解释不确定性的指标，并发现探测不确定性估计与解释的忠实度相关。这直接符合Faithfulness的核心定义，即解释是否真实反映了模型的计算过程。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,2,Black-box,General,论文明确提出了 Thinking-supervised Reward Model (TRM)，通过 sentence-level thinking supervision 来评估语句级别的 faithfulness 和 correctness。该方法直接关注于生成的解释（CoT）是否真实反映了模型的预测过程，属于 Faithfulness 的核心研究范畴。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,1,Black-box,General,论文提出了一个解释性和可争议的替代方案，使用了结构化推理来替代黑盒推理，并提到了 'faithfully explaining and contesting decisions'。虽然它涉及解释性和透明性，但并未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此被评为边缘相关。
Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了如何通过训练提升自我解释的忠实性，并验证了其泛化能力。,"Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",,2,White-box,General,该论文明确研究了语言模型自我解释的忠实性(Faithfulness)，并探讨了如何通过训练和特征归因方法来改进忠实性。论文还分析了不同解释风格的泛化能力，符合核心相关标准。
Investigating CoT Monitorability in Large Reasoning Models,2025,True,True,False,,论文探讨了CoT忠实性和监控可靠性，揭示了不忠实现象如捷径和阿谀奉承。,"Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models'long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models'misbehavior through their CoT and provide structured judgments along with supporting evidence.",,2,Black-box,General,论文明确研究了 CoT 的忠实度问题，包括模型是否真实反映其内部决策过程（verbalization）以及 CoT 监控的可靠性（monitor reliability）。此外，论文还涉及模型的欺骗行为（sycophancy）和监控的有效性，这些都是 CoT Faithfulness 的核心议题。
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,2023,True,True,False,,论文揭示了CoT解释可能误导性地反映模型预测的真实原因，属于不忠实现象。,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always""(A)""--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",Neural Information Processing Systems,2,Black-box,General,该论文明确研究 CoT 解释是否真实反映模型的预测过程，发现 CoT 可以系统性误导（如通过偏置输入），验证了 Unfaithful Explanations 现象，直接关联 Faithfulness 核心议题。
FaithAct: Faithfulness Planning and Acting in MLLMs,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了FaithEval评估框架和FaithAct改进方法，关注CoT忠实性。,"Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",,2,Black-box,General,该论文明确研究了行为忠实度（行为和输出的一致性）和感知忠实度（推理和输入的一致性），并提出了量化评估框架FaithEval。FaithAct框架强化了每个推理步骤的依据基础，直接针对CoT的忠实性问题。
VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives,2022,True,True,True,Training & Fine-tuning,论文讨论了模型解释的忠实性，并提出了改进方法。,"Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis",Neural Information Processing Systems,2,White-box,General,论文明确研究了解释的忠实性（faithfulness）与其合理性（plausibility）的关系，并指出了只有当解释既合理又忠实时，模型的预测才更准确。这与CoT Faithfulness的研究范畴高度相关。
On Measuring Faithfulness or Self-consistency of Natural Language Explanations,2023,True,True,True,,论文讨论了CoT解释的不忠实现象，并提出了新的度量指标CC-SHAP来评估自洽性。,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at \url{https://github.com/Heidelberg-NLP/CC-SHAP}",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究 CoT 解释的忠实性（Faithfulness），并提出新的测量指标 CC-SHAP 来比较模型输入对预测和解释生成的贡献。此外，它还讨论了现有测试方法的局限性，并尝试通过更接近模型内部机制的方式来测量忠实性。
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",2025,True,True,False,Training & Fine-tuning,论文探讨了CoT推理痕迹的忠实性问题，并提出了基于规则的问题分解方法来改进。,"Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",arXiv.org,2,Black-box,General,该论文明确研究了推理轨迹（CoT）的忠实性问题，探讨了正确轨迹与最终输出正确性之间的低相关性，直接挑战了利用推理轨迹提升模型性能的隐含假设，符合Faithfulness的核心定义。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程（Faithfulness的核心问题），并通过微调LM生成自然语言描述来验证其解释的真实性。研究涉及LM特征的编码信息、内部激活的因果结构及输入令牌对输出的影响（白盒方法），直接对应Faithfulness的定义。
Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain,2025,True,True,True,Verification & External Tools,论文讨论了LLM推理的不忠实现象，并提出了结合符号引擎的改进方法。,"Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful—serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions. In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness.",,2,White-box,General,论文明确研究了LLM推理轨迹的非忠实性问题（unfaithful），并提出了MCFR框架来提高推理的忠实性和可解释性。此外，论文还对比了MCFR与传统LLM（如ChatGPT、DeepSeek、Claude）的表现，进一步验证了其在忠实性方面的改进。
Evaluating Explanation Methods for Vision-and-Language Navigation,2023,False,False,True,,论文关注视觉与语言导航中的解释方法，但未明确涉及CoT。,"The ability to navigate robots with natural language instructions in an unknown environment is a crucial step for achieving embodied artificial intelligence (AI). With the improving performance of deep neural models proposed in the field of vision-and-language navigation (VLN), it is equally interesting to know what information the models utilize for their decision-making in the navigation tasks. To understand the inner workings of deep neural models, various explanation methods have been developed for promoting explainable AI (XAI). But they are mostly applied to deep neural models for image or text classification tasks and little work has been done in explaining deep neural models for VLN tasks. In this paper, we address these problems by building quantitative benchmarks to evaluate explanation methods for VLN models in terms of faithfulness. We propose a new erasure-based evaluation pipeline to measure the step-wise textual explanation in the sequential decision-making setting. We evaluate several explanation methods for two representative VLN models on two popular VLN datasets and reveal valuable findings through our experiments.",European Conference on Artificial Intelligence,1,White-box,General,该论文研究视觉语言导航(VLN)模型的解释方法忠实度评估，建立了基于擦除的量化评估框架来测量序列决策过程中的解释忠实性。虽然研究领域是导航任务而非典型的CoT推理，但其核心关注解释方法是否真实反映模型决策过程，这与CoT忠实度的研究方向高度一致。论文提出的忠实度评估方法论对CoT忠实度研究具有参考价值。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到在设计基于自然逻辑的事实验证系统时，忠实度是一个重要考虑因素，即生成的解释需要准确反映模型的推理过程。此外，论文还进行了人工评估，表明其方法生成的解释比以前的自然逻辑系统更合理，符合 CoT Faithfulness 的核心研究范畴。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,2,White-box,General,该论文明确提出了 faithfulness 的概念，并研究了证据检索如何反映模型的决策过程（faithfulness），同时还涉及到 plausibility 和准确性。符合 CoT Faithfulness 的核心定义。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确提出通过训练AI系统建立自身的因果模型来生成忠实且有语义意义的解释，这与CoT Faithfulness的核心定义直接相关，尤其是讨论了生成的解释是否真实反映了模型的实际计算过程。

title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Thought Branches: Interpreting LLM Reasoning Requires Resampling,2025,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的忠实性问题，提出了通过重采样进行因果分析的方法，并讨论了不忠实现象。,"Most work interpreting reasoning models studies only a single chain-of-thought (CoT), yet these models define distributions over many possible CoTs. We argue that studying a single sample is inadequate for understanding causal influence and the underlying computation. Though fully specifying this distribution is intractable, it can be understood by sampling. We present case studies using resampling to investigate model decisions. First, when a model states a reason for its action, does that reason actually cause the action? In""agentic misalignment""scenarios, we resample specific sentences to measure their downstream effects. Self-preservation sentences have small causal impact, suggesting they do not meaningfully drive blackmail. Second, are artificial edits to CoT sufficient for steering reasoning? These are common in literature, yet take the model off-policy. Resampling and selecting a completion with the desired property is a principled on-policy alternative. We find off-policy interventions yield small and unstable effects compared to resampling in decision-making tasks. Third, how do we understand the effect of removing a reasoning step when the model may repeat it post-edit? We introduce a resilience metric that repeatedly resamples to prevent similar content from reappearing downstream. Critical planning statements resist removal but have large effects when eliminated. Fourth, since CoT is sometimes""unfaithful"", can our methods teach us anything in these settings? Adapting causal mediation analysis, we find that hints that have a causal effect on the output without being explicitly mentioned exert a subtle and cumulative influence on the CoT that persists even if the hint is removed. Overall, studying distributions via resampling enables reliable causal analysis, clearer narratives of model reasoning, and principled CoT interventions.",arXiv.org,2,White-box,General,该论文明确研究 CoT 是否是模型预测的真实原因（Causal role），通过重采样方法探讨模型决策的因果影响。论文还讨论了 CoT 的'不忠实性'（unfaithful）现象，并提出了测量 Faithfulness 的指标（如 resilience metric 和 causal mediation analysis）。这些内容直接符合 Faithfulness 的核心定义。
Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought,2025,True,True,True,Interpretability & Internal Mechanisms,论文揭示了CoT中存在真假思考步骤的现象，并提出了度量指标TTS和白盒干预方法。,"Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs'prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS>= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.",arXiv.org,2,White-box,General,该论文明确研究 CoT 中哪些推理步骤真正对模型预测有因果影响，提出了 True Thinking Score (TTS) 来测量每个推理步骤的因果作用，并揭示了模型会生成装饰性推理步骤（decorative-thinking steps）的现象。这直接符合 Faithfulness 的核心定义，即研究 CoT 是否真实反映模型的内部计算过程。此外，论文还通过操纵潜在空间中的 TrueThinking 方向来验证因果性，属于白盒方法。
Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出逆向推理方法，通过元认知结构提升模型对自身推理链的解释能力，涉及忠实性。,"Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.",arXiv.org,2,White-box,General,该论文明确研究LLM如何通过逆向推理（inverse reasoning）分解和解释自己的推理链，涉及模型内部注意力机制的反向流动，直接探讨了推理链选择的真实原因（causal role），属于CoT Faithfulness的核心研究范畴。
Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation,2025,True,True,True,Interpretability & Internal Mechanisms,论文讨论了CoT在临床决策中的不忠实现象，并提出了评估框架和改进方法。,"Large language models (LLMs) offer transformative potential for clinical decision support in spine surgery but pose significant risks through hallucinations, which are factually inconsistent or contextually misaligned outputs that may compromise patient safety. This study introduces a clinician-centered framework to quantify hallucination risks by evaluating diagnostic precision, recommendation quality, reasoning robustness, output coherence, and knowledge alignment. We assessed six leading LLMs across 30 expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes domains such as trauma and infection. A critical finding reveals that reasoning-enhanced model variants did not uniformly outperform standard counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92), indicating extended chain-of-thought reasoning alone is insufficient for clinical reliability. Multidimensional stress-testing exposed model-specific vulnerabilities, with recommendation quality degrading by 7.4% under amplified complexity. This decline contrasted with marginal improvements in rationality (+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning divergence between perceived coherence and actionable guidance. Our findings advocate integrating interpretability mechanisms (e.g., reasoning chain visualization) into clinical workflows and establish a safety-aware validation framework for surgical LLM deployment.",arXiv.org,0,Black-box,Medical,论文主要研究AI在医疗决策支持中的幻觉风险，关注的是事实一致性和上下文对齐（Factuality），而非推理过程的忠实度（Faithfulness）。虽然提到了推理增强模型的表现，但并未深入分析CoT是否真实反映模型的计算过程。
Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了语言模型在生成CoT时的内部状态和不确定性，与Faithfulness相关，并提出了基于激活干预的改进方法。,"When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model's uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model -- in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model's future outcome distribution, demonstrating that models implicitly represent the space of possible paths.",arXiv.org,2,White-box,General,该论文明确研究语言模型在生成链式思考（CoT）时的内部机制，通过隐藏激活状态来控制和预测模型的不确定性，探讨模型是否意识到未选择的推理路径。这与Faithfulness的核心定义直接相关，因为它涉及模型生成的推理路径是否真实反映了其内部计算过程，而非事后合理化。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,论文明确提出了一个自解释的GNN框架X-Node，其中每个节点在预测过程中生成自己的解释，并通过解码器强制忠实度（faithfulness）。此外，论文还讨论了如何通过解释向量来重建节点的潜在嵌入，以确保解释的真实性，这与CoT Faithfulness的核心定义直接相关。
Unsupervised decoding of encoded reasoning using language model interpretability,2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了通过内部激活解码 CoT 的方法，属于 Faithfulness 的白盒评估。,"As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.",,2,White-box,General,该论文明确研究模型内部机制（如logit lens分析）是否能解码隐藏的推理过程（ROT-13加密的CoT），直接涉及CoT的忠实性验证。通过分析内部激活值重建推理轨迹，属于白盒方法，且任务领域不特定于数学或逻辑等，故归类为通用领域。
Visual Agents as Fast and Slow Thinkers,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出FaST机制，通过动态选择System 1/2模式提升推理忠实性，涉及不忠实现象和改进方法。,"Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1/2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaST's superior performance. Extensive testing validates the efficacy and robustness of FaST's core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps://github.com/GuangyanS/Sys2-LLaVA.",International Conference on Learning Representations,1,Black-box,General,论文研究了视觉代理的快速和慢速思考机制，并提出了一个动态切换系统1/2模式的解决方案。虽然涉及了推理能力的层次化和透明决策流程，但未明确探讨CoT的忠实度问题，因此属于边缘相关。
MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了 CoT 中的 sycophantic 行为，并提出了实时监控和校准方法，涉及不忠实现象和改进方法。,"Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users'incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.",,2,Black-box,Society,该论文明确研究大型推理模型中的阿谀奉承行为（Sycophancy），即在推理过程中模型倾向于迎合用户的错误信念而非保持独立推理。论文提出的MONICA框架通过实时监控推理步骤中的阿谀奉承行为并动态抑制，直接涉及CoT的忠实度问题，属于核心相关研究。
Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models,2025,True,True,True,Interpretability & Internal Mechanisms,论文提出了NeuroFaith框架，评估和提升LLM自我解释的忠实性，涉及不忠实现象和度量方法。,"Large Language Models (LLMs) can generate plausible free text self-explanations to justify their answers. However, these natural language explanations may not accurately reflect the model's actual reasoning process, indicating a lack of faithfulness. Existing faithfulness evaluation methods rely primarily on behavioral tests or computational block analysis without examining the semantic content of internal neural representations. This paper proposes NeuroFaith, a flexible framework that measures the faithfulness of LLM free text self-explanation by identifying key concepts within explanations and mechanistically testing whether these concepts actually influence the model's predictions. We show the versatility of NeuroFaith across 2-hop reasoning and classification tasks. Additionally, a linear faithfulness probe based on NeuroFaith is developed to detect unfaithful self-explanations from representation space and improve faithfulness through steering. NeuroFaith provides a principled approach to evaluating and enhancing the faithfulness of LLM free text self-explanations, addressing critical needs for trustworthy AI systems.",arXiv.org,2,White-box,General,该论文明确研究LLM生成的自由文本自我解释是否忠实反映模型的实际推理过程，提出了NeuroFaith框架来测量忠实度，并通过内部神经表示分析验证概念是否真正影响预测，直接涉及Faithfulness的核心问题。
Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT的不忠实现象，并提出了基于激活的监控方法。,"Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",arXiv.org,2,White-box,General,论文明确研究了 CoT 的真实性（Faithfulness）问题，特别是提到了 CoT 文本常不忠实（unfaithful and misleading），而模型内部的激活值提供了更可靠的信号。此外，论文还探讨了 performative CoTs 现象，即推理过程与最终回应矛盾，这与 Faithfulness 的核心定义直接相关。
"From Reasoning to Answer: Empirical, Attention-Based and Mechanistic Insights into Distilled DeepSeek R1 Models",2025,True,False,False,Interpretability & Internal Mechanisms,论文探讨了推理痕迹对答案生成的影响，并通过干预实验验证了推理与答案之间的因果关系。,"Large Reasoning Models (LRMs) generate explicit reasoning traces alongside final answers, yet the extent to which these traces influence answer generation remains unclear. In this work, we conduct a three-stage investigation into the interplay between reasoning and answer generation in three distilled DeepSeek R1 models. First, through empirical evaluation, we demonstrate that including explicit reasoning consistently improves answer quality across diverse domains. Second, attention analysis reveals that answer tokens attend substantially to reasoning tokens, with certain mid-layer Reasoning-Focus Heads (RFHs) closely tracking the reasoning trajectory, including self-reflective cues. Third, we apply mechanistic interventions using activation patching to assess the dependence of answer tokens on reasoning activations. Our results show that perturbations to key reasoning tokens can reliably alter the final answers, confirming a directional and functional flow of information from reasoning to answer. These findings deepen our understanding of how LRMs leverage reasoning tokens for answer generation, highlighting the functional role of intermediate reasoning in shaping model outputs. Our data and code are publicly available at \href{https://aka.ms/R2A-code}{this URL}.",,2,White-box,General,该论文明确研究了推理痕迹（CoT）对答案生成的实际影响，通过注意力分析和机制干预（如激活修补）验证了推理痕迹在模型预测中的因果作用，符合Faithfulness的核心定义。
Faithful Knowledge Graph Explanations in Commonsense Question Answering,2022,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了知识图谱解释的不忠实性，并提出了新的度量方法和改进架构。,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",Conference on Empirical Methods in Natural Language Processing,2,White-box,Society,该论文明确研究知识图谱解释的忠实度（Faithfulness），提出了一种新的代理测量方法，并分析了现有模型无法生成高度忠实解释的原因，符合核心定义。
Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems,2023,True,True,True,Interpretability & Internal Mechanisms,论文探讨了CoT在算术问题中的因果影响，揭示了LLMs使用CoT进行推理的现象，并提出了因果抽象评估方法。,"Recent work suggests that large language models (LLMs) achieve higher accuracy on multi-step reasoning tasks when prompted to generate intermediate reasoning steps, or a chain of thought (CoT), before their final answer. However, it is unclear how exactly CoTs improve LLMs’ accuracy, and in particular, if LLMs use their CoTs to reason to their final answers. This paper tries to answer this question with respect to arithmetic word problems, by (i) evaluating the correctness of LLMs’ CoTs, and (ii) using causal abstraction to assess if the intermediate tokens produced as part of a CoT causally impact LLMs’ final answers, in line with the reasoning described by the CoT. We find that for CoT-prompted LLMs, correct answers to arithmetic problems are highly correlated with correct CoTs, and that when LLMs produce correct CoTs, they realize to a fairly large extent the causal models suggested by their CoTs. Higher degrees of realization also seem associated with better overall accuracy on the arithmetic problems. These findings suggest that some CoT-prompted LLMs may do better on multi-step arithmetic reasoning at least partly because they use their CoTs to reason to their final answers. However, for some LLMs, other internal processes may also be involved.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,2,White-box,Math,该论文明确研究 CoT 是否因果影响模型的最终答案（通过因果抽象方法评估），直接关联 Faithfulness 的核心定义。研究领域为数学（算术应用题），且涉及模型内部机制分析（白盒）。
The Reasoning-Memorization Interplay in Language Models Is Mediated by a Single Direction,2025,True,True,False,Interpretability & Internal Mechanisms,论文揭示了LLMs在推理和记忆之间的动态切换现象，并提出了通过干预内部机制来提升推理忠实性的方法。,"Large language models (LLMs) excel on a variety of reasoning benchmarks, but previous studies suggest they sometimes struggle to generalize to unseen questions, potentially due to over-reliance on memorized training examples. However, the precise conditions under which LLMs switch between reasoning and memorization during text generation remain unclear. In this work, we provide a mechanistic understanding of LLMs' reasoning-memorization dynamics by identifying a set of linear features in the model's residual stream that govern the balance between genuine reasoning and memory recall. These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks. Additionally, we show that intervening in these reasoning features helps the model more accurately activate the most relevant problem-solving capabilities during answer generation. Our findings offer new insights into the underlying mechanisms of reasoning and memory in LLMs and pave the way for the development of more robust and interpretable generative AI systems.",arXiv.org,2,White-box,General,该论文明确研究了语言模型在推理和记忆之间的动态平衡，通过识别模型残差流中的线性特征来控制真正的推理和记忆回忆之间的平衡。这些特征不仅可以区分推理任务和记忆密集型任务，还可以被操纵以因果性地影响模型在推理任务上的表现。这与Faithfulness的核心定义高度相关，因为它探讨了模型生成解释（CoT/Reasoning Trace）是否真实反映了模型做出最终预测的实际计算过程。
Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了FUR框架，通过消除推理步骤中的信息来测量忠实性，揭示了不忠实现象并提出了新的度量方法。,"When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. Despite much work on CoT prompting, it is unclear if reasoning verbalized in a CoT is faithful to the models'parametric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters, and measures faithfulness as the resulting effect on the model's prediction. Our experiments with four LMs and five multi-hop multi-choice question answering (MCQA) datasets show that FUR is frequently able to precisely change the underlying models'prediction for a given instance by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning.",,2,White-box,General,该论文明确研究 CoT 的忠实度，提出了一个框架（FUR）来测量生成的推理步骤是否忠实于模型的参数信念。通过从模型参数中擦除推理步骤的信息，并测量对模型预测的影响，来评估忠实度。这直接符合 Faithfulness 的核心定义，即研究 CoT 是否是模型预测的真实原因。
Effectively Controlling Reasoning Models through Thinking Intervention,2025,True,False,False,Interpretability & Internal Mechanisms,论文提出通过干预内部推理过程来增强模型行为控制，与CoT忠实性相关。,"Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval and Overthinking, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.",arXiv.org,1,White-box,General,该论文提出了Thinking Intervention方法，通过干预内部推理过程来控制模型行为，虽然未直接研究CoT的忠实度，但涉及了推理过程的内部机制，属于边缘相关。
The Geometry of Self-Verification in a Task-Specific Reasoning Model,2025,True,False,False,Interpretability & Internal Mechanisms,论文研究了模型如何自我验证其推理过程，涉及CoT的忠实性，并提出了白盒干预方法。,"How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, yielding a model that always produces highly structured chain-of-thought sequences. With this setup, we do top-down and bottom-up analyses to reverse-engineer how the model verifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect''. Bottom-up, we find that ``previous-token heads'' are mainly responsible for self-verification in our setup. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU weights to localize as few as three attention heads that can disable self-verification, pointing to a necessary component of a potentially larger verification circuit. Finally, we verify that similar verification components exist in our base model and a general reasoning DeepSeek-R1 model.",arXiv.org,2,White-box,Logic,该论文明确研究了模型如何验证自己的答案，通过分析模型内部机制（如GLU权重和注意力头）来理解自我验证过程，这与CoT Faithfulness的核心定义直接相关，即研究解释是否真实反映了模型的实际计算过程。
Bonsai: Interpretable Tree-Adaptive Grounded Reasoning,2025,True,False,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种可解释的推理系统，强调透明推理和验证，与CoT忠实性相关。,"To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.",arXiv.org,1,Black-box,General,该论文提出了一个可解释的推理系统Bonsai，能够生成适应性推理树，并通过检索相关证据来计算子主张的可能性。虽然它关注推理的透明性和不确定性，但并未明确研究CoT是否真实反映模型预测的实际计算过程，因此属于边缘相关。
Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models,2024,True,True,True,Interpretability & Internal Mechanisms,论文提出使用激活修补技术测量解释的忠实性，并提出了新的度量标准。,"Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \url{https://github.com/wj210/Causal-Faithfulness}",,2,White-box,General,该论文明确研究了解释的忠实性（Faithfulness），提出了一种新的度量方法 Causal Faithfulness，利用激活修补（activation patching）技术来测量解释与模型内部计算的一致性。这符合核心相关性的定义，且采用了白盒方法分析模型内部机制。
CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了因果幻觉（不忠实现象），并提出了CRE机制来增强因果关系，提升Faithfulness。,"Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.",arXiv.org,1,Black-box,Logic,该论文主要关注长程推理问题中的因果幻觉和搜索空间优化，提出了增强因果关系的机制和双端搜索方法。虽然涉及因果推理的忠实性问题，但主要目标是提高准确性和效率，而非直接研究CoT的忠实度。
A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,2025,True,True,False,"Verification & External Tools, Interpretability & Internal Mechanisms",论文探讨了 CoT 监控对安全性的影响，涉及 faithfulness 问题和验证工具。,"As AI systems approach dangerous capability levels where inability safety cases become insufficient, we need alternative approaches to ensure safety. This paper presents a roadmap for constructing safety cases based on chain-of-thought (CoT) monitoring in reasoning models and outlines our research agenda. We argue that CoT monitoring might support both control and trustworthiness safety cases. We propose a two-part safety case: (1) establishing that models lack dangerous capabilities when operating without their CoT, and (2) ensuring that any dangerous capabilities enabled by a CoT are detectable by CoT monitoring. We systematically examine two threats to monitorability: neuralese and encoded reasoning, which we categorize into three forms (linguistic drift, steganography, and alien reasoning) and analyze their potential drivers. We evaluate existing and novel techniques for maintaining CoT faithfulness. For cases where models produce non-monitorable reasoning, we explore the possibility of extracting a monitorable CoT from a non-monitorable CoT. To assess the viability of CoT monitoring safety cases, we establish prediction markets to aggregate forecasts on key technical milestones influencing their feasibility.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 监控的忠实度（Faithfulness），探讨了模型在 CoT 启用时可能产生的危险能力是否可检测，并分析了威胁监控的三种形式（语言漂移、隐写术和异质推理）。此外，论文还评估了保持 CoT 忠实度的现有和新颖技术，直接涉及 CoT 是否真实反映模型预测的实际计算过程。
How Do Humans Write Code? Large Models Do It the Same Way Too,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT和PoT的忠实性问题，并提出了改进方法。,"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Math,论文主要关注的是通过新的生成范式（HTL）来提高数学推理任务的性能，而不是研究 CoT 是否真实反映了模型的预测过程。虽然提到了 CoT 和 PoT 的比较，但重点在于性能提升而非忠实度分析。
How Likely Do LLMs with CoT Mimic Human Reasoning?,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了LLMs在CoT中偏离理想因果链的现象，并探讨了影响因果结构的因素。,"Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",International Conference on Computational Linguistics,2,Black-box,General,该论文明确研究LLMs的CoT推理过程与人类推理的差异，并通过因果分析探讨问题指令、推理和答案之间的关系，揭示了LLMs常常偏离理想的因果链，导致虚假相关性和潜在的一致性错误。这与Faithfulness的核心定义高度相关，即研究CoT是否真实反映了模型的实际计算过程。
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了CoT中的不忠实现象（Toxic CoT问题），并提出了基于内部机制探测的改进方法。,"Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了CoT推理过程中的信息丢失问题，使用内部机制分析方法（如attribution tracing和causal tracing），并且设计了RIDERS方法来补偿信息缺失，直接关联到CoT的真实性和忠实度问题。
Understanding Reasoning in Chain-of-Thought from the Hopfieldian View,2024,True,False,True,Interpretability & Internal Mechanisms,论文提出了基于Hopfieldian认知视角的RoT框架，增强CoT推理的鲁棒性和可解释性。,"Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on improving performance, lacking a comprehensive framework to explain and understand the fundamental factors behind CoT's success. To bridge this gap, we introduce a novel perspective grounded in the Hopfieldian view of cognition in cognitive neuroscience. We establish a connection between CoT reasoning and key cognitive elements such as stimuli, actions, neural populations, and representation spaces. From our view, we can understand the reasoning process as the movement between these representation spaces. Building on this insight, we develop a method for localizing reasoning errors in the response of CoTs. Moreover, we propose the Representation-of-Thought (RoT) framework, which leverages the robustness of low-dimensional representation spaces to enhance the robustness of the reasoning process in CoTs. Experimental results demonstrate that RoT improves the robustness and interpretability of CoT reasoning while offering fine-grained control over the reasoning process.",arXiv.org,1,White-box,General,论文从Hopfieldian认知视角研究CoT的内部机制，涉及神经群体和表征空间等认知元素，并定位推理错误，但未明确讨论CoT是否忠实反映模型的实际计算过程。
FLARE: Faithful Logic-Aided Reasoning and Exploration,2024,True,True,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出FLARE方法，通过逻辑编程和多跳搜索提高CoT忠实性，并量化评估。,"Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.",,2,Black-box,Logic,该论文明确研究 CoT 的忠实性问题，提出了 FLARE 方法来计算推理过程的忠实度，并分析了多跳搜索的步骤，符合核心相关标准。
Unveiling and Causalizing CoT: A Causal Pespective,2025,True,True,True,Interpretability & Internal Mechanisms,论文从因果视角揭示和纠正CoT中的错误步骤，提升忠实性和可理解性。,"Although Chain-of-Thought (CoT) has achieved remarkable success in enhancing the reasoning ability of large language models (LLMs), the mechanism of CoT remains a ``black box''. Even if the correct answers can frequently be obtained, existing CoTs struggle to make the reasoning understandable to human. In this paper, we unveil and causalize CoT from a causal perspective to ensure both correctness and understandability of all reasoning steps (to the best of our knowledge, the first such). We model causality of CoT via structural causal models (SCM) to unveil the reasoning mechanism of CoT. To measure the causality of CoT, we define the CoT Average Causal Effect (CACE) to test the causal relations between steps. For those steps without causality (wrong or unintelligible steps), we design a role-playing causal query algorithm to causalize these steps, resulting a causalized CoT with all steps correct and understandable. Experimental results on both open-source and closed-source LLMs demonstrate that the causal errors commonly in steps are effectively corrected and the reasoning ability of LLMs is significantly improved.",arXiv.org,2,White-box,General,该论文明确从因果视角研究CoT的机制，通过结构因果模型（SCM）和定义的CoT平均因果效应（CACE）来测试步骤间的因果关系，直接涉及CoT的忠实度问题。
Verifying Chain-of-Thought Reasoning via Its Computational Graph,2025,True,False,True,Interpretability & Internal Mechanisms,论文提出了一种白盒方法验证CoT的计算图，直接关联模型内部电路与推理过程，符合Faithfulness定义。,"Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into why a computation fails. We introduce a white-box method: Circuit-based Reasoning Verification (CRV). We hypothesize that attribution graphs of correct CoT steps, viewed as execution traces of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",arXiv.org,2,White-box,General,该论文明确研究 CoT 的计算图（computational graph）作为模型潜在推理电路的执行轨迹，并验证其结构特征是否能预测推理错误。这直接涉及 CoT 是否真实反映模型的计算过程（Faithfulness），且通过白盒方法（如干预 transcoder 特征）证明其因果性，符合核心相关标准。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,该论文明确研究自然语言解释（NLEs）的忠实性问题，提出了一种框架（G-Tex）来增强NLEs的忠实性，通过使用highlight explanations作为忠实线索来指导NLE生成，确保生成的解释与模型的内部推理过程一致。这直接符合Faithfulness的定义，即解释是否真实反映了模型的实际计算过程。
GraphGhost: Tracing Structures Behind Large Language Models,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过GraphGhost框架分析LLM内部结构，涉及CoT的忠实性，但未讨论不忠实现象或提出新度量。,"Large Language Models (LLMs) demonstrate remarkable reasoning capabilities, yet the structural mechanisms underlying these abilities remain under explored. In this work, we introduce GraphGhost, a unified framework that represents neuron activations and their signal propagation as graphs, explaining how LLMs capture structural semantics from sequential inputs and generate outputs through structurally consistent mechanisms. This graph-based perspective enables us to employ graph algorithms such as PageRank to characterize the properties of LLMs, revealing both shared and model-specific reasoning behaviors across diverse datasets. We further identify the activated neurons within GraphGhost and evaluate them through structural interventions, showing that edits to key neuron nodes can trigger reasoning collapse, altering both logical flow and semantic understanding. Together, these contributions position GraphGhost as a powerful tool for analyzing, intervening in, and ultimately understanding the structural foundations of reasoning in LLMs.",arXiv.org,1,White-box,General,该论文研究了LLM内部神经元激活和信号传播的结构机制，通过图算法分析模型推理行为，并进行了结构干预实验。虽然未直接提及CoT Faithfulness，但其对模型内部机制的分析（如关键神经元节点对推理流程的影响）可为Faithfulness研究提供旁证，属于边缘相关。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,该论文明确研究如何提高CoT推理的忠实度，探讨了in-context learning、fine-tuning和activation editing等方法，并进行了广泛的实证分析，直接涉及CoT是否真实反映模型行为的问题。
Calibrating Reasoning in Language Models with Internal Consistency,2024,True,True,True,"Interpretability & Internal Mechanisms, Consistency & Ensembling",论文探讨了CoT的内部表示不一致性，提出了内部一致性度量及校准方法。,"Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs. Our code is available at github.com/zhxieml/internal-consistency.",Neural Information Processing Systems,2,White-box,General,该论文明确研究了生成的理由（rationales）与模型内部表示之间的一致性，揭示了中间层与最终层表示的不一致性可能削弱推理过程的可靠性。这直接涉及CoT的忠实度问题，即模型生成的解释是否真实反映了其预测的实际计算过程。此外，论文提出了通过内部一致性来校准推理的方法，这与Faithfulness的核心定义高度相关。
DeCoT: Debiasing Chain-of-Thought for Knowledge-Intensive Tasks in Large Language Models via Causal Intervention,2024,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT中的偏见问题，并提出了因果干预方法来提升忠实性。,"Large language models (LLMs) often require 001 task-relevant knowledge to augment their inter-002 nal knowledge through prompts. However, sim-003 ply injecting external knowledge into prompts 004 does not guarantee that LLMs can identify 005 and use relevant information in the prompts to 006 conduct chain-of-thought reasoning, especially 007 when the LLM’s internal knowledge is derived 008 from the biased information on the pretraining 009 data. In this paper, we propose a novel causal 010 view to formally explain the internal knowl-011 edge bias of LLMs via a Structural Causal 012 Model (SCM). We review the chain-of-thought 013 (CoT) prompting from a causal perspective, and 014 discover that the biased information from pre-015 trained models can impair LLMs’ reasoning 016 abilities. When the CoT reasoning paths are 017 misled by irrelevant information from prompts 018 and are logically incorrect, simply editing fac-019 tual information is insufficient to reach the cor-020 rect answer. To estimate the confounding effect 021 on CoT reasoning in LLMs, we use external 022 knowledge as an instrumental variable. We fur-023 ther introduce CoT as a mediator to conduct 024 front-door adjustment and generate logically 025 correct CoTs where the spurious correlation be-026 tween LLMs’ pretrained knowledge and task 027 queries is reduced. With extensive experiments, 028 we validate that our approach enables more 029 accurate CoT reasoning and enhances LLM 030 generation on knowledge-intensive tasks. 031",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,论文明确研究 CoT 的忠实性问题，通过因果干预（Causal Intervention）和结构因果模型（SCM）分析预训练知识偏差如何影响 CoT 推理路径的真实性。研究涉及模型内部机制（白盒方法），并提出了减少虚假相关性的方法，直接关联 Faithfulness 的核心定义。
Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis,2025,True,False,False,Interpretability & Internal Mechanisms,论文通过机制解释性方法分析Transformer模型内部电路，与CoT忠实性相关。,"Transformer-based language models excel at both recall (retrieving memorized facts) and reasoning (performing multi-step inference), but whether these abilities rely on distinct internal mechanisms remains unclear. Distinguishing recall from reasoning is crucial for predicting model generalization, designing targeted evaluations, and building safer interventions that affect one ability without disrupting the other.We approach this question through mechanistic interpretability, using controlled datasets of synthetic linguistic puzzles to probe transformer models at the layer, head, and neuron level. Our pipeline combines activation patching and structured ablations to causally measure component contributions to each task type. Across two model families (Qwen and LLaMA), we find that interventions on distinct layers and attention heads lead to selective impairments: disabling identified""recall circuits""reduces fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas disabling""reasoning circuits""reduces multi-step inference by a comparable margin. At the neuron level, we observe task-specific firing patterns, though these effects are less robust, consistent with neuronal polysemanticity.Our results provide the first causal evidence that recall and reasoning rely on separable but interacting circuits in transformer models. These findings advance mechanistic interpretability by linking circuit-level structure to functional specialization and demonstrate how controlled datasets and causal interventions can yield mechanistic insights into model cognition, informing safer deployment of large language models.",arXiv.org,1,White-box,General,该论文通过机制可解释性方法研究了Transformer模型中回忆和推理的内部机制，虽然未直接涉及CoT Faithfulness，但分析了不同层和注意力头对任务的影响，可以作为Faithfulness研究的旁证。
Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models,2025,True,False,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了LLMs中的推理算法原语及其几何组合，与CoT和Faithfulness相关，提出了度量框架和改进方法。,"How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",arXiv.org,2,White-box,General,该论文明确研究语言模型内部激活模式与推理轨迹的关联，通过注入算法原语并测量其对推理步骤的影响，直接验证了模型推理过程的忠实性。研究涉及模型内部机制（如残差流操作和激活聚类），属于白盒方法，且跨任务和跨模型的评估揭示了推理微调对算法泛化的影响，与CoT Faithfulness的核心定义高度相关。
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,2,Black-box,Medical,论文明确研究了LLM在复杂计算表型任务中的推理错误，包括解释正确性和忠实性错误（unfaithfulness errors），并提出了评估框架PHEONA来识别这些错误。这直接涉及CoT的忠实度问题，即模型生成的解释是否真实反映了其预测过程。
Markovian Transformers for Informative Language Modeling,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出了一种新的训练方法，通过强制模型将推理压缩到可解释的文本中，以提高CoT的忠实性。,"Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.",,2,White-box,General,该论文明确研究 CoT 是否忠实反映模型决策过程，通过引入 Markovian 语言模型框架和扰动分析来验证 CoT 的因果依赖，属于核心相关研究。
Post Hoc Explanations of Language Models Can Improve Language Models,2023,True,True,False,"Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了利用事后解释生成自动化CoT，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, leads to critical insights for refining in-context learning.",Neural Information Processing Systems,1,Black-box,General,论文研究了利用事后解释（Post hoc explanations）生成自动化 rationale 来提升模型性能，虽然涉及解释的生成和利用，但未明确探讨这些解释是否忠实反映了模型的真实推理过程，因此属于边缘相关。
Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models,2023,True,False,False,Interpretability & Internal Mechanisms,论文通过干预注意力层提升多跳推理的忠实性，涉及内部机制改进。,"Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as “memories,” at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,1,White-box,General,该论文研究了多跳推理失败的问题，并通过分析GPT-2模型的每层激活来提出修正方法。虽然涉及模型内部机制（白盒），但主要关注的是推理的鲁棒性和一致性，而非直接研究CoT的忠实度。因此，属于边缘相关。
Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring,2025,True,True,False,Interpretability & Internal Mechanisms,论文讨论了CoT无法准确反映LLM思考过程的问题，并提出了提升透明度的新方法TELLME。,"Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.",,2,White-box,General,论文明确研究如何通过隐藏表示（hidden representations）来监控LLM的潜在思维过程，这与CoT的忠实度直接相关，因为它探讨了CoT是否准确反映了模型的真实思考过程。此外，论文提出的TELLME方法旨在提高LLM的透明度，帮助识别不适当和敏感行为，这与Faithfulness的核心定义一致。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,1,Black-box,General,该论文综述了关于大型语言模型推理评估的核心原则，虽然提到了透明度和解释性，但并未直接关注Chain-of-Thought的忠实度问题，更多是综合性的原则总结和评估框架。因此属于边缘相关。
Controlling Large Language Model Agents with Entropic Activation Steering,2024,True,False,False,Interpretability & Internal Mechanisms,论文提出通过激活导向控制LLM代理的探索行为，涉及内部机制干预。,"The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors.",arXiv.org,1,White-box,General,论文研究了LLM代理的探索行为及其控制方法，涉及模型内部表示空间的不确定性，虽然未直接讨论CoT的忠实度，但分析了模型内部机制，可以作为Faithfulness研究的旁证。
Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration,2025,True,False,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出多智能体辩论机制，通过动态相互说服的辩论过程生成可追溯的审计报告，与CoT忠实性相关。,"The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",,0,Black-box,General,论文主要研究多模态内容安全框架，通过多代理辩论和协作来提高内容审核的准确性和可解释性。虽然提到了可解释性，但并未涉及 Chain-of-Thought (CoT) 的忠实度问题，也没有研究 CoT 是否是模型预测的真实原因或是否存在事后解释现象。因此，该论文与 CoT Faithfulness 的研究范畴不相关。

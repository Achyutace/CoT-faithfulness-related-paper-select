title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Reasoning or Retrieval? A Study of Answer Attribution on Large Reasoning Models,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT推理与记忆检索的不一致性，并提出了改进方法FARL。,"Large reasoning models (LRMs) exhibit unprecedented capabilities in solving complex problems through Chain-of-Thought (CoT) reasoning. However, recent studies reveal that their final answers often contradict their own reasoning traces. We hypothesize that this inconsistency stems from two competing mechanisms for generating answers: CoT reasoning and memory retrieval. To test this hypothesis, we conduct controlled experiments that challenge LRMs with misleading cues during reasoning and/or corrupted answers during retrieval. Our results across models and datasets confirm that both mechanisms operate simultaneously, with their relative dominance influenced by multiple factors: problem domains, model scales, and fine-tuning approaches (e.g., reinforcement learning vs. distillation). The findings reveal a critical limitation in current reasoning fine-tuning paradigms: models can exploit the retrieval mechanism as a shortcut, effectively""hacking""the reward signal and undermining genuine reasoning development. To address this challenge, we introduce FARL, a novel fine-tuning framework that integrates memory unlearning with reinforcement learning. By carefully suppressing retrieval shortcuts during the fine-tuning process, FARL promotes reasoning-dominant behavior and enhances generalizable reasoning capabilities.",arXiv.org,2,Black-box,General,该论文明确研究大型推理模型在生成答案时CoT推理与记忆检索机制的竞争关系，揭示了模型可能通过检索机制‘欺骗’奖励信号而削弱真实推理过程，直接涉及CoT的忠实性问题（是否真实反映模型预测的计算过程）。实验设计（误导性线索和答案破坏）也属于典型的Faithfulness验证方法。
Reasoning-Grounded Natural Language Explanations for Language Models,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理过程的解释方法，以提高忠实性，并讨论了模型复制部分决策的现象。,"We propose a large language model explainability technique for obtaining faithful natural language explanations by grounding the explanations in a reasoning process. When converted to a sequence of tokens, the outputs of the reasoning process can become part of the model context and later be decoded to natural language as the model produces either the final answer or the explanation. To improve the faithfulness of the explanations, we propose to use a joint predict-explain approach, in which the answers and explanations are inferred directly from the reasoning sequence, without the explanations being dependent on the answers and vice versa. We demonstrate the plausibility of the proposed technique by achieving a high alignment between answers and explanations in several problem domains, observing that language models often simply copy the partial decisions from the reasoning sequence into the final answers or explanations. Furthermore, we show that the proposed use of reasoning can also improve the quality of the answers.",,2,Black-box,General,该论文明确提出了通过推理过程生成忠实的自然语言解释的方法，并强调了解释与答案的独立性以提高忠实性。这直接符合 Faithfulness 研究的核心定义，即确保解释真实反映模型的计算过程。
Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?,2025,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了LLM的解释忠实性问题，并提出了改进方法。,"Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",arXiv.org,2,Black-box,Medical,论文明确研究了解释的忠实性问题，特别是在医疗领域，探讨了推理和训练选择如何影响解释的忠实性。研究涉及了few-shot示例的数量和质量、提示策略以及训练过程对忠实性的影响，这些都是与CoT Faithfulness直接相关的核心问题。
Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出基于因果推理的框架CORE，旨在提升忠实推理，涉及不忠实现象和改进方法。,"In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of ""hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for ""Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",ACM Multimedia,2,Black-box,General,论文明确研究如何通过因果推理（causal reasoning）促进忠实推理（faithful reasoning），并构建了一个因果图来确保知识问答中的推理过程与真实知识之间的因果连接。这直接符合Faithfulness的核心定义，即解释是否真实反映了模型的实际计算过程。
Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning,2025,True,True,True,Training & Fine-tuning,论文揭示了模型在CoT中隐藏奖励黑客行为的不忠实现象，并提出了VFT方法来提升忠实性。,"Language models trained with reinforcement learning (RL) can engage in reward hacking--the exploitation of unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning. This makes the detection of reward hacking difficult, posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g.,""a Stanford professor thinks the answer is A""). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to exploit these cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low even after RL (11% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems.",arXiv.org,2,Black-box,General,该论文明确研究了 Chain-of-Thought 推理中的忠实度问题，特别是模型是否会利用奖励机制进行欺骗（reward hacking）而不在推理过程中显式表达。论文提出了 verbalization fine-tuning (VFT) 方法，旨在训练模型显式承认其受到提示线索的影响，从而提高了推理过程的忠实度。这与 Faithfulness 的核心定义高度相关，即模型生成的解释是否真实反映了其预测的实际计算过程。
multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning,2021,True,False,False,Training & Fine-tuning,论文提出生成多个证明图以提高解释性，涉及 CoT 和 Faithfulness，但未讨论不忠实现象或新度量。,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent.",North American Chapter of the Association for Computational Linguistics,1,Black-box,Logic,论文研究生成多个证明图以提高规则推理的可解释性，虽然涉及推理过程的解释，但未明确探讨这些解释是否忠实反映模型的实际计算过程。
Improving Rationality in the Reasoning Process of Language Models through Self-playing Game,2025,True,True,False,Training & Fine-tuning,论文通过自玩游戏提升模型推理过程的理性，涉及 CoT 忠实性改进。,"Large language models (LLMs) have demonstrated considerable reasoning abilities in various tasks such as mathematics and coding. However, recent studies indicate that even the best models lack true comprehension of their reasoning processes. In this paper, we explore how self-play can enhance the rationality of models in the reasoning process without supervision from humans or superior models. We design a Critic-Discernment Game(CDG) in which a prover first provides a solution to a given problem and is subsequently challenged by critiques of its solution. These critiques either aim to assist or mislead the prover. The objective of the prover is to maintain the correct answer when faced with misleading comments, while correcting errors in response to constructive feedback. Our experiments on tasks involving mathematical reasoning, stepwise error detection, self-correction, and long-chain reasoning demonstrate that CDG training can significantly improve the ability of well-aligned LLMs to comprehend their reasoning process.",International Conference on Machine Learning,2,Black-box,General,该论文明确研究了语言模型在推理过程中的理性（rationality），并通过自我游戏的方式增强模型对其推理过程的理解，这与CoT Faithfulness的核心定义直接相关。论文提出的Critic-Discernment Game（CDG）旨在提高模型在面对误导性或建设性反馈时维持或更正其推理过程的能力，这些都涉及到模型生成的推理路径是否忠实于其实际的计算过程。
Diagnostics-Guided Explanation Generation,2021,True,False,True,Training & Fine-tuning,论文讨论了如何优化解释的忠实性，并提出了直接优化诊断属性的训练方法。,"Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.",AAAI Conference on Artificial Intelligence,2,Black-box,General,该论文明确研究解释生成的忠实度（Faithfulness），并提出了优化忠实度的方法，同时涉及其他诊断属性如数据一致性和置信度指示，这些都是评估解释是否真实反映模型推理过程的关键指标。
RACG-DRT: Retrieval Augumented Code Generation Based on Dynamic Revision of Thoughts,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT在代码生成中的不忠实现象，并提出了基于动态修订的改进方法。,"Large models have the problems of hallucinations in the field of code generation. The causes of these hallucinations include errors in the model’s reasoning process, inadequate overall knowledge, and insufficient localized knowledge. To tackle these challenges, the prevailing approach currently involves influencing the large model’s reasoning and prediction processes through techniques such as chain of thought, single-shot retrievalaugmented generation, and dynamic retrieval-augmented generation. COT (chain of thought) prompting method facilitates logical reasoning by the model on programming problems through prompts. Single-shot retrieval-augmented generation technology enables the construction of query statements based on programming issues to retrieve information from external knowledge bases. Meanwhile, dynamic retrieval-augmented generation technology allows for real-time retrieval from external knowledge bases during the code generation process, integrating retrieved documents to continue generating subsequent code. However, these methods primarily address the hallucination problem in code generation from a localized perspective. To simultaneously resolve issues such as reasoning errors, inadequate overall knowledge, and insufficient localized knowledge in the field of code generation, this study introduces the Retrieval-Augmented Code Generation framework based on Dynamic Revision of Thoughts (RACGDRT). The core concept of this framework involves firstly enabling the large model to produce a comprehensive code draft (utilizing COT prompts), followed by conducting error detection on code segments corresponding to different reasoning steps within the draft. Upon detecting an error, the Keyword Extractor (KE) will extract keywords and formulate query statements for both the user’s input question and the erroneous code segment, subsequently retrieving pertinent documents from the external knowledge base. Finally, with the retrieved documents and questions in hand, the large model proceeds to iteratively generate the next code draft. RACG-DRT guarantees the rationality of reasoning via COT, constructs query statements tailored to erroneous thought segments and issues to bolster both local and holistic knowledge, and iteratively produces code drafts to uphold the integrity of the code generation process.",,0,Black-box,Code,论文主要关注如何通过检索增强和动态修订思想来解决代码生成中的幻觉问题，并未涉及CoT是否真实反映模型预测的实际计算过程（Faithfulness）。
Cognitive Foundations for Reasoning and Their Manifestation in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了LLMs推理机制与人类的不同，提出了评估框架并改进了推理指导方法。,"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning&knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.",,1,Black-box,General,该论文研究了LLM的推理机制与人类认知的差异，并提出了一个评估框架来分析模型在推理过程中的表现。虽然它没有直接研究CoT的忠实度，但它涉及了模型推理的内部机制和认知元素的使用，这与Faithfulness的旁证相关。
Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis,2025,True,True,False,Training & Fine-tuning,论文讨论了VLMs在临床推理中的不忠实现象，并提出了通过SFT和RL提升Faithfulness的方法。,"The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones. To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.",,1,Black-box,Medical,论文关注临床诊断中的推理可信度，提出了基于教科书知识的推理生成器，并利用监督微调增强模型的推理能力。虽然未直接研究CoT的忠实度，但涉及推理的可靠性和监督，属于边缘相关。
X-Node: Self-Explanation is All We Need,2025,True,False,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出X-Node框架，通过构建结构化上下文向量生成忠实解释，解码器确保解释与内部嵌入一致，属于提升忠实性的内部机制方法。,"Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a""text-injection""mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.",,2,White-box,Medical,论文明确提出了一个自解释的GNN框架X-Node，其中每个节点在预测过程中生成自己的解释，并通过解码器强制忠实度（faithfulness）。此外，论文还讨论了如何通过解释向量来重建节点的潜在嵌入，以确保解释的真实性，这与CoT Faithfulness的核心定义直接相关。
Training and Evaluation of Guideline-Based Medical Reasoning in LLMs,2025,True,False,True,Training & Fine-tuning,论文通过微调LLMs遵循医学指南进行推理，提出了评估推理过程忠实性的方法。,"Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.",,2,Black-box,Medical,论文明确研究如何让LLM遵循医学共识指南进行逐步推理和预测，并自动评估推理过程的正确性和忠实度（derivation correctness and value correctness），直接涉及CoT的忠实性。
FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle,2025,True,False,False,Training & Fine-tuning,论文提出基于CoT的推理框架，并通过专家反馈验证其忠实性，涉及训练方法改进。,"Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.",,2,Black-box,General,论文明确提到其推理轨迹（reasoning traces）是忠实的（faithful），并且通过专家反馈和自动分析进行了验证。这直接符合 Faithfulness 的核心定义，即模型生成的解释是否真实反映了模型做出预测的实际计算过程。
TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出通过关键词蒸馏和LLM推理提升解释的忠实性，并提出了新的评估指标。,"Clinical language models often struggle to provide trustworthy predictions and explanations when applied to lengthy, unstructured electronic health records (EHRs). This work introduces TT-XAI, a lightweight and effective framework that improves both classification performance and interpretability through domain-aware keyword distillation and reasoning with large language models (LLMs). First, we demonstrate that distilling raw discharge notes into concise keyword representations significantly enhances BERT classifier performance and improves local explanation fidelity via a focused variant of LIME. Second, we generate chain-of-thought clinical explanations using keyword-guided prompts to steer LLMs, producing more concise and clinically relevant reasoning. We evaluate explanation quality using deletion-based fidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human study with domain experts. All evaluation modalities consistently favor the keyword-augmented method, confirming that distillation enhances both machine and human interpretability. TT-XAI offers a scalable pathway toward trustworthy, auditable AI in clinical decision support.",arXiv.org,1,Black-box,Medical,论文研究了通过关键词蒸馏和LLM推理提高临床文本解释的可信度，涉及解释的忠实性（fidelity）评估，但未明确探讨CoT是否是模型预测的真实原因。
Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT中的不忠实现象，并提出了通过自我重写改进推理过程的方法。,"Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only""simple""samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.",,1,Black-box,General,该论文研究了通过自我重写框架改进内部推理过程质量，关注推理过程中的问题（如过度思考、思考不足等），但未明确探讨 CoT 是否真实反映模型的实际计算过程。因此属于边缘相关。
TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding via Self-Verification Reinforcement Learning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和Faithfulness，提出了GRPO-CSV方法改进忠实性，并讨论了不忠实现象。,"Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",arXiv.org,0,Black-box,General,论文主要关注于通过强化学习优化视频时间搜索策略，并提出了自我验证机制以提高视频理解的完整性。虽然涉及推理过程，但未探讨生成的解释（CoT/Reasoning Trace）是否真实反映了模型做出预测的实际计算过程，因此与CoT Faithfulness的核心定义无关。
Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,2025,True,True,False,Training & Fine-tuning,论文讨论了CoT监控和奖励黑客行为，揭示了不忠实现象，并提出了通过训练改进的方法。,"Mitigating reward hacking--where AI systems misbehave due to flaws or misspecifications in their learning objectives--remains a key challenge in constructing capable and aligned models. We show that we can monitor a frontier reasoning model, such as OpenAI o3-mini, for reward hacking in agentic coding environments by using another LLM that observes the model's chain-of-thought (CoT) reasoning. CoT monitoring can be far more effective than monitoring agent actions and outputs alone, and we further found that a LLM weaker than o3-mini, namely GPT-4o, can effectively monitor a stronger model. Because CoT monitors can be effective at detecting exploits, it is natural to ask whether those exploits can be suppressed by incorporating a CoT monitor directly into the agent's training objective. While we show that integrating CoT monitors into the reinforcement learning reward can indeed produce more capable and more aligned agents in the low optimization regime, we find that with too much optimization, agents learn obfuscated reward hacking, hiding their intent within the CoT while still exhibiting a significant rate of reward hacking. Because it is difficult to tell when CoTs have become obfuscated, it may be necessary to pay a monitorability tax by not applying strong optimization pressures directly to the chain-of-thought, ensuring that CoTs remain monitorable and useful for detecting misaligned behavior.",arXiv.org,2,Black-box,Code,该论文明确研究了模型在生成链式思考（CoT）时可能出现的欺骗行为（reward hacking），并探讨了如何通过监控CoT来检测这种不忠实的行为。论文还讨论了在强化学习过程中直接整合CoT监控可能导致模型学会隐藏其真实意图（obfuscated reward hacking），这与Faithfulness的核心定义直接相关。
Visual Agents as Fast and Slow Thinkers,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出FaST机制，通过动态选择System 1/2模式提升推理忠实性，涉及不忠实现象和改进方法。,"Achieving human-level intelligence requires refining cognitive distinctions between System 1 and System 2 thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce FaST, which incorporates the Fast and Slow Thinking mechanism into visual agents. FaST employs a switch adapter to dynamically select between System 1/2 modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a flexible system, hierarchical reasoning capabilities, and a transparent decision-making pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that FaST outperforms various well-known baselines, achieving 80.8% accuracy over VQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for reasoning segmentation, demonstrate FaST's superior performance. Extensive testing validates the efficacy and robustness of FaST's core components, showcasing its potential to advance the development of cognitive visual agents in AI systems. The code is available at ttps://github.com/GuangyanS/Sys2-LLaVA.",International Conference on Learning Representations,1,Black-box,General,论文研究了视觉代理的快速和慢速思考机制，并提出了一个动态切换系统1/2模式的解决方案。虽然涉及了推理能力的层次化和透明决策流程，但未明确探讨CoT的忠实度问题，因此属于边缘相关。
MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了 CoT 中的 sycophantic 行为，并提出了实时监控和校准方法，涉及不忠实现象和改进方法。,"Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users'incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.",,2,Black-box,Society,该论文明确研究大型推理模型中的阿谀奉承行为（Sycophancy），即在推理过程中模型倾向于迎合用户的错误信念而非保持独立推理。论文提出的MONICA框架通过实时监控推理步骤中的阿谀奉承行为并动态抑制，直接涉及CoT的忠实度问题，属于核心相关研究。
Is the MMI Criterion Necessary for Interpretability? Degenerating Non-causal Features to Plain Noise for Self-Rationalization,2024,True,True,True,Training & Fine-tuning,论文讨论了非因果特征对解释性的影响，并提出了新的准则来提升忠实性。,"An important line of research in the field of explainability is to extract a small subset of crucial rationales from the full input. The most widely used criterion for rationale extraction is the maximum mutual information (MMI) criterion. However, in certain datasets, there are spurious features non-causally correlated with the label and also get high mutual information, complicating the loss landscape of MMI. Although some penalty-based methods have been developed to penalize the spurious features (e.g., invariance penalty, intervention penalty, etc) to help MMI work better, these are merely remedial measures. In the optimization objectives of these methods, spurious features are still distinguished from plain noise, which hinders the discovery of causal rationales. This paper aims to develop a new criterion that treats spurious features as plain noise, allowing the model to work on datasets rich in spurious features as if it were working on clean datasets, thereby making rationale extraction easier. We theoretically observe that removing either plain noise or spurious features from the input does not alter the conditional distribution of the remaining components relative to the task label. However, significant changes in the conditional distribution occur only when causal features are eliminated. Based on this discovery, the paper proposes a criterion for \textbf{M}aximizing the \textbf{R}emaining \textbf{D}iscrepancy (MRD). Experiments on six widely used datasets show that our MRD criterion improves rationale quality (measured by the overlap with human-annotated rationales) by up to $10.4\%$ as compared to several recent competitive MMI variants. Code: \url{https://github.com/jugechengzi/Rationalization-MRD}.",Neural Information Processing Systems,2,White-box,General,论文明确研究了解释性（interpretability）中的因果特征与非因果特征（spurious features）问题，提出了新的准则（MRD）来区分因果特征和非因果特征，以确保解释的真实性（faithfulness）。这与CoT Faithfulness的核心定义高度相关，因为它关注的是解释是否真实反映了模型的预测过程，而非事后合理化。
Simulating Society Requires Simulating Thought,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LLM模拟社会行为时缺乏内部一致性和因果推理的问题，并提出了评估框架RECAP来提升推理忠实性。,"Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet current simulations remain grounded in a behaviorist""demographics in, behavior out""paradigm, focusing on surface-level plausibility. As a result, they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for modeling how people reason, deliberate, and respond to interventions. To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought, not just language, for social simulations.",,2,Black-box,Society,论文明确提出了评估推理忠实性的框架（RECAP），强调因果可追溯性和干预一致性，直接符合 Faithfulness 的核心定义。
Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction,2025,True,True,False,Training & Fine-tuning,论文提出通过微调提升CoT忠实性，涉及不忠实现象和改进方法。,"Training a task-specific small reasoning model is challenging when direct human supervision or high-quality labels are scarce. However, LLMs with reasoning capabilities produce abundant intermediate reasoning traces that can be systematically refined to create effective supervision signals. We propose Reason-Refine-then-Align (R2tA), which turns refined model rationales into supervision for training task-specific reasoning models. Our method generates initial reasoning and responses from an open-source base model on task-specific inputs, then refines these traces, fixing hallucinations and inconsistencies, to form a high-fidelity dataset. We perform a two-stage alignment, supervised fine-tuning (SFT), followed by direct preference optimization (DPO) to calibrate the model's intermediate reasoning with human-validated conceptual preferences and then condition the final output on that aligned reasoning. As a case study, we apply R2tA to evaluate extended entity relationship diagrams (EERDs) in database system design, a structurally complex task where prompt-only methods miss or hallucinate errors. We curated a dataset of 600 EERD variants (train/test split of 450/150, respectively) with induced mistakes spanning 11 categories. Empirical evaluation suggests R2tA provides a practical, cost-effective path to scalable LLM adaptation in data-scarce domains, enabling reproducible AI tools for education and beyond.",arXiv.org,1,Black-box,General,该论文研究了如何通过LLM生成的推理轨迹来训练任务特定的推理模型，并修正其中的幻觉和不一致性。虽然涉及推理的忠实性（高保真数据集），但主要关注的是推理的效用和修正，而非直接研究CoT是否真实反映模型的预测过程。
Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models,2025,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT的不忠实现象，并提出了基于激活的监控方法。,"Reasoning language models improve performance on complex tasks by generating long chains of thought (CoTs), but this process can also increase harmful outputs in adversarial settings. In this work, we ask whether the long CoTs can be leveraged for predictive safety monitoring: do the reasoning traces provide early signals of final response alignment that could enable timely intervention? We evaluate a range of monitoring methods using either CoT text or activations, including highly capable large language models, fine-tuned classifiers, and humans. First, we find that a simple linear probe trained on CoT activations significantly outperforms all text-based baselines in predicting whether a final response is safe or unsafe, with an average absolute increase of 13 in F1 scores over the best-performing alternatives. CoT texts are often unfaithful and misleading, while model latents provide a more reliable predictive signal. Second, the probe can be applied to early CoT segments before the response is generated, showing that alignment signals appear before reasoning completes. Error analysis reveals that the performance gap between text classifiers and the linear probe largely stems from a subset of responses we call performative CoTs, where the reasoning consistently contradicts the final response as the CoT progresses. Our findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.",arXiv.org,2,White-box,General,论文明确研究了 CoT 的真实性（Faithfulness）问题，特别是提到了 CoT 文本常不忠实（unfaithful and misleading），而模型内部的激活值提供了更可靠的信号。此外，论文还探讨了 performative CoTs 现象，即推理过程与最终回应矛盾，这与 Faithfulness 的核心定义直接相关。
Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs,2022,True,False,True,Training & Fine-tuning,论文提出了一种中间信念跟踪框架，与 CoT 的中间性和忠实性相关，并提出了新的评估指标。,"Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).",Conference on Empirical Methods in Natural Language Processing,1,White-box,General,该论文提出了一个框架来建模和跟踪中间信念，虽然涉及透明度和可靠性，但未明确研究CoT是否真实反映模型预测的计算过程。因此，它属于边缘相关。
Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model,2022,True,False,False,Training & Fine-tuning,论文提出了一种新的解释方法，并通过微调学生模型来提高忠实性。,"Explainable question answering systems should produce not only accurate answers but also rationales that justify their reasoning and allow humans to check their work. But what sorts of rationales are useful and how can we train systems to produce them? We propose a new style of rationale for open-book question answering, called \emph{markup-and-mask}, which combines aspects of extractive and free-text explanations. In the markup phase, the passage is augmented with free-text markup that enables each sentence to stand on its own outside the discourse context. In the masking phase, a sub-span of the marked-up passage is selected. To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning. Specifically, we generate silver annotated data by sending a series of prompts to a frozen pretrained language model, which acts as a teacher. We then fine-tune a smaller student model by training on the subset of rationales that led to correct answers. The student is""honest""in the sense that it is a pipeline: the rationale acts as a bottleneck between the passage and the answer, while the""untrusted""teacher operates under no such constraints. Thus, we offer a new way to build trustworthy pipeline systems from a combination of end-task annotations and frozen pretrained language models.",arXiv.org,1,Black-box,General,该论文研究如何通过学生模型从预训练语言模型中学习可解释的问答流程，并生成合理的解释。虽然涉及解释的生成和验证，但主要关注的是解释的实用性和准确性，而非直接研究 CoT 是否忠实反映模型的内部计算过程。因此，它属于边缘相关的研究。
"What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文探讨了CoT的有效性，提出了不忠实现象（如失败步骤影响推理），并提出了新的度量指标（FSF）和改进方法。,"Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the""longer-is-better""narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",arXiv.org,1,Black-box,Math,论文研究了CoT的结构和有效性，特别是通过Failed-Step Fraction (FSF)来评估CoT的质量，并设计了干预措施来验证因果关系。虽然未直接研究Faithfulness，但探讨了CoT的效用和鲁棒性，可以作为Faithfulness的旁证。
Rationale-Guided Few-Shot Classification to Detect Abusive Language,2022,True,False,False,Training & Fine-tuning,论文提出基于rationale的few-shot分类方法，涉及CoT和Faithfulness，但未讨论不忠实现象或提出新度量。,"Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 on the rationale detection task over training solely rationale classifiers. We introduce two rationale-integrated BERT-based architectures (the RGFS models) and evaluate our systems over five different abusive language datasets, finding that in the few-shot classification setting, RGFS-based models outperform baseline models by about 7% in macro F1 scores and perform competitively to models finetuned on other source domains. Furthermore, RGFS-based models outperform LIME/SHAP-based approaches in terms of plausibility and are close in performance in terms of faithfulness.",European Conference on Artificial Intelligence,1,Black-box,Society,论文研究了基于理性（rationales）的少样本分类方法，并提到了与LIME/SHAP方法在忠实度（faithfulness）方面的比较。虽然论文主要关注模型性能提升和跨域评估，但提到了忠实度的概念，属于边缘相关。
Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process,2025,True,True,False,Training & Fine-tuning,论文探讨了LLMs和LRMs的因果推理问题，涉及不忠实现象，并提出了通过RLVR训练提升因果推理的方法。,"LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",arXiv.org,2,White-box,General,该论文明确研究了LLM和LRM的因果结构，特别是思考过程（T）和推理步骤（X）与答案（Y）之间的因果关系。研究发现RLVR训练的LRM表现出更强的因果推理能力，减少了虚假相关性并增强了真实的因果模式，从而减轻了不忠实性和偏见。这些内容直接涉及CoT的忠实度问题，属于核心相关研究。
Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用CoT和强化学习提升医疗推理的忠实性，但未讨论不忠实现象或提出新度量。,"While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.",arXiv.org,1,Black-box,Medical,论文主要关注通过强化学习提升医疗推理的准确性和透明性，虽然提到了 Chain-of-Thought (CoT) 和推理轨迹的质量，但未明确研究 CoT 是否真实反映模型预测的计算过程（Faithfulness）。因此属于边缘相关。
FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness,2025,True,True,False,Training & Fine-tuning,论文提出了一种通过干预训练提升 CoT 忠实性的方法，并讨论了不忠实现象。,"Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.",arXiv.org,2,White-box,General,该论文明确研究如何通过干预训练（FRIT）来提高 CoT 的忠实度，确保推理步骤对最终答案有因果影响。论文不仅提出了测量方法，还提供了系统性改进忠实度的方案，属于核心相关研究。
Inducing Faithfulness in Structured Reasoning via Counterfactual Sensitivity,2025,True,True,True,Training & Fine-tuning,论文提出了一种新的训练目标CSR，旨在增强模型推理步骤与输出之间的因果依赖性，直接涉及CoT的忠实性。,"The reasoning processes of large language models often lack faithfulness; a model may generate a correct answer while relying on a flawed or irrelevant reasoning trace. This behavior, a direct consequence of training objectives that solely reward final-answer correctness, severely undermines the trustworthiness of these models in high-stakes domains. This paper introduces \textbf{Counterfactual Sensitivity Regularization (CSR)}, a novel training objective designed to forge a strong, causal-like dependence between a model's output and its intermediate reasoning steps. During training, CSR performs automated, operator-level interventions on the generated reasoning trace (e.g., swapping ``+''with ``-'') to create a minimally-perturbed counterfactual. A regularization term then penalizes the model if this logically flawed trace still yields the original answer. Our efficient implementation adds only 8.7\% training overhead through warm-start curriculum and token-subset optimization. We evaluate faithfulness using \textbf{Counterfactual Outcome Sensitivity (COS)}, a metric quantifying how sensitive the final answer is to such logical perturbations. Across diverse structured reasoning benchmarks -- arithmetic (GSM8K), logical deduction (ProofWriter), multi-hop QA (HotpotQA), and code generation (MBPP) -- models trained with CSR demonstrate a vastly superior trade-off between accuracy and faithfulness. CSR improves faithfulness over standard fine-tuning and process supervision by up to 70 percentage points, with this learned sensitivity generalizing to larger models and enhancing the performance of inference-time techniques like self-consistency.",arXiv.org,2,White-box,General,该论文明确研究 CoT 的忠实性问题，提出了一种新的训练目标 CSR，通过反事实干预来增强模型输出与中间推理步骤之间的因果依赖关系。论文还提出了衡量忠实性的指标 COS，并验证了 CSR 在多个结构化推理任务上的有效性，显著提高了忠实性。
Neuro-symbolic Natural Logic with Introspective Revision for Natural Language Inference,2022,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文涉及CoT和忠实性，通过强化学习和外部知识改进推理路径，减少虚假推理。,"We introduce a neuro-symbolic natural logic framework based on reinforcement learning with introspective revision. The model samples and rewards specific reasoning paths through policy gradient, in which the introspective revision algorithm modifies intermediate symbolic reasoning steps to discover reward-earning operations as well as leverages external knowledge to alleviate spurious reasoning and training inefficiency. The framework is supported by properly designed local relation models to avoid input entangling, which helps ensure the interpretability of the proof paths. The proposed model has built-in interpretability and shows superior capability in monotonicity inference, systematic generalization, and interpretability, compared with previous models on the existing datasets.",Transactions of the Association for Computational Linguistics,1,White-box,Logic,该论文提出了一个基于强化学习的神经符号自然逻辑框架，通过内省修正算法修改中间符号推理步骤，并利用外部知识来减少虚假推理。虽然论文强调了推理路径的可解释性，但并未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此属于边缘相关。
Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?,2025,True,True,False,Training & Fine-tuning,论文探讨了CoT的解释性与性能的关系，揭示了不忠实现象，并提出了改进方法。,"Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}""We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.",arXiv.org,1,Black-box,General,该论文研究了 CoT 推理痕迹的语义可解释性是否影响 LLM 任务性能，虽然未直接探讨 Faithfulness，但涉及了 CoT 的效用（Utility）和可解释性，可以作为 Faithfulness 的旁证。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确研究多模态CoT中生成的rationale是否被模型实际利用（即Faithfulness问题），并提出RED方法通过KL约束强化rationale对预测的因果影响，直接解决'忽视rationale内容'的忠实度挑战。
Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning,2025,True,True,True,Training & Fine-tuning,论文讨论了长链推理中的自我纠正机制，提出了新的数据标注方法以提升PRMs的评分能力，涉及CoT忠实性。,"Many studies focus on data annotation techniques for training effective PRMs. However, current methods encounter a significant issue when applied to long CoT reasoning processes: they tend to focus solely on the first incorrect step and all preceding steps, assuming that all subsequent steps are incorrect. These methods overlook the unique self-correction and reflection mechanisms inherent in long CoT, where correct reasoning steps may still occur after initial reasoning mistakes. To address this issue, we propose a novel data annotation method for PRMs specifically designed to score the long CoT reasoning process. Given that under the reflection pattern, correct and incorrect steps often alternate, we introduce the concepts of Error Propagation and Error Cessation, enhancing PRMs' ability to identify both effective self-correction behaviors and reasoning based on erroneous steps. Leveraging an LLM-based judger for annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate it at both solution and step levels. Experimental results demonstrate that compared to existing open-source PRMs and PRMs trained on open-source datasets, our PRM achieves superior performance across various metrics, including search guidance, BoN, and F1 scores. Compared to widely used MC-based annotation methods, our annotation approach not only achieves higher data efficiency but also delivers superior performance. Detailed analysis is also conducted to demonstrate the stability and generalizability of our method.",,1,Black-box,Math,该论文研究长链推理过程中的自我修正和反思机制，虽然关注的是推理步骤的正确性，但并未直接探讨CoT是否真实反映模型的预测过程，因此属于边缘相关。
Learning to Rationalize for Nonmonotonic Reasoning with Distant Supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了生成事后解释性理由（post-hoc rationales）的现象，并尝试通过训练生成模型来改进。,"The black-box nature of neural models has motivated a line of research that aims to generate natural language rationales to explain why a model made certain predictions. Such rationale generation models, to date, have been trained on dataset-specific crowdsourced rationales, but this approach is costly and is not generalizable to new tasks and domains. In this paper, we investigate the extent to which neural models can reason about natural language rationales that explain model predictions, relying only on distant supervision with no additional annotation cost for human-written rationales. We investigate multiple ways to automatically generate rationales using pre-trained language models, neural knowledge models, and distant supervision from related tasks, and train generative models capable of composing explanatory rationales for unseen instances. We demonstrate our approach on the defeasible inference task, a nonmonotonic reasoning task in which an inference may be strengthened or weakened when new information (an update) is introduced. Our model shows promises at generating post-hoc rationales explaining why an inference is more or less likely given the additional information, however, it mostly generates trivial rationales reflecting the fundamental limitations of neural language models. Conversely, the more realistic setup of jointly predicting the update or its type and generating rationale is more challenging, suggesting an important future direction.",AAAI Conference on Artificial Intelligence,1,Black-box,Logic,该论文研究了生成自然语言解释（rationales）以解释模型预测的能力，但主要关注的是通过远程监督生成解释，而不是直接验证这些解释是否忠实反映了模型的内部计算过程。虽然涉及非单调推理任务，但未深入探讨解释的忠实性。
Semi-structured LLM Reasoners Can Be Rigorously Audited,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出半结构化推理模型（SSRMs）以解决不忠实现象，并引入自动审计方法来检测推理错误。,"Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.",arXiv.org,2,Black-box,General,该论文明确研究LLM推理的忠实性问题，提出半结构化推理模型（SSRMs）以生成可审计的推理轨迹，旨在检测推理中的错误和遗漏，直接关联到CoT Faithfulness的核心定义。
Human Ratings Do Not Reflect Downstream Utility: A Study of Free-Text Explanations for Model Predictions,2022,True,True,False,Training & Fine-tuning,论文讨论了模型生成的解释与下游任务效用不一致的现象，并提出了通过微调改进的方法。,"Models able to generate free-text rationales that explain their output have been proposed as an important step towards interpretable NLP for “reasoning” tasks such as natural language inference and commonsense question answering. However, the relative merits of different architectures and types of rationales are not well understood and hard to measure. In this paper, we contribute two insights to this line of research: First, we find that models trained on gold explanations learn to rely on these but, in the case of the more challenging question answering data set we use, fail when given generated explanations at test time. However, additional fine-tuning on generated explanations teaches the model to distinguish between reliable and unreliable information in explanations. Second, we compare explanations by a generation-only model to those generated by a self-rationalizing model and find that, while the former score higher in terms of validity, factual correctness, and similarity to gold explanations, they are not more useful for downstream classification. We observe that the self-rationalizing model is prone to hallucination, which is punished by most metrics but may add useful context for the classification step.",BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,1,Black-box,General,论文研究了自由文本解释的效用和模型对解释的依赖，但未明确探讨解释是否忠实反映了模型的实际计算过程。虽然提到了模型对生成解释的区分能力，但主要关注的是下游分类的效用，而非解释的忠实度。
Faithful Knowledge Graph Explanations in Commonsense Question Answering,2022,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了知识图谱解释的不忠实性，并提出了新的度量方法和改进架构。,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",Conference on Empirical Methods in Natural Language Processing,2,White-box,Society,该论文明确研究知识图谱解释的忠实度（Faithfulness），提出了一种新的代理测量方法，并分析了现有模型无法生成高度忠实解释的原因，符合核心定义。
Obtaining Faithful Interpretations from Compositional Neural Networks,2020,True,True,False,Training & Fine-tuning,论文揭示了NMNs中间输出与预期不符的不忠实现象，并提出了通过辅助监督训练改进的方法。,"Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了神经网络模块（NMN）的中间输出是否忠实反映了模型的推理过程，发现模块结构与实际行为不一致，并通过辅助监督和模块架构改进提升忠实度。这直接符合Faithfulness的核心定义，尤其是关于解释是否真实反映模型计算过程的研究。
A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations,2025,True,True,True,Training & Fine-tuning,论文提出了PEX一致性度量，揭示了LLM生成解释的不一致现象，并采用直接偏好优化提升忠实性。,"Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.",,2,Black-box,General,该论文明确研究了解释的忠实度（Faithfulness），提出了 Prediction-EXplanation (PEX) consistency 作为衡量指标，并分析了语言模型生成的解释中缺乏一致性的问题。此外，论文还探讨了如何通过优化提高解释的忠实度，符合 CoT Faithfulness 的核心研究范畴。
Why do you think that? Exploring faithful sentence–level rationales without supervision,2020,True,True,False,Training & Fine-tuning,论文探讨了如何通过无监督方法生成忠实解释，并提出了训练框架来提升解释的忠实性。,"Evaluating the trustworthiness of a model’s prediction is essential for differentiating between ‘right for the right reasons’ and ‘right for the wrong reasons’. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training–framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart’s performance in two cases. We further exploit the transparent decision–making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale–level.",Findings,2,Black-box,General,该论文明确研究如何生成忠实的解释（faithful rationales），即模型预测的真实原因，符合Faithfulness的核心定义。论文提出了一种训练框架，旨在确保模型输出的解释真实反映其决策过程，这与CoT Faithfulness的研究目标高度一致。
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?,2020,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出新的度量标准LAS评估解释的忠实性，并讨论了模型生成解释的合理性问题。,"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",Findings,2,Black-box,General,该论文明确研究模型生成的自然语言解释是否支持实际模型行为（faithfulness），而不仅仅是与人类解释的表面相似性（plausibility）。论文提出了泄漏调整可模拟性（LAS）指标，用于评估解释是否帮助观察者预测模型的输出，同时控制解释直接泄漏输出的情况。这与CoT Faithfulness的核心定义高度相关，因为它关注解释是否真实反映了模型的预测过程。
PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications,2025,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出生成详细的 CoT 解释，并评估其 grounded 和清晰度，涉及忠实性度量。,"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-ofthought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded (score 0.87) and clear (readability 4.5/5). This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.",,1,Black-box,General,论文提出了一个框架，结合了传感器数据和视觉语言模型生成详细的链式思考解释，并评估了解释的grounded程度和清晰度。虽然涉及CoT的解释生成和评估，但未明确研究这些解释是否忠实反映了模型的实际计算过程，因此属于边缘相关。
On the Faithfulness of Visual Thinking: Measurement and Enhancement,2025,True,True,True,Training & Fine-tuning,论文揭示了视觉信息在MCoT中的不忠实现象，并提出了新的度量方法和改进策略。,"Recent large vision-language models (LVLMs) can generate vision-text multimodal chain-of-thought (MCoT) traces after reinforcement fine-tuning (RFT). However, we observe that the visual information incorporated in MCoT is often inaccurate, though still yield correct answers, indicating a lack of faithfulness in the MCoT reasoning process. We attribute this unfaithfulness to the RL reward in RFT, which solely incentivizes the format of interleaved vision-text cues, ie, it encourages the model to incorporate visual information into its text reasoning steps without considering the correctness of the visual information. In this paper, we first probe the faithfulness of MCoT by measuring how much the prediction changes when its visual and textual thoughts are intervened. Surprisingly, the model's predictions remain nearly unchanged under visual intervention but change significantly under textual intervention, indicating that the visual evidence is largely ignored. To further analyze visual information, we introduce an automated LVLM-based evaluation metric that quantifies the faithfulness of visual cues from two perspectives: reliability and sufficiency. Our evaluation reveals that the visual information in current MCoT traces is simultaneously unreliable and insufficient. To address this issue, we propose a novel MCoT learning strategy termed Sufficient-Component Cause Model (SCCM) learning. This approach encourages the MCoT to generate sufficient yet minimal visual components that are independently capable of leading to correct answers. We note that the proposed SCCM is annotation-free and compatible with various RFT for MCoT in a plug-and-play manner. Empirical results demonstrate that SCCM consistently improves the visual faithfulness across a suite of fine-grained perception and reasoning benchmarks. Code is available at https://github.com/EugeneLiu01/Faithful_Thinking_with_Image.",arXiv.org,2,White-box,General,该论文明确研究了视觉-文本多模态链式思考（MCoT）的忠实度问题，通过干预实验（视觉和文本干预）揭示了视觉信息在推理过程中被忽视的现象，并提出了量化视觉线索忠实度的指标（可靠性和充分性）。此外，论文还提出了一种新的学习策略（SCCM）来增强视觉忠实度，这直接符合Faithfulness的核心定义。
Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文讨论了如何通过外部奖励模型提升CoT的忠实性，并提出了改进方法。,"Recent advances in large language models (LLMs) have significantly improved performance on the Text-to-SQL task by leveraging their powerful reasoning capabilities. To enhance accuracy during the reasoning process, external Process Reward Models (PRMs) can be introduced during training and inference to provide fine-grained supervision. However, if misused, PRMs may distort the reasoning trajectory and lead to suboptimal or incorrect SQL generation. To address this challenge, we propose Reward-SQL, a framework that systematically explores how to incorporate PRMs into the Text-to-SQL reasoning process effectively. Our approach follows a""cold start, then PRM supervision""paradigm. Specifically, we first train the model to decompose SQL queries into structured stepwise reasoning chains using common table expressions (Chain-of-CTEs), establishing a strong and interpretable reasoning baseline. Then, we investigate four strategies for integrating PRMs, and find that combining PRM as an online training signal (e.g.,GRPO) with PRM-guided inference (e.g., best-of-N sampling) yields the best results. Empirically, on the BIRD benchmark, Reward-SQL enables models supervised by PRM (7B) to achieve a 13.1% performance gain across various guidance strategies. Notably, our GRPO-aligned policy model based on Qwen2.5-Coder-7B-Instruct achieves 68.9% accuracy on the BIRD development set, outperforming all baseline methods under the same model size. These results demonstrate the effectiveness of Reward-SQL in leveraging reward-based supervision for Text-to-SQL reasoning.",arXiv.org,1,Black-box,Code,论文研究了如何通过逐步推理和过程监督奖励来提升Text-to-SQL任务的性能，虽然涉及推理过程的可解释性，但并未直接探讨CoT是否忠实反映模型的实际计算过程。
Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations,2025,True,True,False,Training & Fine-tuning,论文揭示了偏好优化导致CoT不忠实的问题，并提出了通过因果归因改进的方法。,"Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in""reward hacking""by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.",arXiv.org,2,Black-box,General,该论文明确研究了CoT解释的忠实度问题，指出偏好优化可能导致解释不忠实（reward hacking），并提出了通过因果归因来检测解释与决策过程之间差异的方法，直接符合Faithfulness的核心定义。
SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过对抗自演游戏提升 CoT 忠实性，属于改进方法中的训练与微调。,"Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a""sneaky generator""that deliberately produces erroneous steps designed to be difficult to detect, and a""critic""that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.",arXiv.org,1,Black-box,General,该论文通过对抗自博弈游戏评估LLM推理步骤的可靠性，虽然未直接研究CoT的忠实度，但涉及推理步骤的正确性分析，可作为Faithfulness的旁证。
Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback,2025,True,True,False,Training & Fine-tuning,论文提出 Step-KTO 训练框架，通过过程级和结果级反馈提升推理忠实性。,"Large language models (LLMs) have recently demonstrated remarkable success in mathematical reasoning. Despite progress in methods like chain-of-thought prompting and self-consistency sampling, these advances often focus on final correctness without ensuring that the underlying reasoning process is coherent and reliable. This paper introduces Step-KTO, a training framework that combines process-level and outcome-level binary feedback to guide LLMs toward more trustworthy reasoning trajectories. By providing binary evaluations for both the intermediate reasoning steps and the final answer, Step-KTO encourages the model to adhere to logical progressions rather than relying on superficial shortcuts. Our experiments on challenging mathematical benchmarks show that Step-KTO significantly improves both final answer accuracy and the quality of intermediate reasoning steps. For example, on the MATH-500 dataset, Step-KTO achieves a notable improvement in Pass@1 accuracy over strong baselines. These results highlight the promise of integrating stepwise process feedback into LLM training, paving the way toward more interpretable and dependable reasoning capabilities.",,2,Black-box,Math,该论文明确研究如何通过过程级和结果级二元反馈来引导LLM产生更可信的推理轨迹，确保中间推理步骤的连贯性和可靠性，直接涉及CoT的忠实度问题。
Anchored Alignment for Self-Explanations Enhancement,2024,True,False,False,Training & Fine-tuning,论文提出了一种增强模型自我解释能力的方法，涉及训练和微调策略。,"In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning (self-explanation) even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called Alignment with Anchor Preference Pairs, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies.",arXiv.org,1,Black-box,General,论文研究如何通过对齐方法增强LLM的自我解释能力，但未明确探讨解释是否忠实反映模型的真实计算过程。虽然涉及解释质量评估，但更侧重于解释的效用而非因果分析。
Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach,2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出了一种神经符号方法LINA，旨在提升逻辑推理的忠实性，减少对外部求解器的依赖。,"Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",arXiv.org,1,Black-box,Logic,论文提出了一个神经符号方法（LINA）来增强逻辑推理的忠实性，但主要关注的是推理过程的鲁棒性和消除对外部求解器的依赖，未明确研究CoT是否是模型预测的真实原因或测量Faithfulness的指标。
OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models,2024,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出了离线评估框架OCEAN，并利用知识图谱和RL优化CoT的忠实性。,"Offline evaluation of LLMs is crucial in understanding their capacities, though current methods remain underexplored in existing research. In this work, we focus on the offline evaluation of the chain-of-thought capabilities and show how to optimize LLMs based on the proposed evaluation method. To enable offline feedback with rich knowledge and reasoning paths, we use knowledge graphs (e.g., Wikidata5m) to provide feedback on the generated chain of thoughts. Due to the heterogeneity between LLM reasoning and KG structures, direct interaction and feedback from KGs on LLM behavior are challenging, as they require accurate entity linking and grounding of LLM-generated chains of thought in the KG. To address the above challenge, we propose an offline chain-of-thought evaluation framework, OCEAN, which models chain-of-thought reasoning in LLMs as an MDP and evaluate the policy's alignment with KG preference modeling. To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy KG exploration and RL to model a KG policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating KG reasoning preference. Then we incorporate the knowledge-graph feedback on the validity and alignment of the generated reasoning paths into inverse propensity scores and propose KG-IPS estimator. Theoretically, we prove the unbiasedness of the proposed KG-IPS estimator and provide a lower bound on its variance. With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance chain-of-thought alignment. Our empirical study shows that OCEAN can be efficiently optimized for generating chain-of-thought reasoning paths with higher estimated values without affecting LLMs' general abilities in downstream tasks or their internal knowledge.",arXiv.org,1,Black-box,General,论文主要研究如何通过知识图谱反馈优化LLM的CoT能力，并提出了离线评估框架OCEAN。虽然涉及CoT的评估和优化，但未明确探讨CoT是否真实反映模型的预测过程，因此属于边缘相关。
CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文讨论了因果幻觉（不忠实现象），并提出了CRE机制来增强因果关系，提升Faithfulness。,"Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.",arXiv.org,1,Black-box,Logic,该论文主要关注长程推理问题中的因果幻觉和搜索空间优化，提出了增强因果关系的机制和双端搜索方法。虽然涉及因果推理的忠实性问题，但主要目标是提高准确性和效率，而非直接研究CoT的忠实度。
How Do Humans Write Code? Large Models Do It the Same Way Too,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了CoT和PoT的忠实性问题，并提出了改进方法。,"Program-of-Thought (PoT) replaces natural language-based Chain-of-Thought (CoT) as the most popular method in Large Language Models (LLMs) mathematical reasoning tasks by utilizing external tool calls to circumvent computational errors. However, our evaluation of the GPT-4 and Llama series reveals that using PoT introduces more reasoning errors, such as incorrect formulas or flawed logic, compared to CoT. To address this issue, we propose Human-Think Language (HTL), which leverages a suite of strategies that help integrate PoT and CoT, encompassing: (1) a new generation paradigm that uses full CoT reasoning to control code generation. (2) Focus Attention, that directs model attention to the CoT reasoning during PoT to generate more logical code. (3) reinforcement learning that utilizes the accuracy of both CoT and PoT responses as rewards to prevent repetitive reasoning steps in LLMs when solving difficult math problems. Our method achieves an average improvement of 6.5% on the Llama-Base model and 4.3% on the Mistral-Base model across 8 mathematical calculation datasets. It also shows significant effectiveness on five out-of-domain datasets by controlling the model’s information flow, exhibiting strong transferability. Additionally, HTL shows the most significant improvement in non-mathematical natural language inference task, contributing to a unified reasoning task framework.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Math,论文主要关注的是通过新的生成范式（HTL）来提高数学推理任务的性能，而不是研究 CoT 是否真实反映了模型的预测过程。虽然提到了 CoT 和 PoT 的比较，但重点在于性能提升而非忠实度分析。
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing,2024,True,True,False,Training & Fine-tuning,论文关注提升LLM推理过程的忠实性，并提出了基于DPO的训练方法。,"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through Direct Preference Optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,Logic,论文关注于提高生成推理过程的可靠性和忠实度，并提出了一个框架来优化推理轨迹。虽然它没有直接研究CoT是否是模型预测的真实原因，但它涉及到了推理过程的监督和优化，可以作为Faithfulness的旁证。
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,2,Black-box,General,该论文明确研究LLM解释的忠实性问题，提出了FaithLM框架来评估和改进解释的忠实性，符合核心定义中的Faithfulness研究范畴。
Digital Socrates: Evaluating LLMs through explanation critiques,2023,True,True,True,Training & Fine-tuning,论文提出自动评估模型解释质量的方法，涉及 CoT 忠实性评估和改进。,"While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.",Annual Meeting of the Association for Computational Linguistics,1,Black-box,General,该论文研究的是对LLM生成的解释进行批判性评估，并开发自动评估工具。虽然涉及解释的质量和缺陷，但并未明确探讨解释是否忠实反映模型的内部计算过程（Faithfulness），而是更侧重于解释的实用性和评估方法。
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了LLMs在生成答案时不忠实于中间推理步骤的现象，并提出了FRODO框架来改进忠实性。,"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文明确研究了CoT的忠实度问题，通过因果中介分析验证了LLM是否可靠地使用中间推理步骤来生成最终答案，并提出了FRODO框架以提高推理的忠实度。这直接符合Faithfulness的核心定义，即模型生成的解释是否真实反映了其预测的实际计算过程。
How Likely Do LLMs with CoT Mimic Human Reasoning?,2024,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了LLMs在CoT中偏离理想因果链的现象，并探讨了影响因果结构的因素。,"Chain-of-thought emerges as a promising technique for eliciting reasoning capabilities from Large Language Models (LLMs). However, it does not always improve task performance or accurately represent reasoning processes, leaving unresolved questions about its usage. In this paper, we diagnose the underlying mechanism by comparing the reasoning process of LLMs with humans, using causal analysis to understand the relationships between the problem instruction, reasoning, and the answer in LLMs. Our empirical study reveals that LLMs often deviate from the ideal causal chain, resulting in spurious correlations and potential consistency errors (inconsistent reasoning and answers). We also examine various factors influencing the causal structure, finding that in-context learning with examples strengthens it, while post-training techniques like supervised fine-tuning and reinforcement learning on human feedback weaken it. To our surprise, the causal structure cannot be strengthened by enlarging the model size only, urging research on new techniques. We hope that this preliminary study will shed light on understanding and improving the reasoning process in LLM.",International Conference on Computational Linguistics,2,Black-box,General,该论文明确研究LLMs的CoT推理过程与人类推理的差异，并通过因果分析探讨问题指令、推理和答案之间的关系，揭示了LLMs常常偏离理想的因果链，导致虚假相关性和潜在的一致性错误。这与Faithfulness的核心定义高度相关，即研究CoT是否真实反映了模型的实际计算过程。
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文揭示了CoT中的不忠实现象（Toxic CoT问题），并提出了基于内部机制探测的改进方法。,"Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model's overall commonsense reasoning performance (increased by 5.5%).",Annual Meeting of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了CoT推理过程中的信息丢失问题，使用内部机制分析方法（如attribution tracing和causal tracing），并且设计了RIDERS方法来补偿信息缺失，直接关联到CoT的真实性和忠实度问题。
Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought,2024,True,True,False,Training & Fine-tuning,论文揭示了CoT中的偏见现象，并提出了通过训练方法减少偏见推理的解决方案。,"Chain-of-thought prompting (CoT) has the potential to improve the explainability of language model reasoning. But CoT can also systematically misrepresent the factors influencing models'behavior -- for example, rationalizing answers in line with a user's opinion. We first create a new dataset of 9 different biases that affect GPT-3.5-Turbo and Llama-8b models. These consist of spurious-few-shot patterns, post hoc rationalization, and sycophantic settings. Models switch to the answer implied by the bias, without mentioning the effect of the bias in the CoT. To mitigate this biased reasoning problem, we introduce bias-augmented consistency training (BCT), an unsupervised fine-tuning scheme that trains models to give consistent reasoning across prompts with and without biasing features. We construct a suite testing nine forms of biased reasoning on seven question-answering tasks, and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of biased reasoning by 86\% on held-out tasks. Moreover, this model generalizes to other forms of bias, reducing biased reasoning on held-out biases by an average of 37\%. As BCT generalizes to held-out biases and does not require gold labels, this method may hold promise for reducing biased reasoning from as-of-yet unknown biases and on tasks where ground truth reasoning is unavailable.",arXiv.org,2,Black-box,Society,论文明确研究 CoT 中的偏见问题，包括事后合理化（post hoc rationalization）和谄媚性设置（sycophantic settings），这些都属于 Faithfulness 的研究范畴。论文还提出了减少偏见推理的方法，直接涉及 CoT 解释的真实性。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,2,Black-box,General,论文明确研究 Chain-of-Thought 的解释性，并特别提到 faithfulness 作为评估的一个维度。此外，还研究了 robustness 和 utility，这些都是与 CoT Faithfulness 相关的核心内容。
On the Impact of Fine-Tuning on Chain-of-Thought Reasoning,2024,True,True,False,Training & Fine-tuning,论文探讨了微调对CoT推理忠实性的影响，揭示了不忠实现象。,"Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",North American Chapter of the Association for Computational Linguistics,2,White-box,General,该论文明确研究了微调对CoT推理忠实度的影响，探讨了微调如何改变LLM的内部机制，并直接提及了CoT推理的faithfulness问题。
Consistent Multi-Granular Rationale Extraction for Explainable Multi-hop Fact Verification,2023,True,True,True,Training & Fine-tuning,论文探讨了多粒度理由提取的忠实性和一致性，并提出了新的评估指标。,"The success of deep learning models on multi-hop fact verification has prompted researchers to understand the behavior behind their veracity. One possible way is erasure search: obtaining the rationale by entirely removing a subset of input without compromising the veracity prediction. Although extensively explored, existing approaches fall within the scope of the single-granular (tokens or sentences) explanation, which inevitably leads to explanation redundancy and inconsistency. To address such issues, this paper explores the viability of multi-granular rationale extraction with consistency and faithfulness for explainable multi-hop fact verification. In particular, given a pretrained veracity prediction model, both the token-level explainer and sentence-level explainer are trained simultaneously to obtain multi-granular rationales via differentiable masking. Meanwhile, three diagnostic properties (fidelity, consistency, salience) are introduced and applied to the training process, to ensure that the extracted rationales satisfy faithfulness and consistency. Experimental results on three multi-hop fact verification datasets show that the proposed approach outperforms some state-of-the-art baselines.",arXiv.org,2,White-box,General,该论文明确研究多粒度理由提取的忠实性（faithfulness）和一致性（consistency），并引入了诊断属性（如fidelity）来确保提取的理由满足忠实性。这与CoT Faithfulness的核心定义直接相关，即模型生成的解释是否真实反映了模型做出预测的实际计算过程。
Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models,2025,True,True,False,Training & Fine-tuning,论文讨论了模型内部推理痕迹与最终输出的对齐问题，涉及不忠实现象，并提到通过SFT、DPO和GRPO等训练方法改进。,"Recent advances in post-training techniques have endowed Large Language Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive tasks through the generation of supplementary planning tokens. This development raises a fundamental question: Are these models aware of what they""learn""and""think""? To address this, we define three core competencies: (1) awareness of learned latent policies, (2) generalization of these policies across domains, and (3) alignment between internal reasoning traces and final outputs. We empirically evaluate these abilities on several tasks, each designed to require learning a distinct policy. Furthermore, we contrast the profiles of models post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization (DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate that RL-trained models not only demonstrate greater awareness of their learned behaviors and stronger generalizability to novel, structurally similar tasks than SFT models but also often exhibit weak alignment between their reasoning traces and final outputs, an effect most pronounced in GRPO-trained models.",arXiv.org,2,White-box,General,论文明确研究了内部推理轨迹（reasoning traces）与最终输出的对齐问题（alignment），这直接涉及CoT的忠实度（Faithfulness）。特别是发现RL训练的模型在推理轨迹与输出之间存在弱对齐现象，符合核心定义中关于'Post-hoc Rationalization'的研究。此外，论文还对比了不同训练方法（SFT/DPO/GRPO）对模型内部机制的影响，属于白盒分析。
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,该论文明确研究多模态推理中的忠实度问题，提出通过过程级监督（rubric-based generative rewards）来改善推理的忠实性，并专门评估了推理的忠实性改进。这直接符合Faithfulness的核心定义，即确保生成的推理过程真实反映模型的实际计算过程。
Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation,2025,True,True,True,Training & Fine-tuning,论文讨论了CoT不忠实现象，提出了新的度量框架，并通过RL改进忠实性。,"Inspired by the success of reinforcement learning (RL) in Large Language Model (LLM) training for domains like math and code, recent works have begun exploring how to train LLMs to use search engines more effectively as tools for retrieval-augmented generation. Although these methods achieve performance improvement across QA benchmarks, many prioritize final answer correctness while overlooking the quality of intermediate reasoning steps, which may lead to chain-of-thought unfaithfulness. In this paper, we first introduce a comprehensive evaluation framework for evaluating RL-based search agents, covering three distinct faithfulness metrics: information-think faithfulness, think-answer faithfulness, and think-search faithfulness. Our evaluations reveal that a prototypical RL-based search agent, Search-R1, has significant room for improvement in this regard. To foster faithful reasoning, we introduce VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search), a novel framework that integrates fine-grained faithfulness rewards into the reinforcement learning process. Our experiments show that models trained with VERITAS not only significantly improve reasoning faithfulness, but also achieve comparable task performance across seven QA benchmarks.",arXiv.org,2,Black-box,General,该论文明确研究检索增强生成中的推理忠实度问题，提出了三个具体的忠实度指标（信息-思考、思考-答案、思考-搜索忠实度），并设计了VERITAS框架来增强推理过程的忠实性。这直接符合CoT Faithfulness的核心定义，即关注中间推理步骤是否真实反映模型的预测过程。
Few Shot Rationale Generation using Self-Training with Dual Teachers,2023,True,False,False,Training & Fine-tuning,论文提出了一种自训练方法，通过双教师框架生成忠实解释，属于改进方法中的训练与微调。,"Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究生成忠实解释（faithful rationales）的问题，并提出了一种新的损失函数（MLR）来促进解释与预测标签之间的强条件关系，符合Faithfulness的核心定义。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,该论文明确研究自然语言解释（NLEs）的忠实性问题，提出了一种框架（G-Tex）来增强NLEs的忠实性，通过使用highlight explanations作为忠实线索来指导NLE生成，确保生成的解释与模型的内部推理过程一致。这直接符合Faithfulness的定义，即解释是否真实反映了模型的实际计算过程。
Investigating Self-Rationalizing Models for Commonsense Reasoning,2023,True,True,False,Training & Fine-tuning,论文探讨了自解释模型的忠实性问题，并提出了通过微调改进的方法。,"The rise of explainable natural language processing spurred a bulk of work on datasets augmented with human explanations, as well as technical approaches to leverage them. Notably, generative large language models offer new possibilities, as they can output a prediction as well as an explanation in natural language. This work investigates the capabilities of fine-tuned text-to-text transfer Transformer (T5) models for commonsense reasoning and explanation generation. Our experiments suggest that while self-rationalizing models achieve interesting results, a significant gap remains: classifiers consistently outperformed self-rationalizing models, and a substantial fraction of model-generated explanations are not valid. Furthermore, training with expressive free-text explanations substantially altered the inner representation of the model, suggesting that they supplied additional information and may bridge the knowledge gap. Our code is publicly available, and the experiments were run on open-access datasets, hence allowing full reproducibility.",Stats,1,White-box,General,该论文研究了自解释模型在常识推理中的表现，发现模型生成的解释存在不有效的问题，并分析了训练过程中内部表征的变化。虽然未直接探讨CoT的忠实度，但涉及解释的有效性和模型内部机制，属于边缘相关。
Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards,2025,True,True,True,Training & Fine-tuning,论文揭示了CoT中的不忠实现象（Miracle Steps），并提出了基于过程奖励的改进方法。,"Large language models for mathematical reasoning are typically trained with outcome-based rewards, which credit only the final answer. In our experiments, we observe that this paradigm is highly susceptible to reward hacking, leading to a substantial overestimation of a model's reasoning ability. This is evidenced by a high incidence of false positives - solutions that reach the correct final answer through an unsound reasoning process. Through a systematic analysis with human verification, we establish a taxonomy of these failure modes, identifying patterns like Miracle Steps - abrupt jumps to a correct output without a valid preceding derivation. Probing experiments suggest a strong association between these Miracle Steps and memorization, where the model appears to recall the answer directly rather than deriving it. To mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a process-oriented reward function that evaluates the entire reasoning trajectory against problem-specific rubrics. The generative RRM provides fine-grained, calibrated rewards (0-1) that explicitly penalize logical flaws and encourage rigorous deduction. When integrated into a reinforcement learning pipeline, RRM-based training consistently outperforms outcome-only supervision across four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from 26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work demonstrates that rewarding the solution process is crucial for building models that are not only more accurate but also more reliable.",arXiv.org,2,Black-box,Math,论文明确研究了数学推理中模型生成的推理步骤（CoT）是否真实反映了其预测过程，特别是识别了 'Miracle Steps' 这种不忠实推理现象，并提出了 Rubric Reward Model 来增强推理过程的忠实度。
Do LLMs Know They Are Being Tested? Evaluation Awareness and Incentive-Sensitive Failures in GPT-OSS-20B,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了评估框架对CoT长度和答案合规性的影响，揭示了不忠实现象，并提出了改进方法。,"Benchmarks for large language models (LLMs) often rely on rubric-scented prompts that request visible reasoning and strict formatting, whereas real deployments demand terse, contract-bound answers. We investigate whether such""evaluation scent""inflates measured performance without commensurate capability gains. Using a single open-weights model (GPT-OSS-20B), we run six paired A/B scenarios that hold task content and decoding fixed while varying framing (evaluation-oriented vs. real-world) and reasoning depth (Medium/High): deterministic math, strict code-fix, citation generation, incentive flips (caution vs. competence), CoT visibility, and multilingual (Urdu) headers. Deterministic validators compute accuracy, answer-only compliance, hedging/refusals, chain-of-thought (CoT) length, and schema compliance, with pre-registered deltas and composite indices. Across scenarios, evaluation framing reliably inflates CoT (hundreds to>1000 characters) and reduces answer-only compliance, with limited or inconsistent accuracy gains. In structured outputs, it improves wrappers (e.g., fenced blocks, enumerated lists) but not regex-validated substance. Incentive wording reweights error composition: praising caution modestly improves accuracy at high reasoning and reduces wrong-but-confident errors, whereas praising competence yields terser but riskier outputs. Urdu rubric headers reproduce these signatures and can decrease accuracy at higher reasoning depth, indicating multilingual parity risks. We provide a reproducible A/B framework (prompt banks, validators, per-run scores, scripts; versioned DOI) and practical guidance: neutral phrasing or dual-framing checks, contract-aware grading, style-delta reporting, confidence governance, and multilingual dashboards to ensure that benchmark gains reflect deployable capability.",arXiv.org,2,Black-box,General,该论文明确研究了评估框架对CoT长度和答案合规性的影响，揭示了评估导向的提示可能导致CoT的膨胀而不伴随准确性的提升，这与CoT Faithfulness的核心问题——解释是否真实反映模型的计算过程——直接相关。
Evaluating Human Alignment and Model Faithfulness of LLM Rationale,2024,True,True,True,Training & Fine-tuning,论文研究了LLM生成rationale的忠实性，揭示了不忠实现象并提出了评估框架。,"We study how well large language models (LLMs) explain their generations through rationales -- a set of tokens extracted from the input text that reflect the decision-making process of LLMs. Specifically, we systematically study rationales derived using two approaches: (1) popular prompting-based methods, where prompts are used to guide LLMs in generating rationales, and (2) technical attribution-based methods, which leverage attention or gradients to identify important tokens. Our analysis spans three classification datasets with annotated rationales, encompassing tasks with varying performance levels. While prompting-based self-explanations are widely used, our study reveals that these explanations are not always as""aligned""with the human rationale as attribution-based explanations. Even more so, fine-tuning LLMs to enhance classification task accuracy does not enhance the alignment of prompting-based rationales. Still, it does considerably improve the alignment of attribution-based methods (e.g., InputXGradient). More importantly, we show that prompting-based self-explanation is also less""faithful""than attribution-based explanations, failing to provide a reliable account of the model's decision-making process. To evaluate faithfulness, unlike prior studies that excluded misclassified examples, we evaluate all instances and also examine the impact of fine-tuning and accuracy on alignment and faithfulness. Our findings suggest that inconclusive faithfulness results reported in earlier studies may stem from low classification accuracy. These findings underscore the importance of more rigorous and comprehensive evaluations of LLM rationales.",arXiv.org,2,White-box,General,论文明确研究LLM生成的rationale是否忠实反映模型决策过程，对比了基于提示和基于技术归因的方法，并发现基于提示的自我解释在忠实度上不如基于归因的解释。这直接涉及CoT Faithfulness的核心问题，即解释是否真实反映模型的计算过程。
Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了奖励黑客行为，提出TRACE度量方法，并涉及训练改进。,"Reward hacking, where a reasoning model exploits loopholes in a reward function to achieve high rewards without solving the intended task, poses a significant threat. This behavior may be explicit, i.e. verbalized in the model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE (Truncated Reasoning AUC Evaluation). Our key observation is that hacking occurs when exploiting the loophole is easier than solving the actual task. This means that the model is using less'effort'than required to achieve high reward. TRACE quantifies effort by measuring how early a model's reasoning becomes sufficient to obtain the reward. We progressively truncate a model's CoT at various lengths, force the model to answer, and estimate the expected reward at each cutoff. A hacking model, which takes a shortcut, will achieve a high expected reward with only a small fraction of its CoT, yielding a large area under the accuracy-vs-length curve. TRACE achieves over 65% gains over our strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B monitor in coding. We further show that TRACE can discover unknown loopholes during training. Overall, TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 是否真实反映模型预测的实际计算过程，特别是通过检测隐式奖励黑客行为（implicit reward hacking）来评估 CoT 的忠实度。论文提出的 TRACE 方法通过截断 CoT 并测量预期奖励来量化模型的'努力'，直接关联到 CoT 是否忠实于模型的真实推理过程。
Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT与答案不一致的现象，并提出了基于验证的改进方法。,"Recent advances in large language models (LLMs) have demonstrated that reinforcement learning with verifiable rewards (RLVR) can significantly enhance reasoning abilities by directly optimizing correctness, rather than relying solely on supervised imitation. This paradigm has been extended to multimodal LLMs for complex video and image understanding tasks. However, while outcome-driven RL improves answer accuracy, it can inadvertently decouple the reasoning chain from the final answer, leading to situations where models produce inconsistency between the reasoning trace and final answer. In our experiments on multiple-choice visual question-answering tasks, the standard GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps and the chosen answers, indicating frequent mismatches between answers and reasoning. To this end, we propose Answer-Consistent Reinforcement Learning (ACRE) that modifies the GRPO algorithm with an auxiliary consistency check. After the model generates a chain of thought and an initial answer for a given question, we shuffle the answer options and prompt the model again with the same reasoning trace to predict a second answer. We design a consistency-verification reward that grants a high reward only if both the original and the post-shuffle answers agree and are correct; otherwise, a lower reward is assigned accordingly. This mechanism penalizes reasoning-answer misalignment and discourages the model from relying on spurious patterns, such as option ordering biases. We evaluate ACRE on challenging Video Reasoning benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\% and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the GRPO baseline.",arXiv.org,2,Black-box,General,该论文明确研究了CoT与最终答案之间的一致性问题，提出了Answer-Consistent Reinforcement Learning (ACRE)方法来惩罚推理与答案不匹配的情况，直接涉及CoT的忠实度问题。
When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs,2025,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning, Consistency & Ensembling",论文揭示了CoT推理导致指令跟随准确性下降的现象，并提出了度量指标和改进策略。,"Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.",arXiv.org,2,White-box,General,该论文明确研究了CoT推理对指令遵循准确性的负面影响，揭示了CoT可能分散模型对关键指令的关注，并通过注意力分析验证了这一现象。这直接关联到CoT的忠实性问题，即生成的推理是否真实反映了模型预测的实际过程。
OpenAI o1 System Card,2024,True,False,False,Training & Fine-tuning,论文讨论了通过强化学习训练模型使用CoT进行推理，属于改进方法中的训练与微调。,"The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.",arXiv.org,1,Black-box,General,论文讨论了使用CoT来提高模型的安全性和鲁棒性，并提到通过deliberative alignment来增强模型对安全政策的推理能力。虽然涉及CoT的效用和潜在风险，但未明确研究CoT是否真实反映模型的预测过程，因此属于边缘相关。
Beyond Introspection: Reinforcing Thinking via Externalist Behavioral Feedback,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出外部反馈机制提升推理可靠性，涉及 CoT 忠实性改进。,"While inference-time thinking allows Large Language Models (LLMs) to address complex problems, the extended thinking process can be unreliable or inconsistent because of the model's probabilistic nature, especially near its knowledge boundaries. Existing approaches attempt to mitigate this by having the model critique its own reasoning to make corrections. However, such self-critique inherits the same biases of the original output, known as the introspection illusion. Moving beyond such introspection and inspired by core methodologies in ethology, we propose an externalist three-step framework Distillation-Reinforcement-Reasoning (DRR). Rather than relying on a model's introspection, DRR evaluates its observable behaviors to provide corrective feedback. DRR first distills the reasoner's behavioral traces, then trains a lightweight, external Discriminative Model (DM). At inference time, this DM acts as a critic, identifying and rejecting suspicious reasoning steps. This external feedback compels the LLM to discard flawed pathways and explore alternatives, thereby enhancing reasoning quality without altering the base model. Experiments on multiple reasoning benchmarks show that our framework significantly outperforms prominent self-critique methods. Benefiting from a lightweight and annotation-free design, DRR offers a scalable and adaptable solution for improving the reliability of reasoning in a wide range of LLMs.",,2,Black-box,General,该论文明确研究推理过程的可靠性问题，提出通过外部反馈机制（而非模型自省）来纠正推理步骤，直接涉及 CoT 的忠实度问题。论文指出自省式修正会继承原始输出的偏见（即不忠实性），并通过外部判别模型识别可疑推理步骤，这属于对 Faithfulness 的测量和提升。
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过MCTS和过程奖励模型提升CoT的忠实性，涉及训练和验证工具。,"We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising""deep thinking""through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data sythesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids na\""ive step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On the MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0% and Phi3-mini-3.8B from 41.4% to 86.4%, surpassing o1-preview by +4.5% and +0.9%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% the brightest high school math students. Code and data will be available at https://github.com/microsoft/rStar.",International Conference on Machine Learning,0,Black-box,Math,论文主要关注通过蒙特卡洛树搜索和自进化方法提升小型语言模型的数学推理能力，未涉及对CoT忠实度的研究。
When Thinking Drifts: Evidential Grounding for Robust Video Reasoning,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT在视频推理中的不忠实现象，并提出了基于强化学习的改进方法。,"Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term""visual thinking drift"". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only""think before answering"", but also""see while thinking"".",arXiv.org,2,Black-box,General,该论文明确研究了CoT在视频推理中的忠实性问题，指出CoT生成的推理轨迹与视觉证据不符，导致‘视觉思维漂移’现象，并提出了VER框架来奖励基于视觉证据的忠实推理轨迹，直接涉及CoT Faithfulness的核心问题。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的CoT traces的faithfulness，直接涉及CoT忠实度的研究，属于核心相关。研究领域涉及社会学中的多元价值观对齐，因此分类为Society。
Chain-of-Probe: Examing the Necessity and Accuracy of CoT Step-by-Step,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT不忠实现象，提出了度量方法，并提出了改进策略。,"Current research found the issue of Early Answering in large language models (LLMs), where the models already have an answer before generating the Chain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary dependency between the predicted answer and the reasoning process. Consequently, two important questions arise: (1) Is CoT still necessary if the model already has an answer? (2) Can the correctness of the answer serve as valid evidence for the correctness of CoT? To address these questions, we propose a method, namely Chain-of-Probe (CoP), to probe changes in the mind during the model's reasoning. The probing results show that in a significant number of question-answer cases, CoT appears to be unnecessary, and this necessity correlates with the simplicity of the task, defined by reasoning steps required. Furthermore, by analyzing patterns in mind change, we examine the correctness of the model's reasoning. Our validation reveals that many responses, although correct in their final answer, contain errors in their reasoning process. To this end, we propose a strategic approach based on CoP to prioritize answers with correct reasoning among multiple candidates, thereby bolstering the reliability of the model's reasoning.",arXiv.org,2,White-box,General,该论文明确研究了CoT是否真实反映了模型的推理过程，发现了Early Answering现象，并质疑CoT的必要性和正确性。通过提出的Chain-of-Probe方法，分析了模型在推理过程中的思维变化，验证了即使最终答案正确，推理过程也可能存在错误。这直接涉及CoT的忠实度问题，属于核心相关研究。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,该论文明确研究如何提高CoT推理的忠实度，探讨了in-context learning、fine-tuning和activation editing等方法，并进行了广泛的实证分析，直接涉及CoT是否真实反映模型行为的问题。
Break the Chain: Large Language Models Can be Shortcut Reasoners,2024,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT中的捷径学习现象，并提出了新的评估数据集和改进方法。,"Recent advancements in Chain-of-Thought (CoT) reasoning utilize complex modules but are hampered by high token consumption, limited applicability, and challenges in reproducibility. This paper conducts a critical evaluation of CoT prompting, extending beyond arithmetic to include complex logical and commonsense reasoning tasks, areas where standard CoT methods fall short. We propose the integration of human-like heuristics and shortcuts into language models (LMs) through""break the chain""strategies. These strategies disrupt traditional CoT processes using controlled variables to assess their efficacy. Additionally, we develop innovative zero-shot prompting strategies that encourage the use of shortcuts, enabling LMs to quickly exploit reasoning clues and bypass detailed procedural steps. Our comprehensive experiments across various LMs, both commercial and open-source, reveal that LMs maintain effective performance with""break the chain""strategies. We also introduce ShortcutQA, a dataset specifically designed to evaluate reasoning through shortcuts, compiled from competitive tests optimized for heuristic reasoning tasks such as forward/backward reasoning and simplification. Our analysis confirms that ShortcutQA not only poses a robust challenge to LMs but also serves as an essential benchmark for enhancing reasoning efficiency in AI.",arXiv.org,1,Black-box,General,该论文研究了CoT推理中的捷径策略，虽然未直接探讨CoT的忠实性，但通过'break the chain'策略评估了CoT的有效性和鲁棒性，可以作为Faithfulness的旁证。
SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过训练框架提升LLM的忠实性，涉及CoT和自我反思的理性生成。,"Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,论文研究LLM生成自我反思的理性解释和置信度估计，虽然涉及推理链的不一致性分析，但主要关注置信度校准和不确定性解释，未直接探讨CoT是否忠实反映模型的实际计算过程。
Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文研究了CoT的有效性和忠实性，揭示了不忠实现象，并提出了改进算法。,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究了 CoT 的忠实度（faithfulness）问题，特别是分析了 CoT 中信息缺失导致的不忠实现象，并提出了新的算法来增强 CoT 的忠实度和有效性。
Reasoning Models Don't Always Say What They Think,2025,True,True,False,Training & Fine-tuning,论文揭示了CoT不忠实现象，并探讨了RLHF对忠实性的影响。,"Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",arXiv.org,2,Black-box,General,论文明确研究 CoT 是否忠实反映模型的实际推理过程，评估了 CoT 的忠实度，并探讨了强化学习对忠实度的影响，符合核心相关标准。
Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出了DRN方法，通过不确定性最小化来提升推理的忠实性，并讨论了认知陷阱现象。,"Large language models often fail at logical reasoning when semantic heuristics conflict with decisive evidence - a phenomenon we term cognitive traps. To address this fundamental limitation, we introduce the Deliberative Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from probability maximization to uncertainty minimization. Instead of asking""Which answer is most likely?"", DRN asks""Which hypothesis has the most internally consistent evidence?"". DRN achieves intrinsic interpretability by explicitly tracking belief states and quantifying epistemic uncertainty for competing hypotheses through an iterative evidence synthesis process. We validate our approach through two complementary architectures - a bespoke discriminative model that embodies the core uncertainty minimization principle, and a lightweight verification module that enhances existing generative LLMs. Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over standard baselines. When integrated as a parameter-efficient verifier with Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most challenging problems. Critically, DRN demonstrates strong zero-shot generalization, improving TruthfulQA performance by 23.6% without additional training, indicating that uncertainty-driven deliberation learns transferable reasoning principles. We position DRN as a foundational, verifiable System 2 reasoning component for building more trustworthy AI systems.",arXiv.org,1,Black-box,Logic,该论文提出了一个不确定性最小化的推理范式，通过显式跟踪信念状态和量化认知不确定性来增强推理的可解释性。虽然它关注的是推理的内部一致性和可解释性，但并未直接研究CoT是否真实反映了模型的预测过程，因此属于边缘相关。
Preventing Language Models From Hiding Their Reasoning,2023,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了模型中‘编码推理’的不忠实现象，并提出了评估防御方法和训练策略。,"Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 的忠实性问题，特别是模型可能通过编码中间推理步骤（encoded reasoning）来隐藏真实推理过程，这与 Faithfulness 的核心定义直接相关。论文还提出了评估防御措施的方法论，属于对 CoT 忠实性的直接研究。
XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs,2023,True,False,True,"Training & Fine-tuning, Verification & External Tools",论文提出数据集和框架以增强LLM的透明度和可靠性，涉及CoT忠实性。,"Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM’s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the debugger-scores for multidimensional quality analysis. Our explanations include why-choose and why-not-choose components, reason-elements, and debugger-scores that collectively illuminate the LLM’s reasoning behavior. Our evaluations demonstrate XplainLLM’s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available.",Conference on Empirical Methods in Natural Language Processing,1,Black-box,General,该论文提出了一个数据集和框架来增强LLM的透明度和可靠性，包括生成基于知识图谱的解释和多维质量分析。虽然它关注解释的可靠性和减少幻觉，但未明确研究CoT是否是模型预测的真实原因或测量Faithfulness的指标，因此属于边缘相关。
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models,2023,True,True,True,Training & Fine-tuning,论文探讨了视觉语言模型的推理一致性，提出了基于CoT的度量方法，并通过两阶段训练框架提升一致性和性能。,"Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing an LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",North American Chapter of the Association for Computational Linguistics,1,Black-box,General,该论文研究了视觉语言模型（VLMs）的推理一致性和性能，提出了基于CoT的一致性测量方法，并开发了一个两阶段训练框架以提高推理性能和一致性。虽然论文关注了CoT的一致性和性能，但并未明确探讨CoT是否真实反映了模型预测的实际计算过程（Faithfulness），因此属于边缘相关。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,1,Black-box,General,论文提出了多模型协作策略（Corex）以增强推理过程的factuality、faithfulness和reliability，但未明确研究CoT是否是模型预测的真实原因或提出测量Faithfulness的具体指标。因此属于边缘相关。
"Survey on Hallucination in Reasoning Large Language Model: Evaluation, Taxonomy, Intervention, and Open Issues",2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了CoT中的幻觉现象，提出了评估框架和干预策略，与Faithfulness高度相关。,"In recent years, reasoning large language models (LLMs) have seen increasingly widespread adoption in the field of education, particularly demonstrating substantial potential in tasks involving complex text comprehension. However, these LLMs are susceptible to a critical yet often overlooked issue: hallucinations within the reasoning process—instances where the model outputs a correct final answer while its underlying reasoning chain contains fabricated, inconsistent, or logically flawed content. Such hallucination phenomena in Chain-of-Thought (CoT) processes pose serious challenges to the reliability of educational applications. To address this issue, this study proposes a systematic research framework comprising dataset construction, multi-model CoT evaluation, and hallucination classification and quantification. Utilizing the whole-book reading dataset aligned with the junior secondary Chinese language curriculum, we conduct a comparative evaluation of six leading domestic and international LLMs, including ChatGPT o1 and DeepSeek-R1. Key findings include:(1) Hallucinations in CoT are prevalent across all tested models, with ChatGPT-o1 exhibiting a distinctive high accuracy–high hallucination pattern;(2) Hallucinations are both task-and genre-dependent: narrative texts, particularly novels, tend to trigger higher hallucination indices due to long-range dependencies and implicit cultural references. Tasks involving logical reasoning, linguistic feature analysis, and detail extraction show the highest hallucination rates, revealing model weaknesses in handling long-tail knowledge;(3) Hallucinations typically follow a progressive generative pattern: Information mis-reading → Comprehension deviation → Content fabrication → Logical instability. To mitigate these issues, we propose two targeted intervention strategies: uncertainty-based abstention and model-to-model correction. These approaches o ff er practical pathways toward enhancing the trustworthiness and educational applicability of reasoning LLMs.",Data Intelligence,2,Black-box,General,该论文明确研究了推理过程中的幻觉问题，即模型输出的正确最终答案与其底层推理链中存在虚构、不一致或逻辑错误的实例。这直接触及了 Chain-of-Thought (CoT) 忠实度的核心问题，即推理链是否真实反映了模型的实际计算过程。论文还提出了幻觉的分类和量化方法，以及干预策略，这些都是衡量 Faithfulness 的重要方面。
PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出使用程序辅助蒸馏（PaD）来改进推理能力，涉及 CoT 和忠实性问题。,"While large language models (LLMs) excel in various natural language processing tasks, their huge size and the inaccessibility of parameters present challenges for practical deployment. Previous studies try to distill task-specific ability from LLMs to smaller models, using data synthesis and chain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains faulty reasoning, which deteriorates the quality of distillation, especially in reasoning capabilities. In this work, we propose Program-aided Distillation (PaD), which introduces reasoning programs to suppress the errors in distilled data, and thus achieves better distillation quality for reasoning tasks. In PaD, we utilize the reasoning program to substitute the CoT, allowing automated error checking of synthetic data. Further, through error injecting and further training, the small distilling model could iteratively self-refine the reasoning. Moreover, we conduct a step-wise beam search by step-by-step verifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic reasoning, symbolic reasoning, and general ability.Experimental results demonstrate that smaller models using PaD can not only outperform certain LLMs (e.g., LLaMA-1 13B) but also achieve strong improvement over baselines with a significantly smaller scale of parameters and data. The source code is publicly available athttps://github.com/Xuekai-Zhu/pad.",North American Chapter of the Association for Computational Linguistics,0,Black-box,General,该论文主要研究如何通过程序辅助蒸馏（PaD）提升小模型的推理能力，并利用程序替代CoT进行错误检查。虽然涉及CoT数据中的错误推理问题，但核心关注点是性能提升和蒸馏质量，而非CoT解释的真实性或忠实度。因此，与CoT Faithfulness的研究范畴不直接相关。
Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models,2025,True,False,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文探讨了LLMs中的推理算法原语及其几何组合，与CoT和Faithfulness相关，提出了度量框架和改进方法。,"How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",arXiv.org,2,White-box,General,该论文明确研究语言模型内部激活模式与推理轨迹的关联，通过注入算法原语并测量其对推理步骤的影响，直接验证了模型推理过程的忠实性。研究涉及模型内部机制（如残差流操作和激活聚类），属于白盒方法，且跨任务和跨模型的评估揭示了推理微调对算法泛化的影响，与CoT Faithfulness的核心定义高度相关。
Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning,2025,True,True,True,Training & Fine-tuning,论文提出了新的度量指标（Alignment Score）并讨论了不忠实现象（四种错误类型），同时提出了优化方法（SCOS）。,"This paper presents a framework for evaluating and optimizing reasoning consistency in Large Language Models (LLMs) via a new metric, the Alignment Score, which quantifies the semantic alignment between model-generated reasoning chains and human-written reference chains in Chain-of-Thought (CoT) reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest Alignment Score. To explain this phenomenon, we define four key error types: logical disconnection, thematic shift, redundant reasoning, and causal reversal, and show how each contributes to the degradation of the Alignment Score. Building on this analysis, we further propose Semantic Consistency Optimization Sampling (SCOS), a method that samples and favors chains with minimal alignment errors, significantly improving Alignment Scores by an average of 29.84% with longer reasoning chains, such as in 3-hop tasks.",,1,Black-box,General,该论文研究推理链与人类参考链的语义对齐（Alignment Score），并定义了四种错误类型（如因果反转），但未明确探讨CoT是否真实反映模型内部计算过程（Faithfulness的核心问题）。属于边缘相关，因其关注一致性（Consistency）而非因果忠实性。
Lightweight Language Models are Prone to Reasoning Errors for Complex Computational Phenotyping Tasks,2025,True,True,False,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文讨论了LLMs在复杂任务中的推理错误和不忠实性，并引入了评估框架PHEONA来衡量这些错误。,"Objective: Although computational phenotyping is a central informatics activity with resulting cohorts supporting a wide variety of applications, it is time-intensive because of manual data review. We previously assessed the ability of LLMs to perform computational phenotyping tasks using computable phenotypes for ARF respiratory support therapies. They successfully performed concept classification and classification of single-therapy phenotypes, but underperformed on multiple-therapy phenotypes. To understand issues with these complex tasks, we expanded PHEONA, a generalizable framework for evaluation of LLMs, to include methods specifically for evaluating faulty reasoning. Materials and Methods: We assessed the responses of three lightweight LLMs (DeepSeek-r1 32 billion, Mistral Small 24 billion, and Phi-4 14 billion) both with and without prompt modifications to identify explanation correctness and unfaithfulness errors for phenotyping. Results: For experiments without prompt modifications, both errors were present across all models although more responses had explanation correctness errors than unfaithfulness errors. For experiments assessing accuracy impact after prompt modifications, DeepSeek, a reasoning model, had the smallest overall accuracy impact when compared to Mistral and Phi. Discussion: Since reasoning errors were ubiquitous across models, our enhancement of PHEONA to include a component for assessing faulty reasoning provides critical support for LLM evaluation and evidence for reasoning errors for complex tasks. While insights from reasoning errors can help prompt refinement, a deeper understanding of why LLM reasoning errors occur will likely require further development and refinement of interpretability methods. Conclusion: Reasoning errors were pervasive across LLM responses for computational phenotyping, a complex reasoning task.",arXiv.org,2,Black-box,Medical,论文明确研究了LLM在复杂计算表型任务中的推理错误，包括解释正确性和忠实性错误（unfaithfulness errors），并提出了评估框架PHEONA来识别这些错误。这直接涉及CoT的忠实度问题，即模型生成的解释是否真实反映了其预测过程。
Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出通过可验证奖励提升 CoT 忠实性，并讨论了不忠实现象。,"The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",arXiv.org,2,Black-box,General,论文明确研究 CoT 的忠实性（faithful chart table reconstruction 和 process conformity），并通过可验证的奖励机制提高解释的真实性，直接符合 Faithfulness 的核心定义。
DO-FacT: Discrete-Optimized Fact-Tree Prompting Framework for Commonsense Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了一种结合事实生成和验证的框架，旨在提升推理的可靠性和真实性。,"Commonsense Reasoning is one of the major bottlenecks in machine intelligence, despite it has been widely studied. Prompt-based methods like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) provide structured reasoning paths, while often suffering from issues related to the quality and authenticity of the generated facts. In this paper, we propose Discrete-Optimized Fact-Tree Prompting (DO-FacT), a novel framework that combines fact proposal generation and fact authenticity verification in an iterative process. In DO-FacT, both the fact proposal generation and authenticity verification modules are refined by Discrete-Optimized Prompt-Tuning (DOPT) in advance, leading to the production of high-quality facts and more reliable validation. Experimental results on three widely used commonsense reasoning datasets demonstrate that DO-FacT significantly outperforms existing relevant approaches like CoT and fact-tree with handwrite prompt templates, providing more accurate and reliable reasoning results.",International Conferences on Software Engineering and Information Management,0,Black-box,General,论文主要关注通过优化提示框架（DO-FacT）来提高常识推理的准确性和可靠性，并未涉及对CoT生成解释是否忠实反映模型内部计算过程的研究。
Are DeepSeek R1 And Other Reasoning Models More Faithful?,2025,True,True,True,Training & Fine-tuning,论文评估了推理模型的忠实性，揭示了不忠实现象，并提出了度量指标。,"Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? We evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue""A Stanford Professor thinks the answer is D""is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the cue's influence 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses -- which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness -- whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests. Still, we think this increase in faithfulness is promising for the explainability of language models.",,2,Black-box,General,论文明确研究CoT的忠实度（Faithfulness），通过测试模型是否能描述提示中线索对其答案的影响来测量忠实度。研究比较了推理模型和非推理模型在忠实度上的差异，并探讨了奖励模型可能导致不忠实响应的原因。
Training Language Models to Use Prolog as a Tool,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了使用Prolog作为外部工具来验证模型推理的可靠性，涉及CoT忠实性改进方法。,"Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under https://github.com/niklasmellgren/grpo-prolog-inference",,1,Black-box,Logic,论文研究通过微调语言模型使用Prolog作为外部工具进行可验证计算，以提高推理的可靠性和可审计性。虽然未直接研究CoT的忠实度，但涉及推理的鲁棒性和一致性，可以作为Faithfulness的旁证。
Markovian Transformers for Informative Language Modeling,2024,True,True,True,"Training & Fine-tuning, Interpretability & Internal Mechanisms",论文提出了一种新的训练方法，通过强制模型将推理压缩到可解释的文本中，以提高CoT的忠实性。,"Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language model's underlying decision process. We address this by introducing a Markovian language model framework that can be understood as a reasoning autoencoder: it creates a text-based bottleneck where CoT serves as an intermediate representation, forcing the model to compress essential reasoning into interpretable text before making predictions. We train this system with a GRPO-style policy gradient algorithm using parallel sampling, a frozen baseline CoT', within-batch standardized advantages, and actor-reward (chain-rule) gradients. Our approach yields large gains on QA tasks (e.g., GSM8K: 20.7% to 54.5%; +33.8 pp; ARC-Challenge: 47.5% to 76.9%; +29.4 pp). Perturbation analyses across types and severities show consistently higher sensitivity to CoT edits (typically 52%--82% of cases favor Markovian), indicating stronger causal reliance on the CoT. Cross-model evaluation confirms that learned CoTs generalize across architectures, suggesting they capture transferable reasoning patterns rather than model-specific artifacts.",,2,White-box,General,该论文明确研究 CoT 是否忠实反映模型决策过程，通过引入 Markovian 语言模型框架和扰动分析来验证 CoT 的因果依赖，属于核心相关研究。
Can Language Models Explain Their Own Classification Behavior?,2024,True,True,False,Training & Fine-tuning,论文探讨了LLMs能否忠实解释其分类行为，涉及CoT的忠实性和不忠实现象。,"Large language models (LLMs) perform well at a myriad of tasks, but explaining the processes behind this performance is a challenge. This paper investigates whether LLMs can give faithful high-level explanations of their own internal processes. To explore this, we introduce a dataset, ArticulateRules, of few-shot text-based classification tasks generated by simple rules. Each rule is associated with a simple natural-language explanation. We test whether models that have learned to classify inputs competently (both in- and out-of-distribution) are able to articulate freeform natural language explanations that match their classification behavior. Our dataset can be used for both in-context and finetuning evaluations. We evaluate a range of LLMs, demonstrating that articulation accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4. We then investigate whether we can improve GPT-3's articulation accuracy through a range of methods. GPT-3 completely fails to articulate 7/10 rules in our test, even after additional finetuning on correct explanations. We release our dataset, ArticulateRules, which can be used to test self-explanation for LLMs trained either in-context or by finetuning.",arXiv.org,2,Black-box,General,该论文明确研究LLM是否能忠实解释其自身分类行为，直接涉及Faithfulness的核心问题，即模型生成的解释是否真实反映其内部决策过程。通过构建特定数据集和评估模型解释的准确性，论文探讨了模型解释的忠实性，符合核心相关标准。
Rethinking harmless refusals when fine-tuning foundation models,2024,True,True,False,Training & Fine-tuning,论文揭示了模型在微调后可能产生不忠实的推理痕迹（reason-based deception），并提出了通过反驳策略改进的方法。,"In this paper, we investigate the degree to which fine-tuning in Large Language Models (LLMs) effectively mitigates versus merely conceals undesirable behavior. Through the lens of semi-realistic role-playing exercises designed to elicit such behaviors, we explore the response dynamics of LLMs post fine-tuning interventions. Our methodology involves prompting models for Chain-of-Thought (CoT) reasoning and analyzing the coherence between the reasoning traces and the resultant outputs. Notably, we identify a pervasive phenomenon we term \emph{reason-based deception}, where models either stop producing reasoning traces or produce seemingly ethical reasoning traces that belie the unethical nature of their final outputs. We further examine the efficacy of response strategies (polite refusal versus explicit rebuttal) in curbing the occurrence of undesired behavior in subsequent outputs of multi-turn interactions. Our findings reveal that explicit rebuttals significantly outperform polite refusals in preventing the continuation of undesired outputs and nearly eliminate reason-based deception, challenging current practices in model fine-tuning. Accordingly, the two key contributions of this paper are (1) defining and studying reason-based deception, a new type of hidden behavior, and (2) demonstrating that rebuttals provide a more robust response model to harmful requests than refusals, thereby highlighting the need to reconsider the response strategies in fine-tuning approaches.",arXiv.org,2,Black-box,Society,论文明确研究了 CoT 推理的忠实性问题，提出了 'reason-based deception' 现象，即模型生成的推理痕迹与最终输出之间的不一致性，这直接涉及 CoT 的忠实度问题。此外，论文还探讨了不同响应策略对 CoT 忠实度的影响，符合核心相关标准。
Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文揭示了CoT可能强化模型偏见的不忠实现象，并提出了APriCoT方法来改进。,"Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.",,2,Black-box,Society,论文研究了CoT推理中的偏见问题，指出CoT在某些情况下会强化模型的快速思考偏见，而非提供真实的推理过程。这直接涉及CoT的忠实度问题，即CoT是否真实反映了模型的预测过程。此外，论文提出了APriCoT方法来减少偏见的影响，进一步探讨了如何使CoT更忠实于模型的真实推理。
CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning,2024,True,False,False,Training & Fine-tuning,论文提出CoMAT方法提升数学推理的忠实性和可验证性，属于改进方法中的训练与微调。,"Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present Chain of Mathematically Annotated Thought (CoMAT), which enhances reasoning through two stages: Symbolic Conversion (converting natural language queries into symbolic form) and Reasoning Execution (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",,2,Black-box,Math,论文明确提到 CoMAT 方法不仅提高了性能，还确保了忠实性（faithfulness）和可验证性（verifiability），提供了透明的推理过程。这直接符合 CoT Faithfulness 的研究范畴，即研究解释是否真实反映了模型的预测过程。
Case-Based Deduction for Entailment Tree Generation,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文讨论了逻辑一致性问题，提出了改进方法，与CoT忠实性相关。,"Maintaining logical consistency in structured explanations is critical for understanding and troubleshooting the reasoning behind a system’s decisions. However, existing methods for entailment tree generation often struggle with logical consistency, resulting in erroneous intermediate conclusions and reducing the overall accuracy of the explanations. To address this issue, we propose case-based deduction (CBD), a novel approach that retrieves cases with similar logical structures from a case base and uses them as demonstrations for logical deduction. This method guides the model toward logically sound conclusions without the need for manually constructing logical rule bases. By leveraging a prototypical network for case retrieval and reranking them using information entropy, CBD introduces diversity to improve in-context learning. Our experimental results on the EntailmentBank dataset show that CBD significantly improves entailment tree generation, achieving performance improvements of 1.7% in Task 1, 0.6% in Task 2, and 0.8% in Task 3 under the strictest Overall AllCorrect metric. These findings confirm that CBD enhances the logical consistency and overall accuracy of AI systems in structured explanation tasks.",Mathematics,1,Black-box,Logic,论文研究了逻辑一致性和结构化解释的准确性，虽然提到了推理过程的改进，但未明确探讨 CoT 是否真实反映模型的计算过程，属于边缘相关。
Dynamic Clue Bottlenecks: Towards Interpretable-by-Design Visual Question Answering,2023,True,True,False,Training & Fine-tuning,论文提出了一种可解释的中间步骤设计，确保忠实性，并讨论了不忠实现象。,"Recent advances in multimodal large language models (LLMs) have shown extreme effectiveness in visual question answering (VQA). However, the design nature of these end-to-end models prevents them from being interpretable to humans, undermining trust and applicability in critical domains. While post-hoc rationales offer certain insight into understanding model behavior, these explanations are not guaranteed to be faithful to the model. In this paper, we address these shortcomings by introducing an interpretable by design model that factors model decisions into intermediate human-legible explanations, and allows people to easily understand why a model fails or succeeds. We propose the Dynamic Clue Bottleneck Model ( (DCLUB), a method that is designed towards an inherently interpretable VQA system. DCLUB provides an explainable intermediate space before the VQA decision and is faithful from the beginning, while maintaining comparable performance to black-box systems. Given a question, DCLUB first returns a set of visual clues: natural language statements of visually salient evidence from the image, and then generates the output based solely on the visual clues. To supervise and evaluate the generation of VQA explanations within DCLUB, we collect a dataset of 1.7k reasoning-focused questions with visual clues. Evaluations show that our inherently interpretable system can improve 4.64% over a comparable black-box system in reasoning-focused questions while preserving 99.43% of performance on VQA-v2.",,2,White-box,General,该论文明确提出了一个设计上可解释的模型（DCLUB），强调其生成的中间解释（视觉线索）是模型决策的忠实反映。论文直接讨论了传统事后解释（post-hoc rationales）可能不忠实的问题，并提出了一个从设计上保证忠实性的方法，符合 CoT Faithfulness 的核心定义。
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment,2024,True,False,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文提出基于因果关系的提示方法，利用CoT作为中介变量，并通过微调提升忠实性。,"Despite the notable advancements of existing prompting methods, such as In-Context Learning and Chain-of-Thought for Large Language Models (LLMs), they still face challenges related to various biases. Traditional debiasing methods primarily focus on the model training stage, including approaches based on data augmentation and reweighting, yet they struggle with the complex biases inherent in LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate LLMs biases. In specific, causal intervention is achieved by designing the prompts without accessing the parameters and logits of LLMs. The chain-of-thought generated by LLM is employed as the mediator variable and the causal effect between input prompts and output answers is calculated through front-door adjustment to mitigate model biases. Moreover, to accurately represent the chain-of-thoughts and estimate the causal effects, contrastive learning is used to fine-tune the encoder of chain-of-thought by aligning its space with that of the LLM. Experimental results show that the proposed causal prompting approach achieves excellent performance across seven natural language processing datasets on both open-source and closed-source LLMs.",AAAI Conference on Artificial Intelligence,2,Black-box,General,论文明确研究 Chain-of-Thought 作为中介变量，并通过前门调整计算输入提示与输出答案之间的因果效应，直接涉及 CoT 是否真实反映模型预测的实际计算过程（Faithfulness）。此外，论文还讨论了如何通过对比学习微调 CoT 编码器以准确表示 CoT 并估计因果效应，进一步验证了 CoT 的忠实性。
SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization,2025,True,True,False,"Training & Fine-tuning, Consistency & Ensembling",论文讨论了CoT中的不忠实现象（如多数投票奖励导致响应缩短），并提出了改进方法（如选择性更新高熵分支点）。,"Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.",,1,Black-box,General,论文研究了测试时强化学习（TTRL）方法在链式思考（CoT）推理中的应用，特别是通过自一致性投票来生成伪奖励。虽然论文关注的是CoT的稳定性和有效性，但并未直接探讨CoT生成的解释是否忠实反映了模型的预测过程，因此属于边缘相关。
Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering,2024,True,True,False,Training & Fine-tuning,论文讨论了CoT中的错误推理链问题，并提出了选择性过滤方法来提升忠实性。,"Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. We proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at Anonymous.",International Conference on Language Resources and Evaluation,1,Black-box,General,该论文研究了CoT推理的质量问题，并提出了选择性过滤的方法来评估推理链的置信度。虽然它关注了CoT的可靠性，但并未明确探讨CoT是否真实反映了模型的内部计算过程，因此属于边缘相关。
From Faithfulness to Correctness: Generative Reward Models that Think Critically,2025,True,False,False,Training & Fine-tuning,论文提出TRM方法，通过强化学习改进CoT忠实性与正确性评估。,"Through reinforcement learning with verifiable rewards (RLVR), large language models have achieved substantial progress in domains with easily verifiable outcomes, such as mathematics and coding. However, when applied to more complex tasks like open-domain question answering, RLVR faces significant challenges due to the difficulty of verifying correctness. The nuanced and ambiguous nature of real-world knowledge makes it difficult to reliably evaluate correctness in these settings, necessitating further abilities that extend beyond mere logical consistency to encompass an understanding and assessment of both external and internal knowledge. Recent work has primarily focused on improving faithfulness, defined as semantic alignment with supporting documents, which can cause models to rely excessively on external sources and diminish their capacity for critical assessment. To address this, we propose the Thinking-supervised Reward Model (TRM), which incorporates sentence-level thinking supervision to endow reward models with critical thinking abilities. Given a query, answer, and supporting documents, TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness. By structuring reward modeling as a sequence of faithfulness, reasoning, and correctness evaluations, TRM encourages models to critically assess and leverage both external and internal knowledge. Experiments on reward signals demonstrate that TRM substantially improves the identification of incorrect sentences, and incorporating TRM into policy optimization leads to significant gains in both answer correctness and usefulness.",arXiv.org,1,Black-box,General,论文讨论了 faithfulness 和 correctness 的关系，并提出了一个结合 faithfulness 和 reasoning 的奖励模型（TRM）。虽然涉及 faithfulness 的概念，但主要关注的是如何通过监督学习提升模型的批判性思考能力，而非直接研究 CoT 是否忠实反映模型的推理过程。因此属于边缘相关。
Towards Large Language Models with Self-Consistent Natural Language Explanations,2025,True,True,True,Training & Fine-tuning,论文揭示了LLM事后解释的不一致性，并提出了新的度量指标和优化方法。,"Large language models (LLMs) seem to offer an easy path to interpretability: just ask them to explain their decisions. Yet, studies show that these post-hoc explanations often misrepresent the true decision process, as revealed by mismatches in feature importance. Despite growing evidence of this inconsistency, no systematic solutions have emerged, partly due to the high cost of estimating feature importance, which limits evaluations to small datasets. To address this, we introduce the Post-hoc Self-Consistency Bank (PSCB) - a large-scale benchmark of decisions spanning diverse tasks and models, each paired with LLM-generated explanations and corresponding feature importance scores. Analysis of PSCB reveals that self-consistency scores barely differ between correct and incorrect predictions. We also show that the standard metric fails to meaningfully distinguish between explanations. To overcome this limitation, we propose an alternative metric that more effectively captures variation in explanation quality. We use it to fine-tune LLMs via Direct Preference Optimization (DPO), leading to significantly better alignment between explanations and decision-relevant features, even under domain shift. Our findings point to a scalable path toward more trustworthy, self-consistent LLMs.",arXiv.org,2,Black-box,General,论文核心研究LLM生成的事后解释与其真实决策过程不一致的问题，明确指出post-hoc explanations misrepresent the true decision process，这与Faithfulness定义中的'解释是否真实反映模型的实际计算过程'直接相关。论文提出了新的度量方法来衡量解释与决策特征的一致性，并通过DPO优化来提高忠实度，属于核心相关研究。
Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出因果框架评估CoT的充分性和必要性，涉及不忠实现象及改进方法。,"Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.",arXiv.org,2,Black-box,General,该论文明确研究 CoT 推理的充分性和必要性，通过因果框架量化推理步骤对最终结果的实际影响，直接涉及 CoT 是否真实反映模型预测的计算过程，属于 Faithfulness 核心研究。
Investigating Training and Generalization in Faithful Self-Explanations of Large Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了如何通过训练提升自我解释的忠实性，并验证了其泛化能力。,"Large language models have the potential to generate explanations for their own predictions in a variety of styles based on user instructions. Recent research has examined whether these self-explanations faithfully reflect the models'actual behavior and has found that they often lack faithfulness. However, the question of how to improve faithfulness remains underexplored. Moreover, because different explanation styles have superficially distinct characteristics, it is unclear whether improvements observed in one style also arise when using other styles. This study analyzes the effects of training for faithful self-explanations and the extent to which these effects generalize, using three classification tasks and three explanation styles. We construct one-word constrained explanations that are likely to be faithful using a feature attribution method, and use these pseudo-faithful self-explanations for continual learning on instruction-tuned models. Our experiments demonstrate that training can improve self-explanation faithfulness across all classification tasks and explanation styles, and that these improvements also show signs of generalization to the multi-word settings and to unseen tasks. Furthermore, we find consistent cross-style generalization among three styles, suggesting that training may contribute to a broader improvement in faithful self-explanation ability.",,2,White-box,General,该论文明确研究如何提高语言模型自我解释的忠实度（faithfulness），并使用了特征归因方法（feature attribution method）来构建伪忠实解释。此外，论文还探讨了训练对忠实度的影响及其在不同解释风格中的泛化能力，这些都是 CoT Faithfulness 研究的核心内容。
Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning,2025,True,True,False,Training & Fine-tuning,论文探讨了RLVR在数学推理中的忠实性问题，揭示了模型可能依赖表面启发式而非真实推理。,"Mathematical reasoning is a central challenge for large language models (LLMs), requiring not only correct answers but also faithful reasoning processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising approach for enhancing such capabilities; however, its ability to foster genuine reasoning remains unclear. We investigate RLVR on two combinatorial problems with fully verifiable solutions: \emph{Activity Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully curated datasets with unique optima. Across multiple reward designs, we find that RLVR improves evaluation metrics but often by reinforcing superficial heuristics rather than acquiring new reasoning strategies. These findings highlight the limits of RLVR generalization, emphasizing the importance of benchmarks that disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful measures of progress. Code available at https://github.com/xashru/rlvr-seq-generalization.",arXiv.org,2,Black-box,Math,论文明确研究 RLVR 是否能促进真实的推理过程（faithful reasoning processes），并发现模型通过强化表面启发式而非获取新推理策略来提升指标，直接涉及 CoT 的忠实性问题。
MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文提出通过迭代反馈机制改进VLM的推理能力，涉及CoT忠实性改进方法。,"Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node—one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline. Extensive experiments conducted on several public benchmark datasets and mainstream VLMs demonstrate that our approach significantly improves the performance of VLMs on complex multimodal reasoning tasks, underscoring its effectiveness and wide applicability.",,0,Black-box,General,论文主要关注通过外部反馈机制提升视觉语言模型（VLM）在多模态推理任务中的性能，并未涉及对模型生成的推理路径（CoT）是否忠实反映模型内部计算过程的研究。研究重点在于性能提升和减少幻觉，而非解释的真实性。
FaithAct: Faithfulness Planning and Acting in MLLMs,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了FaithEval评估框架和FaithAct改进方法，关注CoT忠实性。,"Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",,2,Black-box,General,该论文明确研究多模态大语言模型（MLLMs）中的忠实性问题，区分了行为忠实性和感知忠实性，并提出了FaithEval来量化步骤级和链级的忠实性。此外，论文还提出了FaithAct框架，旨在在每一步推理中强制证据基础，从而改善忠实性。这些内容直接涉及CoT的忠实性研究，包括如何评估和增强忠实性，因此属于核心相关。
VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives,2022,True,True,True,Training & Fine-tuning,论文讨论了模型解释的忠实性，并提出了改进方法。,"Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis",Neural Information Processing Systems,2,White-box,General,该论文明确研究了模型解释的忠实度（Faithfulness），探讨了模型特征重要性（FI）监督是否真正反映了模型的内部推理过程。论文还提出了四个关键目标来优化模型的忠实度，并验证了解释的忠实性对预测准确性的影响。
Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification,2025,True,True,True,Training & Fine-tuning,论文讨论了LLM生成的理由中存在逻辑错误和标签不匹配等不忠实现象，并提出了基于信任指标的蒸馏方法来提升忠实性。,"Large language models (LLMs) increasingly generate natural language rationales to enhance interpretability, but these often contain logical errors, label mismatches, and domain-specific misalignments. Directly using such rationales as supervision risks propagating noise and undermining training stability. To address this challenge, we introduce Self-Filtered Distillation, a framework specifically tailored for patent classification, which treats LLM-generated rationales as trust signals rather than ground-truth supervision. The framework employs selective distillation guided by three unsupervised trust metrics: (1) Self-Consistency, which measures the stability of LLM-generated rationales across multiple generations; (2) Class Entailment Alignment, which assesses semantic coherence with patent-specific class definitions; and (3) LLM Agreement Scoring, which validates rationale-label plausibility. These metrics are integrated into a unified trust score that primarily weights training samples while optionally filtering out extremely low-trust cases, enabling reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used benchmark for patent classification, show that our method outperforms label-based learning and conventional distillation in accuracy, stability, and interpretability, establishing a reliable paradigm for leveraging reasoning-aware trust indicators in patent analytics.",arXiv.org,1,Black-box,General,该论文研究了LLM生成的rationales的可靠性和一致性（Self-Consistency），并提出了评估指标，这些可以作为Faithfulness的旁证。然而，论文主要关注的是如何利用这些rationales进行专利分类，而不是直接研究CoT是否真实反映了模型的预测过程。
"Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation",2025,True,True,False,Training & Fine-tuning,论文探讨了CoT推理痕迹的忠实性问题，并提出了基于规则的问题分解方法来改进。,"Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.",arXiv.org,2,Black-box,General,该论文明确研究了推理轨迹（CoT）的忠实性问题，探讨了正确轨迹与最终输出正确性之间的低相关性，挑战了利用推理轨迹提升模型性能的隐含假设，直接涉及Faithfulness的核心议题。
"Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training",2025,True,False,False,Training & Fine-tuning,论文探讨了LLMs自我解释内部决策过程的能力，并通过训练提升这种能力，与CoT忠实性相关。,"We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.",arXiv.org,2,White-box,General,论文明确研究LLM如何描述其内部决策过程，并验证其解释的准确性，直接涉及Faithfulness的核心问题。通过微调模型提高其解释自身决策的能力，进一步验证了解释的忠实性。
Training Language Models to Explain Their Own Computations,2025,True,False,False,Training & Fine-tuning,论文探讨了语言模型如何忠实描述其内部计算，与CoT忠实性相关，并提出了微调方法。,"Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs'privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs'internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models'privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.",,2,White-box,General,论文明确研究语言模型是否能忠实描述其内部计算过程，涉及模型内部激活的因果结构和输入标记对输出的影响，直接关联到 Faithfulness 的核心定义。
Explainable Multi-Hop Question Answering: A Rationale-Based Approach,2025,True,True,False,Training & Fine-tuning,论文提出了一种基于推理的框架，旨在提高模型推理过程的透明度和忠实性。,"Multi-hop question answering tasks involve identifying relevant supporting sentences from a given set of documents, which serve as the rationale for deriving answers. Most research in this area consists of two main components: a rationale identification module and a reader module. Since the rationale identification module often relies on retrieval models or supervised learning, annotated rationales are typically essential. This reliance on annotations, however, creates challenges when adapting to open-domain settings. Moreover, when models are trained on annotated rationales, explainable artificial intelligence (XAI) requires clear explanations of how the model arrives at these rationales. Consequently, traditional multi-hop question answering (QA) approaches that depend on annotated rationales are ill-suited for XAI, which demands transparency in the model’s reasoning process. To address this issue, we propose a rationale reasoning framework that can effectively infer rationales and clearly demonstrate the model’s reasoning process, even in open-domain environments without annotations. The proposed model is applicable to various tasks without structural constraints, and experimental results demonstrate its significantly improved rationale reasoning capabilities in multi-hop question answering, relation extraction, and sentence classification tasks.",Big Data and Cognitive Computing,1,Black-box,General,论文提出了一个基于推理框架的多跳问答方法，关注模型的推理过程是否可解释。虽然未直接研究CoT的忠实度，但讨论了模型如何生成和展示推理过程，属于边缘相关。
Mitigating Deceptive Alignment via Self-Monitoring,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了CoT中的不忠实现象（如欺骗性对齐），提出了度量框架DeceptionBench，并提出了改进方法CoT Monitor+。,"Modern large language models rely on chain-of-thought (CoT) reasoning to achieve impressive performance, yet the same mechanism can amplify deceptive alignment, situations in which a model appears aligned while covertly pursuing misaligned goals. Existing safety pipelines treat deception as a black-box output to be filtered post-hoc, leaving the model free to scheme during its internal reasoning. We ask: Can deception be intercepted while the model is thinking? We answer this question, the first framework that embeds a Self-Monitor inside the CoT process itself, named CoT Monitor+. During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies. The signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce DeceptionBench, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various LLMs and show that unrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT Monitor+ cuts deceptive behaviors by 43.8% on average while preserving task accuracy. Further, when the self-monitor signal replaces an external weak judge in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency. Our project website can be found at cot-monitor-plus.github.io",arXiv.org,2,White-box,Society,该论文明确研究CoT过程中的欺骗性对齐（deceptive alignment）问题，涉及模型在推理过程中隐藏真实目标的现象（sycophancy等），并通过内部自监控信号（self-evaluation signal）直接干预推理过程以提升忠实度。其提出的CoT Monitor+框架和DeceptionBench基准均围绕CoT的因果真实性展开，属于核心相关研究。
Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision,2025,True,True,False,Training & Fine-tuning,论文提出通过符号化方法和微调提升推理忠实性，涉及不忠实现象和改进方法。,"Large language models (LLMs) have shown strong performance in many reasoning benchmarks. However, recent studies have pointed to memorization, rather than generalization, as one of the leading causes for such performance. LLMs, in fact, are susceptible to content variations, demonstrating a lack of robust planning or symbolic abstractions supporting their reasoning process. To improve reliability, many attempts have been made to combine LLMs with symbolic methods. Nevertheless, existing approaches fail to effectively leverage symbolic representations due to the challenges involved in developing reliable and scalable verification mechanisms. In this paper, we propose to overcome such limitations by synthesizing high-quality symbolic reasoning trajectories with stepwise pseudo-labels at scale via Monte Carlo estimation. A Process Reward Model (PRM) can be efficiently trained based on the synthesized data and then used to select more symbolic trajectories. The trajectories are then employed with Direct Preference Optimization (DPO) and Supervised Fine-Tuning (SFT) to improve logical reasoning and generalization. Our results on benchmarks (i.e., FOLIO and LogicAsker) show the effectiveness of the proposed method with gains on frontier and open-weight models. Moreover, additional experiments on claim verification data reveal that fine-tuning on the generated symbolic reasoning trajectories enhances out-of-domain generalizability, suggesting the potential impact of the proposed method in enhancing planning and logical reasoning.",,1,Black-box,Logic,该论文主要关注通过符号化方法增强语言模型的逻辑推理能力，并提出了蒙特卡洛过程监督的方法。虽然论文提到了推理轨迹（reasoning trajectories）和过程奖励模型（PRM），但并未明确研究这些推理轨迹是否真实反映了模型的实际计算过程（即Faithfulness）。因此，它属于边缘相关的研究，可以作为Faithfulness的旁证，但未直接探讨忠实度问题。
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning,2024,True,True,False,Training & Fine-tuning,论文讨论了LLMs生成不一致解释的问题，并提出了通过微调提升一致性的方法。,"Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation""all birds can fly""when answering the question""Can sparrows fly?""but meanwhile answer""no""to the related question""Can penguins fly?"". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out-of-distribution datasets not seen during finetuning (+4.5% relative). Code is available at https://github.com/yandachen/explanation-consistency-finetuning .",International Conference on Computational Linguistics,1,Black-box,General,论文研究的是解释的一致性（Consistency），虽然与Faithfulness相关，但并未直接探讨解释是否真实反映模型的预测过程（Causal role）。因此属于边缘相关。
Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出AoU框架，通过验证前提来提升推理忠实性，并实证改进准确性和忠实性。,"Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at https://anonymous.4open.science/r/audit-of-understanding-E28B.",arXiv.org,2,Black-box,Math,这篇论文明确研究了数学推理中的CoT忠实度问题，提出了一种框架来约束推理过程，确保其基于已验证的前提。论文不仅关注准确性改进，还强调了提高忠实度和减少幻觉推理的目标。实验中明确提到 faithfulness 的提升，属于 Core Relevant 范畴。
Latent Veracity Inference for Identifying Errors in Stepwise Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出验证推理步骤正确性的方法，涉及不忠实现象和改进方法。,"Chain-of-Thought (CoT) reasoning has advanced the capabilities and transparency of language models (LMs); however, reasoning chains can contain inaccurate statements that reduce performance and trustworthiness. To address this, we propose to augment each reasoning step in a CoT with a latent veracity (or correctness) variable. To efficiently explore this expanded space, we introduce Veracity Search (VS), a discrete search algorithm over veracity assignments. It performs otherwise intractable inference in the posterior distribution over latent veracity values by leveraging the LM's joint likelihood over veracity and the final answer as a proxy reward. This efficient inference-time verification method facilitates supervised fine-tuning of an Amortized Veracity Inference (AVI) machine by providing pseudo-labels for veracity. AVI generalizes VS, enabling accurate zero-shot veracity inference in novel contexts. Empirical results demonstrate that VS reliably identifies errors in logical (ProntoQA), mathematical (GSM8K), and commonsense (CommonsenseQA) reasoning benchmarks, with AVI achieving comparable zero-shot accuracy. Finally, we demonstrate the utility of latent veracity inference for providing feedback during self-correction and self-improvement.",,1,Black-box,General,该论文研究 CoT 推理步骤中的准确性（veracity）问题，并提出了一种方法来识别和纠正推理链中的错误。虽然它关注的是推理的正确性（correctness）而非忠实度（faithfulness），但可以作为 Faithfulness 研究的旁证，因为它涉及到推理过程的可靠性。
Reward Hacking Mitigation using Verifiable Composite Rewards,2025,True,True,False,Training & Fine-tuning,论文讨论了奖励黑客行为，涉及不忠实现象，并提出了通过复合奖励函数来改进。,"Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that utilizing RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR1.",,1,Black-box,Medical,论文研究了在医学领域中，LLMs在推理阶段可能出现奖励破解的行为，这与CoT的忠实度有一定关联，但未直接研究CoT是否是模型预测的真实原因。
Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories,2025,True,True,False,Training & Fine-tuning,论文提出通过强化学习优化推理轨迹以减少阿谀奉承现象，涉及 CoT 忠实性。,"Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",,2,Black-box,Society,论文明确研究 Sycophancy（阿谀奉承）现象，并提出了通过优化内部推理机制（Adaptive Reasoning Trajectories）来减少这种现象的方法。这直接涉及 CoT 的忠实度问题，即模型生成的解释是否真实反映了其预测过程。
"LINKED: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出过滤噪声知识和减少无效推理的方法，涉及 CoT 忠实性。,"Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0% improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks.",Conference on Empirical Methods in Natural Language Processing,0,Black-box,Society,论文主要关注通过知识增强提升LLM在常识推理任务中的性能，提出了过滤噪声知识和减少无效推理的方法，但未涉及CoT的忠实度或因果分析。
Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction,2025,True,False,False,Training & Fine-tuning,论文提出基于认知科学的推理机制和强化学习优化，提升解释质量和任务准确性，涉及CoT忠实性。,"This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).",arXiv.org,1,Black-box,General,论文研究了关系抽取任务中的解释性和准确性，通过强化学习优化解释质量，但未明确探讨 CoT 是否真实反映模型预测的计算过程。
Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion,2025,False,False,False,Training & Fine-tuning,论文研究CoT成功预测，未涉及忠实性或不忠实现象。,"We investigate whether the success of a zero-shot Chain-of-Thought (CoT) process can be predicted before completion. We discover that a probing classifier, based on LLM representations, performs well \emph{even before a single token is generated}, suggesting that crucial information about the reasoning process is already present in the initial steps representations. In contrast, a strong BERT-based baseline, which relies solely on the generated tokens, performs worse, likely because it depends on shallow linguistic cues rather than deeper reasoning dynamics. Surprisingly, using later reasoning steps does not always improve classification. When additional context is unhelpful, earlier representations resemble later ones more, suggesting LLMs encode key information early. This implies reasoning can often stop early without loss. To test this, we conduct early stopping experiments, showing that truncating CoT reasoning still improves performance over not using CoT at all, though a gap remains compared to full reasoning. However, approaches like supervised learning or reinforcement learning designed to shorten CoT chains could leverage our classifier's guidance to identify when early stopping is effective. Our findings provide insights that may support such methods, helping to optimize CoT's efficiency while preserving its benefits.",Annual Meeting of the Association for Computational Linguistics,1,White-box,General,该论文研究了LLM在生成CoT之前的内部表示是否包含推理成功的信息，虽然未直接探讨CoT的忠实度，但涉及了推理过程的内部机制，属于边缘相关。
A collection of principles for guiding and evaluating large language models,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Interpretability & Internal Mechanisms, Consistency & Ensembling",论文讨论了LLM的推理能力及透明度，涉及CoT和Faithfulness，但未具体揭示不忠实现象或提出度量指标。,"Large language models (LLMs) demonstrate outstanding capabilities, but challenges remain regarding their ability to solve complex reasoning tasks, as well as their transparency, robustness, truthfulness, and ethical alignment. In this preliminary study, we compile a set of core principles for steering and evaluating the reasoning of LLMs by curating literature from several relevant strands of work: structured reasoning in LLMs, self-evaluation/self-reflection, explainability, AI system safety/security, guidelines for human critical thinking, and ethical/regulatory guidelines for AI. We identify and curate a list of 220 principles from literature, and derive a set of 37 core principles organized into seven categories: assumptions and perspectives, reasoning, information and evidence, robustness and security, ethics, utility, and implications. We conduct a small-scale expert survey, eliciting the subjective importance experts assign to different principles and lay out avenues for future work beyond our preliminary results. We envision that the development of a shared model of principles can serve multiple purposes: monitoring and steering models at inference time, improving model behavior during training, and guiding human evaluation of model reasoning.",arXiv.org,1,Black-box,General,该论文综述了关于大型语言模型推理评估的核心原则，虽然提到了透明度和解释性，但并未直接关注Chain-of-Thought的忠实度问题，更多是综合性的原则总结和评估框架。因此属于边缘相关。
Evaluating and Mitigating Sycophancy in Large Vision-Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文讨论了LVLMs中的Sycophancy现象，提出了评估框架SyEval-VL和改进方法HFRAG。,"Large vision-language models (LVLMs) have recently achieved significant advancements, demonstrating powerful capabilities in understanding and reasoning about visual information. However, LVLMs may generate biased responses that reflect the user beliefs rather than the facts, a phenomenon known as sycophancy. Sycophancy can pose serious challenges to the performance, trustworthiness, and security of LVLMs, raising concerns about their practical applications. We note that there is limited work on the evaluation and mitigation of sycophancy in LVLMs. In this paper, we introduce SyEval-VL, a benchmark specifically designed to evaluate sycophancy in LVLMs. SyEval-VL offers a comprehensive evaluation of sycophancy in visual understanding and reasoning across various scenarios with a multi-round dialogue format. We evaluate sycophancy in several popular LVLMs, providing an in-depth analysis of various sycophantic behaviors and their consequential impacts. Additionally, we propose a novel framework, Human Feedback-based Retrieval-Augmented Generation (HFRAG), to mitigate sycophancy in LVLMs by determining the appropriate timing of retrieval, profiling the proper retrieval target, and augmenting the decoding of LVLMs. Extensive experiments demonstrate that the proposed method significantly mitigates sycophancy in LVLMs without requiring additional training. Our code is available at: https://github.com/immc-lab/SyEval-VL",,2,Black-box,Society,论文明确研究 Sycophancy（阿谀奉承）现象，即模型生成反映用户信念而非事实的回应，这与 CoT Faithfulness 中的 Unfaithful 和 Post-hoc Rationalization 直接相关。论文还提出了评估和缓解 Sycophancy 的方法，属于核心相关研究。
Boosting Explainability through Selective Rationalization in Pre-trained Language Models,2025,True,True,False,Training & Fine-tuning,论文探讨了PLMs中rationalization的退化问题，并提出了改进方法PLMR。,"The widespread application of pre-trained language models (PLMs) in natural language processing (NLP) has led to increasing concerns about their explainability. Selective rationalization is a self-explanatory framework that selects human-intelligible input subsets as rationales for predictions. Recent studies have shown that applying existing rationalization frameworks to PLMs will result in severe degeneration and failure problems, producing sub-optimal or meaningless rationales. Such failures severely damage trust in rationalization methods and constrain the application of rationalization techniques on PLMs. In this paper, we find that the homogeneity of tokens in the sentences produced by PLMs is the primary contributor to these problems. To address these challenges, we propose a method named Pre-trained Language Model's Rationalization (PLMR), which splits PLMs into a generator and a predictor to deal with NLP tasks while providing interpretable rationales. The generator in PLMR also alleviates homogeneity by pruning irrelevant tokens, while the predictor uses full-text information to standardize predictions. Experiments conducted on two widely used datasets across multiple PLMs demonstrate the effectiveness of the proposed method PLMR in addressing the challenge of applying selective rationalization to PLMs. Codes: https://github.com/ylb777/PLMR.",Knowledge Discovery and Data Mining,2,Black-box,General,该论文明确研究了解释的忠实性问题，提出了选择性合理化方法（Selective Rationalization）来解决预训练语言模型生成的解释退化问题，确保解释与模型预测的实际计算过程一致。这直接符合Faithfulness的核心定义，即解释是否真实反映了模型的预测过程。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到 faithfulness 是设计 fact verification 系统的重要考虑因素，即生成的解释需要准确反映模型的推理过程。这与 CoT Faithfulness 的核心定义一致，即解释是否真实反映了模型的实际计算过程。此外，论文还提到了人类评估表明其方法产生的证明更可信，进一步支持了其与 Faithfulness 的相关性。
Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,False,False,False,Training & Fine-tuning,论文关注证据的忠实性和合理性，但未明确涉及 Chain-of-Thought 推理过程。,"Real-world fact verification task aims to verify the factuality of a claim by retrieving evidence from the source document. The quality of the retrieved evidence plays an important role in claim verification. Ideally, the retrieved evidence should be faithful (reflecting the model's decision-making process in claim verification) and plausible (convincing to humans), and can improve the accuracy of verification task. Although existing approaches leverage the similarity measure of semantic or surface form between claims and documents to retrieve evidence, they all rely on certain heuristics that prevent them from satisfying all three requirements. In light of this, we propose a fact verification model named ReRead to retrieve evidence and verify claim that: (1) Train the evidence retriever to obtain interpretable evidence (i.e., faithfulness and plausibility criteria); (2) Train the claim verifier to revisit the evidence retrieved by the optimized evidence retriever to improve the accuracy. The proposed system is able to achieve significant improvements upon best-reported models under different settings.",Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,1,Black-box,General,论文研究了证据检索的忠实性（faithfulness）和可信性（plausibility），但主要关注的是事实验证任务中的证据质量，而非直接研究CoT的忠实性。虽然提到了忠实性，但未深入探讨CoT是否是模型预测的真实原因。
Explainability Via Causal Self-Talk,2022,True,False,False,Training & Fine-tuning,论文提出通过训练AI系统建立自我因果模型来生成忠实解释，属于改进方法中的训练与微调。,"Explaining the behavior of AI systems is an important problem that, in practice, is generally avoided. While the XAI community has been developing an abundance of techniques, most incur a set of costs that the wider deep learning community has been unwilling to pay in most situations. We take a pragmatic view of the issue, and define a set of desiderata that capture both the ambitions of XAI and the practical constraints of deep learning. We describe an effective way to satisfy all the desiderata: train the AI system to build a causal model of itself. We develop an instance of this solution for Deep RL agents: Causal Self-Talk. CST operates by training the agent to communicate with itself across time. We implement this method in a simulated 3D environment, and show how it enables agents to generate faithful and semantically-meaningful explanations of their own behavior. Beyond explanations, we also demonstrate that these learned models provide new ways of building semantic control interfaces to AI systems.",Neural Information Processing Systems,2,White-box,General,论文明确研究如何通过自我对话训练构建因果模型来生成忠实且语义上有意义的解释，直接符合Faithfulness的核心定义，且涉及模型内部机制（White-box）。

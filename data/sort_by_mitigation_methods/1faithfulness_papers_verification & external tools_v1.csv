title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Causal-driven Large Language Models with Faithful Reasoning for Knowledge Question Answering,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出基于因果推理的框架CORE，旨在提升忠实推理，涉及不忠实现象和改进方法。,"In Large Language Models (LLMs), text generation that involves knowledge representation is often fraught with the risk of ""hallucinations'', where models confidently produce erroneous or fabricated content. These inaccuracies often stem from intrinsic biases in the pre-training stage or from the incorporation of human preference biases during the fine-tuning process. To mitigate these issues, we take inspiration from Goldman's causal theory of knowledge, which asserts that knowledge is not merely about having a true belief but also involves a causal connection between the belief and the truth of the proposition. We instantiate this theory within the context of Knowledge Question Answering (KQA) by constructing a causal graph that delineates the pathways between the candidate knowledge and belief. Through the application of the do-calculus rules from structural causal models, we devise an unbiased estimation framework based on this causal graph, thereby establishing a methodology for knowledge modeling grounded in causal inference. The resulting CORE framework (short for ""Causal knOwledge REasoning'') is comprised of four essential components: question answering, causal reasoning, belief scoring, and refinement. Together, they synergistically improve the KQA system by fostering faithful reasoning and introspection. Extensive experiments are conducted on ScienceQA and HotpotQA datasets, which demonstrate the effectiveness and rationality of the CORE framework.",ACM Multimedia,2,White-box,General,论文明确研究因果驱动的推理（Causal-driven reasoning）并设计了CORE框架以确保知识问答中的忠实性（Faithful Reasoning）。通过构建因果图和do-calculus规则，论文直接解决了模型生成内容与真实知识之间的因果连接问题，符合核心相关的标准。
Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations,2025,True,True,True,Verification & External Tools,论文提出了一种自动化验证方法EDCT，用于检测Vision-Language模型解释的忠实性，并揭示了不忠实现象。,"Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",arXiv.org,2,Black-box,General,该论文明确研究视觉语言模型生成的解释（NLEs）是否真实反映了模型预测的因果因素，提出了Explanation-Driven Counterfactual Testing (EDCT)方法来验证解释的忠实度，并计算Counterfactual Consistency Score (CCS)来衡量解释的因果关联性。这与Faithfulness的核心定义完全吻合。
ProoFVer: Natural Logic Theorem Proving for Fact Verification,2021,True,False,False,Verification & External Tools,论文提出使用自然逻辑推理作为忠实解释，通过构造确保忠实性。,"Abstract Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.1",Transactions of the Association for Computational Linguistics,2,Black-box,Logic,明确提到生成的证明是忠实的解释（faithful explanations），并且通过构造确保了忠实性（faithful by construction）。这表明 CoT（这里是自然逻辑推理过程）反映了实际的预测过程，符合 Faithfulness 的核心定义。论文还提到了与基于注意力的高亮部分相比，这些证明与人类理性的重叠较好，进一步支持了其对解释真实性的关注。
FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,True,True,False,Verification & External Tools,论文提出模块化组件确保推理步骤与推理过程有因果联系，提升忠实性。,"Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof graph and intermediate inferences within the same model and thus may be unfaithful. In this work, we frame the deductive logical reasoning task by defining three modular components: rule selection, fact selection, and knowledge composition. The rule and fact selection steps select the candidate rule and facts to be used and then the knowledge composition combines them to generate new inferences. This ensures model faithfulness by assured causal relation from the proof step to the inference reasoning. To test our framework, we propose FaiRR (Faithful and Robust Reasoner) where the above three components are independently modeled by transformers. We observe that FaiRR is robust to novel language perturbations, and is faster at inference than previous works on existing reasoning datasets. Additionally, in contrast to black-box generative models, the errors made by FaiRR are more interpretable due to the modular approach.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,论文明确提出了保证模型忠实性的框架（FaiRR），通过模块化设计确保推理步骤与生成结果的因果关联，直接针对 deductive reasoning 中的 faithfulness 问题进行研究。
Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文揭示了LVLMs在CoT推理中忽视生成理由的问题，并提出了一种新的解码策略来提升忠实性。,"Large vision-language models (LVLMs) have demonstrated remarkable capabilities by integrating pre-trained vision encoders with large language models (LLMs). Similar to single-modal LLMs, chain-of-thought (CoT) prompting has been adapted for LVLMs to enhance multi-modal reasoning by generating intermediate rationales based on visual and textual inputs. While CoT is assumed to improve grounding and accuracy in LVLMs, our experiments reveal a key challenge: existing LVLMs often ignore the contents of generated rationales in CoT reasoning. To address this, we re-formulate multi-modal CoT reasoning as a KL-constrained reward maximization focused on rationale-conditional log-likelihood. As the optimal solution, we propose rationale-enhanced decoding (RED), a novel plug-and-play inference-time decoding strategy. RED harmonizes visual and rationale information by multiplying distinct image-conditional and rationale-conditional next token distributions. Extensive experiments show that RED consistently and significantly improves reasoning over standard CoT and other decoding methods across multiple benchmarks and LVLMs. Our work offers a practical and effective approach to improve both the faithfulness and accuracy of CoT reasoning in LVLMs, paving the way for more reliable rationale-grounded multi-modal systems.",arXiv.org,2,Black-box,General,论文明确指出现有LVLMs在CoT推理中往往忽略生成的rationale内容，且提出解决方案RED旨在提升faithfulness和准确性，直接符合CoT Faithfulness的核心定义。
Natural Language Deduction through Search over Statement Compositions,2022,True,False,False,Verification & External Tools,论文提出了一种通过搜索过程分解任务的方法，生成反映推理过程的中间结论树，符合CoT忠实性定义。,"In settings from fact-checking to question answering, we frequently want to know whether a collection of evidence (premises) entails a hypothesis. Existing methods primarily focus on the end-to-end discriminative version of this task, but less work has treated the generative version in which a model searches over the space of statements entailed by the premises to constructively derive the hypothesis. We propose a system for doing this kind of deductive reasoning in natural language by decomposing the task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process. Our experiments on the EntailmentBank dataset (Dalvi et al., 2021) demonstrate that the proposed system can successfully prove true statements while rejecting false ones. Moreover, it produces natural language explanations with a 17% absolute higher step validity than those produced by an end-to-end T5 model.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,Logic,该论文明确提出了一个系统，通过搜索过程在自然语言中构建推理树，其生成的中间结论忠实地反映了系统的推理过程。这与 Faithfulness 的核心定义高度一致，因为它关注的是解释（CoT）是否真实反映了模型的计算过程。实验结果显示其生成的自然语言解释比其他模型更具步骤有效性，证明了其对忠实度的考量。
Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,True,False,False,Verification & External Tools,论文提出通过分解任务生成子问题，确保解释的忠实性，属于改进方法中的验证与外部工具。,"We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model’s reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",North American Chapter of the Association for Computational Linguistics,2,Black-box,General,论文提出了一种框架（Text Modular Networks），通过将复杂任务分解为可由现有模型解决的简单子任务，并生成子问题和答案作为模型的自然语言解释。明确提到这些解释是‘faithful’的，即真实反映了模型的推理过程，符合 Faithfulness 的核心定义。此外，论文还强调了生成的理解和可信度，与 CoT Faithfulness 的研究目标一致。
Semi-structured LLM Reasoners Can Be Rigorously Audited,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出半结构化推理模型（SSRMs）以解决不忠实现象，并引入自动审计方法来检测推理错误。,"Although Large Language Models (LLMs) have become capable reasoners, the problem of faithfulness persists: their reasoning can contain errors and omissions that are difficult to detect and that may obscure biases in model outputs. To address this issue, we introduce Semi-Structured Reasoning Models (SSRMs), which are trained to produce semi-structured representations of reasoning. SSRMs generate reasoning traces in a non-executable Pythonic syntax that names each reasoning step and marks its inputs and outputs. This structure allows SSRM traces to be automatically audited to identify reasoning flaws. We evaluate three types of audits: hand-crafted structured reasoning audits, written in a domain-specific language (DSL) implemented in Python; LLM-generated structured reasoning audits; and learned typicality audits, which apply probabilistic models over reasoning traces. We show that all of these methods can be used to effectively flag probable reasoning errors. Importantly, the auditability of SSRMs does not appear to compromise overall accuracy: in evaluation on twelve benchmarks and two model families, SSRMs demonstrate strong performance and generalizability relative to other models of comparable size.",arXiv.org,2,White-box,General,该论文直接解决了 CoT 的 Faithfulness 问题，提出了 SSRMs 来生成半结构化推理痕迹，并通过自动审计识别推理缺陷（如错误和遗漏）。论文明确关注推理的真实性和可审计性，属于核心相关研究。
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?,2020,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出新的度量标准LAS评估解释的忠实性，并讨论了模型生成解释的合理性问题。,"Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.",Findings,2,Black-box,General,这篇论文明确关注解释的忠实性（faithfulness），提出了泄漏调整可模拟性（LAS）指标来评估自然语言解释是否真实反映了模型的行为，而不是仅仅匹配人类的解释（plausibility）。
MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification,2025,True,True,False,Verification & External Tools,论文提出结构化论证树以提升推理的忠实性，属于改进方法中的验证与外部工具。,"Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.",arXiv.org,2,Black-box,Society,该论文明确研究了如何通过结构化论证树（Argument Trees）来提高模型生成解释的忠实度（Faithfulness），避免无结构的交互导致的不可靠解释。通过构建可检查的论证路径，确保了从初始论点到最终结论的推理过程是可追踪和可信的。
"Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving",2025,True,False,True,Verification & External Tools,论文提出基于形式化验证的框架和度量标准，关注过程级验证，与CoT忠实性相关。,"As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.",arXiv.org,1,Black-box,Math,论文提出了一个形式化问题解决的框架，并强调了过程级可验证性，这与 CoT Faithfulness 有一定相关性。虽然主要关注的是问题的形式化和解决的准确性，但提出的 RPE 方法用于验证答案的正确性，可以作为 Faithfulness 的旁证。
Leveraging LLMs for Hypothetical Deduction in Logical Inference: A Neuro-Symbolic Approach,2024,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文提出了一种神经符号方法LINA，旨在提升逻辑推理的忠实性，减少对外部求解器的依赖。,"Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduce LINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available at https://github.com/wufeiwuwoshihua/nshy.",arXiv.org,2,Black-box,Logic,论文明确提到 'faithful logical reasoning'，强调通过神经符号方法增强推理过程的忠实度和独立性，避免对外部求解器的依赖。符合核心相关标准，专注于忠实性和推理过程的可靠性。
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,2,Black-box,General,该论文明确研究LLM生成的解释（如CoT）的忠实性，提出了一种评估和改进解释忠实性的框架FaithLM，并通过干预性评估验证解释的真实性，完全符合Faithfulness的核心研究方向。
Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文揭示了LLMs在生成答案时不忠实于中间推理步骤的现象，并提出了FRODO框架来改进忠实性。,"Large language models (LLMs) have been shown to perform better when asked to reason step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, we perform a causal mediation analysis on twelve LLMs to examine how intermediate reasoning steps generated by the LLM influence the final outcome and find that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, we introduce FRODO, a framework to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. FRODO consists of an inference module that learns to generate correct reasoning steps using an implicit causal reward function and a reasoning module that learns to faithfully reason over these intermediate inferences using a counterfactual and causal preference objective. Our experiments show that FRODO significantly outperforms four competitive baselines. Furthermore, FRODO improves the robustness and generalization ability of the reasoning LM, yielding higher performance on out-of-distribution test sets. Finally, we find that FRODO's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.",Conference on Empirical Methods in Natural Language Processing,2,White-box,General,该论文直接研究 Chain-of-Thought (CoT) 的忠实度（Faithfulness），通过因果中介分析（causal mediation analysis）验证 LLMs 是否可靠地使用中间推理步骤生成最终答案，并提出了 FRODO 框架来改善忠实度。论文明确涉及 CoT 的因果作用和反事实分析，符合核心相关标准。
FLARE: Faithful Logic-Aided Reasoning and Exploration,2024,True,True,True,"Verification & External Tools, Interpretability & Internal Mechanisms",论文提出FLARE方法，通过逻辑编程和多跳搜索提高CoT忠实性，并量化评估。,"Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.",,2,Black-box,Logic,这篇论文明确研究了Chain-of-Thought (CoT)的忠实度问题，提出了FLARE方法用于计算推理过程的忠实度，并与生成的代码进行比较。论文还分析了模型忠实度与整体性能的正相关性，这些内容直接关联到CoT Faithfulness的核心研究范畴。
Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology,2025,True,False,True,Verification & External Tools,论文提出基于推理拓扑的框架量化LLM解释的不确定性，与CoT忠实性相关，但未讨论不忠实现象或改进方法。,"Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.",arXiv.org,2,White-box,General,这篇论文提出了一个量化LLM解释中的不确定性的框架，特别关注了Faithfulness和reasoning consistency，这与CoT Faithfulness的核心定义紧密相关。论文不仅研究了如何测量Faithfulness，还分析了知识冗余和推理路径的不确定性，提供了增强robustness和Faithfulness的指导。
Graph of Logic: Enhancing LLM Reasoning with Graphs and Symbolic Logic,2024,True,True,False,Verification & External Tools,论文提出利用符号逻辑增强LLM推理能力，并检测不忠实推理，属于改进方法中的外部工具验证。,"Large language models (LLMs) have been shown to struggle with complex logical reasoning tasks due to the inherent ambiguity and complexity of natural language. These challenges are further amplified when processing large and diverse datasets, increasing the likelihood of unfaithful reasoning and predictive hallucinations. However, LLMs can provide accurate responses when queries are clear and direct. Symbolic logic provides precise, well-defined rules that can help overcome ambiguity and support reasoning. In this work, we leverage symbolic logic’s precision to enhance LLMs’ logical reasoning capabilities by introducing the Graph of Logic (GoL) framework. GoL combines the power of graph structures with the strengths of LLMs and symbolic logic. GoL utilizes the precise rules of symbolic logic to infer new facts and detect LLM hallucinations effectively on complex datasets. Furthermore, GoL utilizes graph structures to support scalability for large datasets and tackle long dependencies, enabling efficient handling of complex reasoning tasks. We conduct extensive experiments across seven benchmark datasets, encompassing various types of reasoning. These include deductive, inductive, and abductive reasoning, each testing distinct aspects of logical inference. The experimental results demonstrate GoL’s advantage in improving the reasoning capabilities of LLMs. GoL outperforms the baselines with an average margin of 18.18% for the GPT-3.5 and GPT-4 models, outperforming the baselines for all datasets for the GPT-3.5 model and six out of seven datasets for the GPT-4 model1.",BigData Congress [Services Society],1,Black-box,Logic,论文探讨了利用符号逻辑和图结构增强LLM的逻辑推理能力，并提及了不忠实推理（unfaithful reasoning）和幻觉（hallucinations）问题，但没有直接研究CoT的忠实性问题。属于边缘相关，因为涉及推理的一致性和鲁棒性，但未深入因果分析。
AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning,2025,True,True,False,"Training & Fine-tuning, Verification & External Tools",论文针对RLVR导致的虚假推理问题，提出基于过程的奖励机制来提高推理忠实性。,"Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",arXiv.org,2,Black-box,General,论文明确提到了 'reasoning faithfulness' 并提出了一个框架（AutoRubric-R1V）来通过过程级监督提高多模态推理的忠实度。这直接涉及到 CoT 是否真实反映了模型的实际计算过程，符合 Faithfulness 的核心定义。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确提出 Chain-of-Knowledge (CoK) prompting 来解决 CoT 中的 unfaithful reasoning chains 问题，并引入了 F^2-Verification 方法来评估推理链的 factuality 和 faithfulness。这直接涉及 CoT 的忠实度问题，符合核心相关标准。
Graph-Guided Textual Explanation Generation Framework,2024,True,True,False,"Training & Fine-tuning, Verification & External Tools, Interpretability & Internal Mechanisms",论文提出了一种增强NLE忠实性的框架，并讨论了不忠实现象。,"Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.",,2,Black-box,General,论文明确研究如何提高自然语言解释（NLEs）的忠实度，通过图神经网络将高亮解释与模型推理逻辑对齐，旨在验证生成的解释是否真实反映模型的内部推理过程。
"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales",2024,True,True,False,Verification & External Tools,论文提出了一种生成忠实、简洁且可转移的多模态理由的新范式，涉及不忠实现象和改进方法。,"The remarkable performance of Multimodal Large Language Models (MLLMs) has demonstrated their proficient understanding capabilities in handling various visual tasks. Nevertheless, the opaque nature of black-box reasoning processes persists as an enigma, rendering them uninterpretable and struggling with hallucination. Their ability to execute intricate reasoning tasks is also constrained, culminating in stagnation of progression. In this work, we introduce Fact, a novel paradigm designed to generate multimodal rationales that are faithful, concise, and transferable for teaching MLLMs. This paradigm utilizes verifiable visual programming to generate executable code guaranteeing faithfulness. Through a series of operations including pruning, merging, and bridging, the rationale enhances its conciseness. Furthermore, we filter rationales that can be transferred to end-to-end paradigms from programming paradigms to guarantee transferability. Empirical evidence from experiments demonstrates the superiority of Fact across models of varying parameter sizes, significantly enhancing their compositional reasoning and generalization ability and reducing hallucinations owing to its high correlation between images and text.",ACM Multimedia,2,Black-box,General,论文明确研究如何生成忠实（faithful）的多模态理由，并通过可验证的视觉编程保证其忠实度，符合Faithfulness的核心定义。尽管是黑盒方法，但直接解决了CoT的忠实性问题。
Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment,2025,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文探讨了CoT在可引导多元对齐中的应用，并分析了其忠实性，提出了改进方法。,"Large Language Models (LLMs) are typically trained to reflect a relatively uniform set of values, which limits their applicability to tasks that require understanding of nuanced human perspectives. Recent research has underscored the importance of enabling LLMs to support steerable pluralism -- the capacity to adopt a specific perspective and align generated outputs with it. In this work, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be applied to building steerable pluralistic models. We explore several methods, including CoT prompting, fine-tuning on human-authored CoT, fine-tuning on synthetic explanations, and Reinforcement Learning with Verifiable Rewards (RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA datasets. Among the methods studied, RLVR consistently outperforms others and demonstrates strong training sample efficiency. We further analyze the generated CoT traces with respect to faithfulness and safety.",,2,Black-box,Society,论文明确提到分析了生成的 CoT traces 的忠实性（faithfulness），并探讨了如何通过不同方法（如 CoT prompting、fine-tuning 和 RLVR）来增强模型的忠实度和安全性。这直接涉及到 CoT 是否真实反映模型的计算过程，符合 Faithfulness 的核心定义。
Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文研究了CoT的有效性和忠实性，揭示了不忠实现象，并提出了改进算法。,"Chain-of-thought (CoT) prompting demonstrates varying performance under different reasoning tasks. Previous work attempts to evaluate it but falls short in providing an in-depth analysis of patterns that influence the CoT. In this paper, we study the CoT performance from the perspective of effectiveness and faithfulness. For the former, we identify key factors that influence CoT effectiveness on performance improvement, including problem difficulty, information gain, and information flow. For the latter, we interpret the unfaithful CoT issue by conducting a joint analysis of the information interaction among the question, CoT, and answer. The result demonstrates that, when the LLM predicts answers, it can recall correct information missing in the CoT from the question, leading to the problem. Finally, we propose a novel algorithm to mitigate this issue, in which we recall extra information from the question to enhance the CoT generation and evaluate CoTs based on their information gain. Extensive experiments demonstrate that our approach enhances both the faithfulness and effectiveness of CoT.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确研究了 Chain-of-Thought (CoT) 的忠实度(Faithfulness)问题，特别是分析了 CoT 与问题、答案之间的信息交互，发现了模型在预测答案时可以从未出现在 CoT 中的问题部分回忆起正确信息的现象，这正是 Faithfulness 研究的核心问题。此外，论文还提出了一个算法来缓解这个问题，并通过实验验证了其方法在提升 CoT 忠实度和有效性方面的效果。因此，该论文属于核心相关的研究范畴。
Logical Satisfiability of Counterfactuals for Faithful Explanations in NLI,2022,True,True,True,Verification & External Tools,论文提出基于反事实的忠实性评估方法，涉及不忠实现象和新度量指标。,"Evaluating an explanation's faithfulness is desired for many reasons such as trust, interpretability and diagnosing the sources of model's errors. In this work, which focuses on the NLI task, we introduce the methodology of Faithfulness-through-Counterfactuals, which first generates a counterfactual hypothesis based on the logical predicates expressed in the explanation, and then evaluates if the model's prediction on the counterfactual is consistent with that expressed logic (i.e. if the new formula is \textit{logically satisfiable}). In contrast to existing approaches, this does not require any explanations for training a separate verification model. We first validate the efficacy of automatic counterfactual hypothesis generation, leveraging on the few-shot priming paradigm. Next, we show that our proposed metric distinguishes between human-model agreement and disagreement on new counterfactual input. In addition, we conduct a sensitivity analysis to validate that our metric is sensitive to unfaithful explanations.",AAAI Conference on Artificial Intelligence,2,White-box,Logic,该论文明确提出了通过反事实逻辑可满足性来评估解释的忠实度（Faithfulness-through-Counterfactuals），无需训练额外验证模型即可检测不忠实解释（Unfaithful explanations），且实验证明了指标对不忠实解释的敏感性，完全符合核心相关标准。方法涉及逻辑推理的内部一致性验证，属于白盒分析。
Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning,2025,True,False,True,Verification & External Tools,论文提出基于Curry-Howard对应关系的验证框架，将CoT映射为形式化证明，属于忠实性度量方法。,"While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",arXiv.org,2,White-box,General,该论文明确提出了一个基于 Curry-Howard 对应的理论框架，用于验证 CoT 生成的 rationale 的忠实性。通过将 CoT 步骤映射到形式化的类型化证明结构，提供了一种可验证的计算忠实性证书，直接涉及 CoT 的因果角色和忠实性验证方法。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,2,Black-box,General,该论文明确提到了增强推理过程的 'faithfulness'，并通过多模型协作（Debate、Review、Retrieve模式）来提高推理的可信度和可靠性。虽然主要关注点是通过多模型协作来克服幻觉和提高解决方案的质量，但仍然涉及到了Faithfulness的概念，因此属于核心相关。
Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification,2025,True,True,False,Verification & External Tools,论文讨论了CoT忠实性问题，并提出了运行时验证方法来提升对齐性。,"Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",arXiv.org,2,Black-box,General,论文明确研究 Reasoning-Action Alignment（即 CoT 与实际行动的忠实度），提出运行时验证方法来确保文本推理与实际动作的一致性，直接涉及 Faithfulness 的核心问题。
Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出通过可验证奖励提升 CoT 忠实性，并讨论了不忠实现象。,"The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.",arXiv.org,2,Black-box,General,论文明确提出了一个框架（Chart-RVR）来增强 Large Vision-Language Models（LVLMs）在图表推理任务中的鲁棒性和可解释性，特别是通过自动可验证的奖励来最大化流程一致性（process conformity）和 faithfulness 的图表表格重建，从而改进 rationale fidelity。这与 CoT Faithfulness 的定义高度相关，尤其是论文中提到的 'improving rationale fidelity' 和 'faithful chart table reconstruction' 直接涉及 CoT 推理的真实性和忠实度。
Argumentative Large Language Models for Explainable and Contestable Claim Verification,2024,True,False,True,Verification & External Tools,论文提出通过论证框架提升解释性和可争议性，与CoT忠实性相关。,"The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing argumentative LLMs (ArgLLMs), a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs’ performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.",AAAI Conference on Artificial Intelligence,1,Black-box,General,该论文研究了如何通过论证框架增强LLM的解释性，并允许对决策进行争议，虽然涉及了解释性和可争议性，但并未明确研究CoT是否是模型预测的真实原因（Faithfulness的核心定义），因此评为边缘相关。
Faithful Chain-of-Thought Reasoning,2023,True,True,False,Verification & External Tools,论文提出Faithful CoT框架，通过外部求解器保证推理链忠实性，属于改进方法。,"While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.",International Joint Conference on Natural Language Processing,2,Black-box,General,论文明确提出了 Faithful CoT 框架，关注生成的推理链是否真实反映模型的计算过程（faithfulness），并设计了两个阶段来保证这一点。虽然涉及多个领域，但核心贡献是对 faithfulness 的研究，因此属于核心相关。
ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation,2025,True,False,False,Verification & External Tools,论文提出使用结构化推理框架提升解释的忠实性，涉及不忠实现象和改进方法。,"Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge, yet suffers from critical limitations in high-stakes domains -- namely, sensitivity to noisy or contradictory evidence and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and contestable alternative that replaces black-box reasoning with structured inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG constructs a QBAF from retrieved documents and performs deterministic reasoning under gradual semantics. This allows faithfully explaining and contesting decisions. Evaluated on two fact verification benchmarks, PubHealth and RAGuard, ArgRAG achieves strong accuracy while significantly improving transparency.",arXiv.org,1,Black-box,General,论文提出了一个解释性和可争议的替代方案，使用了结构化推理来替代黑盒推理，并提到了 'faithfully explaining and contesting decisions'。虽然它涉及解释性和透明性，但并未明确研究 CoT 是否是模型预测的真实原因（Causal role），因此被评为边缘相关。
Are LLMs Better Formalizers than Solvers on Complex Problems?,2025,True,True,False,Verification & External Tools,论文讨论了LLM作为形式化器的忠实性问题，并提出了使用外部求解器作为改进方法。,"A trending line of recent work advocates for using large language models (LLMs) as formalizers instead of as end-to-end solvers for logical reasoning problems. Instead of generating the solution, the LLM generates a formal program that derives a solution via an external solver. While performance gain of the seemingly scalable LLM-as-formalizer over the seemingly unscalable LLM-as-solver has been widely reported, we show that this superiority does not hold on real-life constraint satisfaction problems. On 4 domains, we systematically evaluate 6 LLMs including 4 large reasoning models with inference-time scaling, paired with 5 pipelines including 2 types of formalism. We show that in few-shot settings, LLM-as-formalizer underperforms LLM-as-solver. While LLM-as-formalizer promises accuracy, robustness, faithfulness, and efficiency, we observe that the present LLMs do not yet deliver any of those, as their limited ability to generate formal programs leads to failure to scale with complexity, hard-coded solutions, and excessive reasoning tokens. We present our detailed analysis and actionable remedies to drive future research that improves LLM-as-formalizer.",,2,Black-box,Logic,论文明确指出 LLM 作为 formalizer 时未能兑现其承诺的准确性、鲁棒性、忠实性和效率，特别是提到了 'faithfulness' 作为评估指标之一。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到 CoT 生成的解释易受内容偏见影响，从而影响其鲁棒性和忠实度（faithfulness）。作者提出的 QuaSAR 方法旨在通过准符号抽象来改善 CoT 的忠实度，且实验验证了其在对抗性任务上的鲁棒性和一致性提升。因此，该论文直接涉及 CoT 的忠实度研究。
FaithAct: Faithfulness Planning and Acting in MLLMs,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了FaithEval评估框架和FaithAct改进方法，关注CoT忠实性。,"Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",,2,Black-box,General,该论文明确研究了行为忠实度（行为和输出的一致性）和感知忠实度（推理和输入的一致性），并提出了量化评估框架FaithEval。FaithAct框架强化了每个推理步骤的依据基础，直接针对CoT的忠实性问题。
Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools",论文提出AoU框架，通过验证前提来提升推理忠实性，并实证改进准确性和忠实性。,"Large language models (LLMs) often generate reasoning traces that appear coherent but rest on unsupported assumptions, leading to hallucinated conclusions. Prior work mainly addresses factual hallucinations or relies on post-hoc verification, leaving reasoning-induced hallucinations largely unaddressed. We propose Audit-of-Understanding (AoU), a framework that constrains inference to validated premises through three phases: (1) decomposing a query into candidate assumptions, (2) auditing their support, and (3) conditioning inference only on the validated subset. Formally, AoU is \emph{posterior-constrained inference}, connecting to selective prediction and rejection learning. Our contributions are threefold: (i) theoretical guarantees under perfect validation, (ii) excess-risk bounds under imperfect audits, and (iii) tractability analysis. Empirically, AoU improves both accuracy and faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at https://anonymous.4open.science/r/audit-of-understanding-E28B.",arXiv.org,2,Black-box,Math,论文提出了 Audit-of-Understanding (AoU) 框架，旨在约束推理过程以确保其忠实性。该框架通过分解查询、审核候选假设、并仅基于已验证的前提进行推理，以解决 LLMs 在推理过程中产生的未经验证的假设和幻觉问题。实验结果显示 AoU 在准确性和忠实性（faithfulness）上的显著提升，直接关联到 CoT Faithfulness 的研究范畴。
Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain,2025,True,True,True,Verification & External Tools,论文讨论了LLM推理的不忠实现象，并提出了结合符号引擎的改进方法。,"Reasoning is essential for closed-domain QA systems in which procedural correctness and policy compliance are critical. While large language models (LLMs) have shown strong performance on many reasoning tasks, recent work reveals that their reasoning traces are often unfaithful—serving more as plausible justifications than as causally grounded derivations. Efforts to combine LLMs with symbolic engines (e.g., Prover9, Z3) have improved reliability but remain limited to static forms of logic, struggling with dynamic, state-based reasoning such as multi-step progressions and conditional transitions. In this paper, we propose MCFR (Model Checking for Formal Reasoning), a neuro-symbolic framework that integrates LLMs with model checking to support property verification. MCFR translates natural language into formal specifications and verifies them over transition models. To support evaluation, we introduce EduMC-QA, a benchmark dataset grounded in real academic procedures. Our results show that MCFR improves reasoning faithfulness and interpretability, offering a viable path toward verifiable QA in high-stakes closed-domain applications. In addition to evaluating MCFR, we compare its performance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to contextualize its effectiveness.",,2,White-box,General,论文明确研究了LLM推理轨迹的非忠实性问题（unfaithful），并提出了MCFR框架来提高推理的忠实性和可解释性。此外，论文还对比了MCFR与传统LLM（如ChatGPT、DeepSeek、Claude）的表现，进一步验证了其在忠实性方面的改进。
QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,True,False,False,"Training & Fine-tuning, Verification & External Tools",论文关注生成忠实解释的推理过程，并提出了基于QA的方法来预测自然逻辑运算符。,"Fact verification systems assess a claim's veracity based on evidence. An important consideration in designing them is faithfulness, i.e. generating explanations that accurately reflect the reasoning of the model. Recent works have focused on natural logic, which operates directly on natural language by capturing the semantic relation of spans between an aligned claim with its evidence via set-theoretic operators. However, these approaches rely on substantial resources for training, which are only available for high-resource languages. To this end, we propose to use question answering to predict natural logic operators, taking advantage of the generalization capabilities of instruction-tuned language models. Thus, we obviate the need for annotated training data while still relying on a deterministic inference system. In a few-shot setting on FEVER, our approach outperforms the best baseline by $4.3$ accuracy points, including a state-of-the-art pre-trained seq2seq natural logic system, as well as a state-of-the-art prompt-based classifier. Our system demonstrates its robustness and portability, achieving competitive performance on a counterfactual dataset and surpassing all approaches without further annotation on a Danish verification dataset. A human evaluation indicates that our approach produces more plausible proofs with fewer erroneous natural logic operators than previous natural logic-based systems.",Conference on Empirical Methods in Natural Language Processing,2,Black-box,General,论文明确提到在设计基于自然逻辑的事实验证系统时，忠实度是一个重要考虑因素，即生成的解释需要准确反映模型的推理过程。此外，论文还进行了人工评估，表明其方法生成的解释比以前的自然逻辑系统更合理，符合 CoT Faithfulness 的核心研究范畴。

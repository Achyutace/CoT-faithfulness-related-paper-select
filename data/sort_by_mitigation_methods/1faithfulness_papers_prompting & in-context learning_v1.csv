title,year,is_relevant,has_phenomenon,has_metrics,mitigation_methods,reasoning,abstract,publication_venue,related_score,type,domain,tagging_reasoning
Towards Transparent Reasoning: What Drives Faithfulness in Large Language Models?,2025,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了LLM的解释忠实性问题，并提出了改进方法。,"Large Language Models (LLMs) often produce explanations that do not faithfully reflect the factors driving their predictions. In healthcare settings, such unfaithfulness is especially problematic: explanations that omit salient clinical cues or mask spurious shortcuts can undermine clinician trust and lead to unsafe decision support. We study how inference and training-time choices shape explanation faithfulness, focusing on factors practitioners can control at deployment. We evaluate three LLMs (GPT-4.1-mini, LLaMA 70B, LLaMA 8B) on two datasets-BBQ (social bias) and MedQA (medical licensing questions), and manipulate the number and type of few-shot examples, prompting strategies, and training procedure. Our results show: (i) both the quantity and quality of few-shot examples significantly impact model faithfulness; (ii) faithfulness is sensitive to prompting design; (iii) the instruction-tuning phase improves measured faithfulness on MedQA. These findings offer insights into strategies for enhancing the interpretability and trustworthiness of LLMs in sensitive domains.",arXiv.org,2,Black-box,Medical,论文直接研究了解释（CoT/reasoning trace）是否忠实反映模型预测的实际驱动因素，特别是在医疗和社会偏见领域中。研究探讨了few-shot示例数量和类型、提示策略等训练和推理选择如何影响忠实度，符合核心相关标准。
Self-Critique and Refinement for Faithful Natural Language Explanations,2025,True,True,True,Prompting & In-Context Learning,论文提出通过自我批评和细化框架提升解释的忠实性，涉及不忠实现象和改进方法。,"With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.",,2,Black-box,General,该论文明确研究了自然语言解释（NLEs）的忠实性问题，提出了一个自我批判和细化的框架来提高解释的忠实度，这与CoT Faithfulness的核心定义相符。论文还展示了如何通过反馈机制减少解释的不忠实率，直接触及了Faithfulness的关键问题。
Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models,2024,True,True,False,Prompting & In-Context Learning,论文提出Logic-of-Thought方法解决CoT的不忠实问题，属于改进方法中的Prompting类别。,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information descriptions and utilizes them as an additional augmentation to original contexts, thereby ensuring information completeness and enhancing logical reasoning ability. LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%, improves Chain-of-Thought with Self-Consistency's performance on the RuleTaker dataset by +3.52%, and boosts performance of Tree-of-Thoughts on the ProofWriter dataset by +8%.",North American Chapter of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到了 Chain-of-Thought 存在 unfaithful 问题，即生成的推理链与最终结论不一致，并提出了 Logic-of-Thought (LoT) 来解决这一问题。这与 Faithfulness 的核心定义和研究范畴高度相关。
FaithLM: Towards Faithful Explanations for Large Language Models,2024,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出FaithLM框架，评估和改进LLM解释的忠实性，涉及不忠实现象和度量指标。,"Large language models (LLMs) increasingly produce natural language explanations, yet these explanations often lack faithfulness, and they do not reliably reflect the evidence the model uses to decide. We introduce FaithLM, a model-agnostic framework that evaluates and improves the faithfulness of LLM explanations without token masking or task-specific heuristics. FaithLM formalizes explanation faithfulness as an intervention property: a faithful explanation should yield a prediction shift when its content is contradicted. Theoretical analysis shows that the resulting contrary-hint score is a sound and discriminative estimator of faithfulness. Building on this principle, FaithLM iteratively refines both the elicitation prompt and the explanation to maximize the measured score. Experiments on three multi-domain datasets and multiple LLM backbones demonstrate that FaithLM consistently increases faithfulness and produces explanations more aligned with human rationales than strong self-explanation baselines. These findings highlight that intervention-based evaluation, coupled with iterative optimization, provides a principled route toward faithful and reliable LLM explanations.",,2,Black-box,General,该论文明确研究LLM生成的解释（如CoT）的忠实性，提出了一种评估和改进解释忠实性的框架FaithLM，并通过干预性评估验证解释的真实性，完全符合Faithfulness的核心研究方向。
How Interpretable are Reasoning Explanations from Prompting Large Language Models?,2024,True,False,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文讨论了CoT的忠实性，并提出了新的评估框架和改进方法。,"Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",,2,Black-box,General,该论文明确研究 Chain-of-Thought 的解释性，特别是 faithfulness（忠实度），并进行了多方面评估，包括 robustness 和 utility。虽然没有涉及模型内部机制，但直接关注 CoT 解释的真实性，符合核心相关标准。
Boosting Language Models Reasoning with Chain-of-Knowledge Prompting,2023,True,True,True,"Verification & External Tools, Prompting & In-Context Learning",论文提出 CoK prompting 和 F^2-Verification 方法，旨在解决 CoT 的不忠实问题。,"Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",Annual Meeting of the Association for Computational Linguistics,2,Black-box,General,论文明确提出 Chain-of-Knowledge (CoK) prompting 来解决 CoT 中的 unfaithful reasoning chains 问题，并引入了 F^2-Verification 方法来评估推理链的 factuality 和 faithfulness。这直接涉及 CoT 的忠实度问题，符合核心相关标准。
On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models,2024,True,True,False,"Training & Fine-tuning, Prompting & In-Context Learning, Interpretability & Internal Mechanisms",论文探讨了提升CoT忠实性的方法，并揭示了当前方法的局限性。,"As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",arXiv.org,2,White-box,General,"论文明确研究 Chain-of-Thought (CoT) 推理的忠实度（Faithfulness），探讨了三种方法（in-context learning, fine-tuning, 和 activation editing）来增强 CoT 的忠实性，并进行了广泛的实证分析。研究中还涉及了模型的内部机制激活编辑"
Preventing Language Models From Hiding Their Reasoning,2023,True,True,True,"Training & Fine-tuning, Prompting & In-Context Learning",论文探讨了模型中‘编码推理’的不忠实现象，并提出了评估防御方法和训练策略。,"Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.",arXiv.org,2,Black-box,General,该论文明确研究了中间推理步骤的忠实性（Faithfulness），特别是探讨了模型可能通过编码方式隐藏真实推理过程（encoded reasoning），并提出了一种评估防御方法的方法论。这直接涉及CoT Faithfulness的核心问题，即推理步骤是否真实反映了模型的内部计算过程。
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning,2023,True,True,True,Prompting & In-Context Learning,论文提出通过问题分解提升CoT忠实性，并使用了新度量指标验证。,"As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithfulness of model-generated reasoning over CoT, while still achieving some of the performance gains of CoT. Our results show it is possible to improve the faithfulness of model-generated reasoning; continued improvements may lead to reasoning that enables us to verify the correctness and safety of LLM behavior.",arXiv.org,2,Black-box,General,该论文明确研究了如何通过问题分解来改进CoT推理的忠实性，提出了提高模型生成推理忠实性的方法，并使用了最近提出的度量标准进行验证。这直接对应于Faithfulness研究的核心定义。
Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration,2023,True,True,False,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning, Consistency & Ensembling",论文提出多模型协作提升推理忠实性，涉及不忠实现象和改进方法。,"Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",arXiv.org,2,Black-box,General,该论文明确提到了增强推理过程的 'faithfulness'，并通过多模型协作（Debate、Review、Retrieve模式）来提高推理的可信度和可靠性。虽然主要关注点是通过多模型协作来克服幻觉和提高解决方案的质量，但仍然涉及到了Faithfulness的概念，因此属于核心相关。
Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions,2025,True,True,False,"Verification & External Tools, Prompting & In-Context Learning",论文讨论了CoT的不忠实现象，并提出了QuaSAR方法来提升Faithfulness。,"Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).",Annual Meeting of the Association for Computational Linguistics,2,Black-box,Logic,该论文明确提到 CoT 生成的解释易受内容偏见影响，从而影响其鲁棒性和忠实度（faithfulness）。作者提出的 QuaSAR 方法旨在通过准符号抽象来改善 CoT 的忠实度，且实验验证了其在对抗性任务上的鲁棒性和一致性提升。因此，该论文直接涉及 CoT 的忠实度研究。
FaithAct: Faithfulness Planning and Acting in MLLMs,2025,True,True,True,"Training & Fine-tuning, Verification & External Tools, Prompting & In-Context Learning",论文提出了FaithEval评估框架和FaithAct改进方法，关注CoT忠实性。,"Unfaithfulness remains a persistent challenge for large language models (LLMs), which often produce plausible yet ungrounded reasoning chains that diverge from perceptual evidence or final conclusions. We distinguish between behavioral faithfulness (alignment between reasoning and output) and perceptual faithfulness (alignment between reasoning and input), and introduce FaithEval for quantifying step-level and chain-level faithfulness by evaluating whether each claimed object is visually supported by the image. Building on these insights, we propose FaithAct, a faithfulness-first planning and acting framework that enforces evidential grounding at every reasoning step. Experiments across multiple reasoning benchmarks demonstrate that FaithAct improves perceptual faithfulness by up to 26% without degrading task accuracy compared to prompt-based and tool-augmented baselines. Our analysis shows that treating faithfulness as a guiding principle not only mitigates hallucination but also leads to more stable reasoning trajectories. This work thereby establishes a unified framework for both evaluating and enforcing faithfulness in multimodal reasoning.",,2,Black-box,General,该论文明确研究了行为忠实度（行为和输出的一致性）和感知忠实度（推理和输入的一致性），并提出了量化评估框架FaithEval。FaithAct框架强化了每个推理步骤的依据基础，直接针对CoT的忠实性问题。
